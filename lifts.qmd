---
title: "Lifts: parameterizations made easy"
subtitle: "Use lifts to compose an optimization problem with a parameterization"
toc: true
toc-depth: 3
toc-expand: 2
sidebar: tutorial
---


::: {.callout-warning}
## Run `importmanopt`
Lifts were added to Manopt late June 2024.
To use them, you need a sufficiently recent version of the code (e.g., from [github](https://github.com/NicolasBoumal/manopt/)).
Lifts are stored in a new folder `/manopt/lifts` that needs to be on Matlab's path.
To ensure this, call `importmanopt`.
:::

## General idea ![](images/icon_salute.gif)

At first sight, optimization on manifolds is restricted to optimization problems on smooth sets.
Notwithstanding, we can use smooth parameterizations of (possibly) nonsmooth sets to extend the realm of applications.
See built-in examples in @tbl-lifts.
This eases the process of working with:

* constraints,
* nonsmooth sets, or simply
* a change of variables.

<!--This is done with the tool `manoptlift`. -- Commented out because the pink color of the code suggests it corresponds to the pink parts in the diagram, but it doesn't. We have manoptlift in the title of the next section, so it's fine. -->

Consider the following commutative diagram.

<!--https://q.uiver.app/#q=WzAsNSxbNCwzLCJcXG1hdGhjYWx7Tn0iXSxbOCwzLCJcXG1hdGhiYntSfSJdLFs0LDIsIlxcbWF0aGNhbHtNfSJdLFs0LDEsIlxcbWF0aGNhbHtFfSIsWzMwMCw2MCw2MCwxXV0sWzAsMF0sWzAsMSwiZiIsMl0sWzIsMCwiXFx2YXJwaGkiXSxbMiwxLCJnPWZcXGNpcmMgXFx2YXJwaGkiXSxbMiwzLCIiLDIseyJjb2xvdXIiOlszMDAsNjAsNjBdLCJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJob29rIiwic2lkZSI6ImJvdHRvbSJ9fX1dLFszLDEsIlxcYmFye2d9IiwwLHsiY3VydmUiOi0yLCJjb2xvdXIiOlszMDAsNjAsNjBdfSxbMzAwLDYwLDYwLDFdXSxbMywwLCJcXGJhcntcXHZhcnBoaX0iLDIseyJjdXJ2ZSI6MywiY29sb3VyIjpbMzAwLDYwLDYwXX0sWzMwMCw2MCw2MCwxXV1d-->
![Commutative diagram on which lifts are based.](images/lift_diagram_EMN.png){#fig-diagramEMN width=25em}

The principal objects in this diagram are in black.
They are as follows:

* $\calM$ is a Riemannian manifold.
* $\calN$ is a Riemannian manifold too (often, it is a Euclidean space).
* $f \colon \calN \to \reals$ is the cost function we wish to minimize *on some subset*.
* $\varphi \colon \calM \to \calN$ is the *lift* or *parameterization*: its image $\calX = \varphi(\calM)$ is a subset of $\calN$.
* $g = f \circ \varphi \colon \calM \to \reals$ is the function we will actually minimize (without restriction on $\calM$).

Notice that by minimizing $g$ on $\calM$, we are effectively minimizing $f$ on $\calX$, because $g(y) = f(\varphi(y))$ so that we only ever evaluate $f$ at points of the form $x = \varphi(y)$ which, by definition, are in $\calX$.

Moreover, if $f$ and $\varphi$ are both smooth, then so is $g$ by composition.

This means that even if $\calX$ is a nonsmooth set, we can still use Manopt to (try to) minimize $f$ on $\calX \subseteq \calN$.

We can do this explicitly by constructing a `problem` structure for $g$ on $\calM$.
Alternatively, we can construct a problem structure for $f$ on $\calN$ and compose it with $\varphi$ by using `manoptlift`.
To do so, this tool requires some information about $\varphi$ and its derivatives: this is all stored in a `lift` structure.
Manopt comes with some built-in lifts (see below), and users can create their own.

## Upstairs, downstairs and `manoptlift`

There are now two related optimization problems to consider.
The problem **downstairs** is
$$
    \min_{x \in \calN} f(x) \quad \textrm{ subject to } \quad x \in \calX,
$$
where $\calX = \varphi(\calM) \subseteq \calN$ is the image of the lift and $f \colon \calN \to \reals$ is the cost function.
The problem **upstairs** is
$$
    \min_{y \in \calM} g(y),
$$
where $g = f \circ \varphi \colon \calM \to \reals$.
The names "upstairs" and "downstairs" are reminiscent of the diagram in @fig-diagramEMN, and echo the meaning of a "lift" as a tool to go from downstairs to upstairs.

In order to solve the downstairs problem, we first go through the following steps:

1. Build a problem structure `downstairs` for $f \colon \calN \to \reals$ (*ignoring* the constraint $x \in \calX$).
2. Build a `lift` structure for $\varphi \colon \calM \to \calN$, e.g., by selecting an existing one.
3. Call `upstairs = manoptlift(downstairs, lift)`.

The outcome is a problem structure `upstairs` which encodes $g \colon \calM \to \reals$ with $g = f \circ \varphi$.
Then,

4. Call a solver on the upstairs problem, e.g., `y = trustregions(upstairs)`.
5. Map $y$ down to $x = \varphi(y)$ with `x = lift.phi(y)`.

The resulting $x$ is guaranteed to be in $\calX$ (by construction).
If $y$ is optimal, so is $x$.

There are several ways to call `manoptlift`:

``` matlab
[upstairs, downstairs] = manoptlift(downstairs, lift)
[upstairs, downstairs] = manoptlift(downstairs, lift, 'AD')
[upstairs, downstairs] = manoptlift(downstairs, lift, 'noAD')
[upstairs, downstairs] = manoptlift(downstairs, lift, 'ADnohess')
```

* The third input (optional) is a string:
    - If it is omitted, or `''`, or `'noAD'`, there is no effect.
    - If it is `'AD'`, then automatic differentiation is used on `downstairs` with `manoptAD` before `upstairs` is built.
    - If it is `'ADnohess'`, then `manoptAD` is called with the `nohess` option.
* The `downstairs` structure is available as second output (optional) because `manoptlift` may modify it in the following circumstances:
    - If `downstairs.M` is omitted, then it is set to `lift.N`.
    - If automatic differentiation is used.

## Landscape relations

A point $y \in \calM$ maps to a point $x = \varphi(y) \in \calX$.
With respect to the landscapes of the cost functions $g = f \circ \varphi$ and $f$, we may be interested in the following properties:

* **global $\Rightarrow$ global**: if $y$ is a global minimum upstairs, then $x$ is a global minimum downstairs. This is always true, simply because $\varphi$ is a surjection to $\calX$.
* **local $\Rightarrow$ local**: if $y$ is a local minimum upstairs, then $x$ is a local minimum downstairs. This is true if and only if $\varphi$ is an open map from $\calM$ to $\calX$.
* **1 $\Rightarrow$ 1**: if $y$ is a first-order critical point upstairs, then $x$ is a first-order stationary point downstairs. This is not often the case. In particular, it requires the tangent cone to $\calX$ at $x$ to be linear.
* **2 $\Rightarrow$ 1**: if $y$ is a second-order critical point upstairs, then $x$ is a first-order stationary point downstairs. This is more common.

Context, proofs and general theory are developed in the following paper:

> Eitan Levin, Joe Kileel, Nicolas Boumal<br>
> *The effect of smooth parametrizations on nonconvex optimization landscapes*<br>
> [Mathematical Programming](https://link.springer.com/article/10.1007/s10107-024-02058-3), 2024

Many of the lifts included in Manopt are treated as explicit examples in that paper.


## Lift structures: what's in them?

The `lift` structure must provide enough information about $\varphi$ so that Manopt can deduce how to compute $g = f \circ \varphi$, its gradient and (ideally) its Hessian, based on the corresponding information about $f$.
@tbl-liftfields lists the fields in `lift`, including manifold structures for $\calM$ and $\calN$, and function handles for $\varphi$ and more.

Let $y \in \calM$ and $x = \varphi(y) \in \calN$.
By the chain rule, for all $v \in \T_y\calM$ we have
$$
    \D g(y)[v] = \D(f \circ \varphi)(y)[v] = \D f(x)[\D\varphi(y)[v]],
$$
where $\D\varphi(y) \colon \T_y\calM \to \T_x\calN$ is the differential of $\varphi$ at $y$.
By definition of the Riemannian gradient, then by definition of the adjoint of a linear map (indicated by a star $\cdot^*$), this yields
$$
    \inner{\grad g(y)}{v}_y = \inner{\grad f(x)}{\D\varphi(y)[v]}_x = \inner{\D\varphi(y)^*[\grad f(x)]}{v}_y.
$$
This holds for all $v$.
Thus,
$$
    \grad g(y) = \D\varphi(y)^*[\grad f(x)].
$$
This illustrates how we can get the gradient of $g$ from that of $f$ provided the lift gives us access to $\D\varphi(y)^*$.
<!--(We might call this map `fgrad2ggrad` to echo the similar role it plays when compared to `egrad2rgrad` if $\varphi$ were the inclusion map from the manifold into its embedding space.)-->
For the Hessian, additional computations shown later on this page tell us what else needs to be included: see @eq-hessgtotal.


| Name  | Field usage  | Functionality  |
|:-|:--|:---|
| $\calM$ | `lift.M` | Manifold structure from a factory ("upstairs") |
| $\calN$ | `lift.N` | Manifold structure from a factory ("downstairs") |
| $\varphi$ | `x = lift.phi(y)` | Computes the image of $y \in \calM$ through $\varphi \colon \calM \to \calN$ |
| $\D\varphi(y)$ | `u = lift.Dphi(y, v)` | Differential of $\varphi$ at $y \in \calM$ along $v \in \T_y\calM$. The output is $u = \D\varphi(y)[v]$ in $\T_x\calN$ where $x = \varphi(y)$. |
| $\D\varphi(y)^*$ | `v = lift.Dphit(y, u)` | Adjoint of $\D\varphi(y)$ with respect ot the inner products on $\T_y\calM$ and $\T_x\calN$, where $x = \varphi(y)$. Thus, $$\inner{\D\varphi(y)[v]}{u}_x = \inner{v}{\D\varphi(y)^*[u]}_y.$$ |
| $\Hess h_w(y)$ | `s = lift.hesshw(y, v, w)` | Given $y \in \calM$ and $w \in \T_x\calN$ where $x = \varphi(y)$, choose $q_w \colon \calN \to \reals$ such that $\grad q_w(x) = w$ (for the specified $x$). Let $h_w = q_w \circ \varphi \colon \calM \to \reals$. Then $s = \Hess h_w(y)[v]$. If $\calN$ is Euclidean, a typical choice is $q_w = \inner{\cdot}{w}_\calN$.  |
| $\Hess q_w(x)$ | `s = lift.hessqw(x, u, w)` | Given $x \in \calN$ and $w \in \T_x\calN$, choose $q_w \colon \calN \to \reals$ such that $\grad q_w(x) = w$ (for the specified $x$). Then $s = \Hess q_w(x)[u]$. If $\calN$ is Euclidean, a typical choice is $q_w = \inner{\cdot}{w}_\calN$, in which case $\Hess q_w(x) = 0$. If the field is omitted, it is assumed to be zero.  |
| $\varphi$ or $\bar{\varphi}$? | `lift.embedded` | Boolean flag set to `false` (by default) if all of the above indeed pertains to $\varphi$, and set to `true` if the above instead describes $\bar{\varphi} \colon \calE \to \calN$, where $\calE$ is the embedding space for $\calM$ (as specified in the documentation for the corresponding factory), as a smooth extension such that $\bar{\varphi}|_\calM = \varphi$. See @fig-diagramEMN. In that case, the lift enables computation of $\grad \bar{g}$ and $\Hess \bar{g}$, which $\calM$ can then convert to $\grad g$ and $\Hess g$ using `egrad2rgrad` and `ehess2rhess`. |
: Fields in a `lift` structure. {#tbl-liftfields}





## Available lifts

A number of lifts are readily available in Manopt, making it possible to optimize over various nonsmooth sets $\calX = \varphi(\calM)$.
We list them in a table now, and detail the corresponding lifts below with some examples.

| Name | $\calX = \varphi(\calM)$ | lift |
|:--|:---|:---|
| Cube | $\{ x \in \Rn : -1 \leq x_{i} \leq 1 \, \forall i \}$ | `cubeslift(n)` |
| Cubes | $\{ X \in \Rnm : -1 \leq X_{ij} \leq 1 \, \forall i,j \}$ | `cubeslift(n, m)` |
| Ball | $\{ x \in \Rn : \|x\| \leq 1 \}$ | `ballslift(n)` |
| Balls | $\{ X \in \Rnm : \|X_{:, i}\| \leq 1 \, \forall i \}$ | `ballslift(n, m)` |
| Simplex | $\{ x \in \Rn : x_1 + \cdots + x_n = 1, x_i \geq 0 \, \forall i \}$ | `hadamardlift('simplex', n)` |
| Column stochastic | $\{ X \in \Rnm : X^\top\One = \One, X_{ij} \geq 0 \}$ | `hadamardlift('colstochastic', n, m)` | 
| Row stochastic | $\{ X \in \Rnm : X\One = \One, X_{ij} \geq 0 \}$ | `hadamardlift('rowstochastic', n, m)` | 
| Nonnegative orthant | $\{ X \in \Rnm : X_{ij} \geq 0 \, \forall i,j \}$ | `hadamardlift('nonnegative', n, m)` |
| Bounded-rank | $\{ X \in \Rmn : \rank(X) \leq r \}$ | `burermonteiroLRlift(m, n, r)` |
| Bounded-rank, semidefinite | $S_n^p \triangleq \{ X \in \Rnn : X \succeq 0, \rank(X) \leq p \}$ | `burermonteirolift('free', n, p)` |
| &rdca; unit trace | $\{ X \in S_n^p : \trace(X) = 1 \}$ | `burermonteirolift('unittrace', n, p)` |
| &rdca; unit diagonal | $\{ X \in S_n^p : \diag(X) = \One \}$ | `burermonteirolift('unitdiag', n, p)` |
: Examples of nonsmooth sets on which we can optimize with Manopt using a lift. {#tbl-lifts}


### Cubes and squares

``` matlab
lift = cubeslift(n);     % m = 1 by default
lift = cubeslift(n, m);
```

The target set $\calX$ consists of vectors or matrices whose entries are in the closed interval $[-1, 1]$:
$$
    \calX = \{ X \in \Rnm : -1 \leq X_{ij} \leq 1 \ \forall i, j \}.
$$
One way to think about it is as $m$ points in the cube $[-1, 1]^n$ (the columns of $X$), or $n$ points in the cube $[-1, 1]^m$ (the rows of $X$).
This is a subset of $\calN = \Rnm$, realized as the image of the lift $\varphi \colon \calM \to \calN$ with $\calM = \Rnm$ and
$$
    \varphi(Y) = \sin(Y),
$$
where $\sin$ is applied entrywise.

This lift satisfies **local $\Rightarrow$ local** and **2 $\Rightarrow$ 1** everywhere.
It also satisifies 1 $\Rightarrow$ 1 at all $Y \in \Rnm$ such that $|X_{ij}| < 1$ for all $i,j$ with $X = \varphi(Y)$.
<!--One way to prove this is by studying the lift explicitly for $m = n = 1$ and then using the general results in our paper for products of lifts.-->

As an example, consider $\min_{x \in \Rn} \frac{1}{2} x^\top A x + b^\top x$ subject to $-1 \leq x_i \leq 1$ for $i = 1, \ldots, n$:

::: {.panel-tabset}

## Without automatic differentiation

``` matlab
n = 5;
A = randsym(n);
b = randn(n, 1);

downstairs.M = euclideanfactory(n); % optional
downstairs.cost = @(x) .5*(x'*A*x) + b'*x;
downstairs.grad = @(x) A*x + b;
downstairs.hess = @(x, xdot) A*xdot;

lift = cubeslift(n);

upstairs = manoptlift(downstairs, lift);

y = trustregions(upstairs);

x = lift.phi(y);
```

## With AD



``` matlab
n = 5;
A = randsym(n);
b = randn(n, 1);

downstairs.cost = @(x) .5*(x'*A*x) + b'*x;

lift = cubeslift(n);

[upstairs, downstairs] = manoptlift(downstairs, lift, 'AD');

y = trustregions(upstairs);

x = lift.phi(y);
```

:::

Note that $\calX$ is convex.
**If $f$ is convex** (e.g., in the example above, if $A$ is positive semidefinite), then the problem downstairs is convex.
Therefore, first-order stationary points are global minima.
Since the lift satisfies 2 $\Rightarrow$ 1, it follows that second-order critical points upstairs are global minima.
In other words: the problem upstairs enjoys the **strict saddle property (benign nonconvexity)**.


### Balls and disks

### Hadamard (entrywise product)

### Burer-Monteiro for smooth SDP

### Burer-Monteiro for general bounded-rank


:::{.callout-note}
## Desingularization lift of bounded rank matrices.
The `desingularizationfactory` encodes the geometry of a Riemannian manifold, but `egrad2rgrad` and `ehess2rhess` in that factory actually correspond to a lift, as described in the documentation of the factory.
Thus, it is not a lift in Manopt's code base, but mathematically it acts as a lift of the set of bounded-rank matrices to a smooth manifold.
This is a good alternative to the $LR^\top$ lift, with stronger theoretical properties as detailed in [this paper](https://arxiv.org/abs/2406.14211).
:::


## The Hessian of $g$

To obtain the Hessian of $g = f \circ \varphi$, it is convenient to go through curves.
Let $c \colon \reals \to \calM$ be a smooth curve satisfying $c(0) = y$, $c'(0) = v$ and $c''(0) = 0$ (zero initial intrinsic acceleration).

Push it to a curve downstairs as $\gamma = \varphi \circ c$.
It satisfies $\gamma(0) = \varphi(y) = x$.
Let $u = \gamma'(0) = \D\varphi(c(0))[c'(0)] = \D\varphi(y)[v]$.

The cost function along these curves satisfies
$$
    g \circ c = f \circ \varphi \circ c = f \circ \gamma.
$$
Thus, the derivatives of $g \circ c$ and $f \circ \gamma$ (two functions from $\reals$ to $\reals$) are identical.
On the one hand, we compute
$$
    (g \circ c)'(t) = \D g(c(t))[c'(t)] = \inner{\grad g(c(t))}{c'(t)}_{c(t)}
$$
and
$$
    (g \circ c)''(t) = \inner{\Hess g(c(t))[c'(t)]}{c'(t)}_{c(t)} + \inner{\grad g(c(t))}{c''(t)}_{c(t)}.
$$
At $t = 0$, these yield
$$
    (g \circ c)'(0) = \inner{\grad g(y)}{v}_{y}
$$
and
$$
    (g \circ c)''(0) = \inner{\Hess g(y)[v]}{v}_{y} + \inner{\grad g(y)}{c''(0)}_{y}.
$$
On the other hand, applying the same computations to $f \circ \gamma$ instead of $g \circ c$ yields
$$
    (f \circ \gamma)'(0) = \inner{\grad f(x)}{u}_{x}
$$
and
$$
    (f \circ \gamma)''(0) = \inner{\Hess f(x)[u]}{u}_{x} + \inner{\grad f(x)}{\gamma''(0)}_{x}.
$$
We know that $(g \circ c)'(0) = (f \circ \gamma)'(0)$ and also that $(g \circ c)''(0) = (f \circ \gamma)''(0)$.
We could use the first-order derivatives to recover the identity
$$
    \grad g(y) = \D\varphi(y)^*[\grad f(x)].
$$
Focusing on second-order derivatives, we have found that
$$
    \inner{\Hess g(y)[v]}{v}_{y} = \inner{\Hess f(x)[u]}{u}_{x} + \inner{\grad f(x)}{\gamma''(0)}_{x} - \inner{\grad g(y)}{c''(0)}_{y}.
$$ {#eq-hessgfoo}
Recall that we assume $c''(0) = 0$.
To handle $\gamma''(0)$, we proceed as follows.

Fix an arbitrary $w \in \T_x\calN$ and let $q_w \colon \calN \to \reals$ satisfy $\grad q_w(x) = w$ (that is, at the specific $x$ under consideration, the gradient of $q_w$ is $w$).
For example, if $\calN$ is a Euclidean space, then a convenient choice is $q_w = \inner{\cdot}{w}_\calN$ (a linear function whose gradient at *all* points is $w$: it's more than we need).

Let $h_w = q_w \circ \varphi$.
This is a real function on $\calM$.
Plug $h_w$ and $q_w$ in place of $g$ and $f$ in @eq-hessgfoo.
This reveals
$$
    \inner{\Hess h_w(y)[v]}{v}_{y} = \inner{\Hess q_w(x)[u]}{u}_{x} + \inner{\grad q_w(x)}{\gamma''(0)}_{x}.
$$
Since $\grad q_w(x) = w$, we have found that
$$
    \inner{w}{\gamma''(0)}_{x} = \inner{\Hess h_w(y)[v]}{v}_{y} - \inner{\Hess q_w(x)[u]}{u}_{x}.
$$
Now let $w = \grad f(x)$ and plug this into @eq-hessgfoo in order to get rid of $\gamma''(0)$, as follows:
$$
    \inner{\Hess g(y)[v]}{v}_{y} = \inner{\Hess f(x)[u]}{u}_{x} + \inner{\Hess h_w(y)[v]}{v}_{y} - \inner{\Hess q_w(x)[u]}{u}_{x}.
$$
Recall that $u = \D\varphi(y)[v]$.
Then, all in all, we have the following formula for the Riemannian Hessian of $g$ at $y$:
$$
    \Hess g(y) = \D\varphi(y)^* \circ \left( \Hess f(x) - \Hess q_w(x) \right) \circ \D\varphi(y) + \Hess h_w(y),
$$ {#eq-hessgtotal}
where

* $x = \varphi(y)$,
* $w = \grad f(x)$,
* $q_w \colon \calN \to \reals$ is any smooth function which satisfies $\grad q_w(x) = w$, and
* $h_w = q_w \circ \varphi \colon \calM \to \reals$.

(The choice of curve $c$ no longer plays a role.)

The final formula @eq-hessgtotal tells us exactly what information we need about $\varphi$ in order to be able to compute the Hessian of $g$ based on the gradient and Hessian of $f$.
The fields `lift.Dphi` and `lift.Dphit` encode the maps $\D\varphi(y)$ and $\D\varphi(y)^*$, while the field `lift.hesshw` encodes the map $\Hess h_w(y)$.
If $\calN$ is a Euclidean space (which is typical), then we always choose $q_w = \inner{\cdot}{w}_\calN$ so that $\Hess q_w(x) = 0$.
Otherwise, we need to provide that map too, in `lift.hessqw`.
