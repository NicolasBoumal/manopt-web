<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of gradcomputefixedrankembedded</title>
  <meta name="keywords" content="gradcomputefixedrankembedded">
  <meta name="description" content="Computes the Riemannian gradient of the cost function at x via AD for">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../index.html">Home</a> &gt;  <a href="#">manopt</a> &gt; <a href="index.html">autodiff</a> &gt; gradcomputefixedrankembedded.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../index.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for manopt\autodiff&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>gradcomputefixedrankembedded
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>Computes the Riemannian gradient of the cost function at x via AD for</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function grad = gradcomputefixedrankembedded(problem,x) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Computes the Riemannian gradient of the cost function at x via AD for
 fixed-rank matices with an embedded geometry

 grad = gradcomputefixedrankembedded(problem,x)

 The first-order method follows the paper: 
 &quot;Automatic differentiation for Riemannian optimization on low-rank matrix 
 and tensor-train manifolds&quot;, A. Novikov, M. Rakhuba, I. Oseledets, 2021

 Paper link: https://arxiv.org/pdf/2103.14974.pdf

 Please cite the Manopt paper as well as the research paper:
    @Misc{novikov2021automatic,
      Title         = {Automatic differentiation for Riemannian optimization on low-rank matrix and tensor-train manifolds}, 
      Author        = {Alexander Novikov and Maxim Rakhuba and Ivan Oseledets},
      Year          = {2021},
      Eprint        = {2103.14974},
      ArchivePrefix = {arXiv},
      PrimaryClass  = {math.OC}
    }

 See also: <a href="autograd.html" class="code" title="function autogradfunc = autograd(problem, fixedrankflag)">autograd</a></pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="dl2mat.html" class="code" title="function x = dl2mat(dlx)">dl2mat</a>	Convert the data type of x from dlarray into double</li><li><a href="mat2dl.html" class="code" title="function dlx = mat2dl(x)">mat2dl</a>	Convert the data type of x from numeric into dlarray</li></ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="manoptAD.html" class="code" title="function problem = manoptAD(problem, flag)">manoptAD</a>	Preprocess automatic differentiation for a manopt problem structure</li></ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function grad = gradcomputefixedrankembedded(problem,x)</a>
0002 <span class="comment">% Computes the Riemannian gradient of the cost function at x via AD for</span>
0003 <span class="comment">% fixed-rank matices with an embedded geometry</span>
0004 <span class="comment">%</span>
0005 <span class="comment">% grad = gradcomputefixedrankembedded(problem,x)</span>
0006 <span class="comment">%</span>
0007 <span class="comment">% The first-order method follows the paper:</span>
0008 <span class="comment">% &quot;Automatic differentiation for Riemannian optimization on low-rank matrix</span>
0009 <span class="comment">% and tensor-train manifolds&quot;, A. Novikov, M. Rakhuba, I. Oseledets, 2021</span>
0010 <span class="comment">%</span>
0011 <span class="comment">% Paper link: https://arxiv.org/pdf/2103.14974.pdf</span>
0012 <span class="comment">%</span>
0013 <span class="comment">% Please cite the Manopt paper as well as the research paper:</span>
0014 <span class="comment">%    @Misc{novikov2021automatic,</span>
0015 <span class="comment">%      Title         = {Automatic differentiation for Riemannian optimization on low-rank matrix and tensor-train manifolds},</span>
0016 <span class="comment">%      Author        = {Alexander Novikov and Maxim Rakhuba and Ivan Oseledets},</span>
0017 <span class="comment">%      Year          = {2021},</span>
0018 <span class="comment">%      Eprint        = {2103.14974},</span>
0019 <span class="comment">%      ArchivePrefix = {arXiv},</span>
0020 <span class="comment">%      PrimaryClass  = {math.OC}</span>
0021 <span class="comment">%    }</span>
0022 <span class="comment">%</span>
0023 <span class="comment">% See also: autograd</span>
0024 
0025 <span class="comment">% This file is part of Manopt: www.manopt.org.</span>
0026 <span class="comment">% Original author: Xiaowen Jiang, Aug. 31, 2021.</span>
0027 <span class="comment">% Contributors:</span>
0028 <span class="comment">% Change log:</span>
0029 
0030     <span class="comment">% check availability</span>
0031     assert(isfield(problem,<span class="string">'autogradfunc'</span>),[<span class="string">'the problem structure must'</span><span class="keyword">...</span><span class="comment">,</span>
0032         <span class="string">' contain the field autogradfunc, see autograd.'</span>])
0033     assert(sum(isfield(x,{<span class="string">'U'</span>,<span class="string">'S'</span>,<span class="string">'V'</span>}))==3 &amp;&amp;<span class="keyword">...</span><span class="comment">, </span>
0034         (contains(problem.M.name(),<span class="string">'rank'</span>,<span class="string">'IgnoreCase'</span>,true)) &amp;&amp;<span class="keyword">...</span><span class="comment">,</span>
0035         (~startsWith(problem.M.name(),<span class="string">'Product manifold'</span>)),[<span class="string">'The manifold'</span><span class="keyword">...</span>
0036         <span class="string">'must be fixed-rank matices with an embedded geometry'</span>]);
0037 
0038     <span class="comment">% convert A,B into dlarrays to prepare for AD</span>
0039     A = <a href="mat2dl.html" class="code" title="function dlx = mat2dl(x)">mat2dl</a>(x.U*x.S); B = <a href="mat2dl.html" class="code" title="function dlx = mat2dl(x)">mat2dl</a>(x.V*x.S);
0040     
0041     <span class="comment">% compute egrad according to autogradfunc</span>
0042     [~,egrad] = dlfeval(problem.autogradfunc,x,A,B);
0043     
0044     <span class="comment">% compute grad</span>
0045     Udelta = <a href="dl2mat.html" class="code" title="function x = dl2mat(dlx)">dl2mat</a>(egrad.A); Vdelta = <a href="dl2mat.html" class="code" title="function x = dl2mat(dlx)">dl2mat</a>(egrad.B);
0046     grad.M = x.U'*Udelta;
0047     grad.Up = Udelta - x.U*((x.U)'*Udelta);
0048     grad.Vp = Vdelta - x.V*((x.V)'*Vdelta);
0049 
0050 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Fri 30-Sep-2022 13:18:25 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>