<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of autograd</title>
  <meta name="keywords" content="autograd">
  <meta name="description" content="Apply automatic differentiation to computing the Euclidean gradient">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../index.html">Home</a> &gt;  <a href="#">manopt</a> &gt; <a href="index.html">autodiff</a> &gt; autograd.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../index.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for manopt\autodiff&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>autograd
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>Apply automatic differentiation to computing the Euclidean gradient</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function autogradfunc = autograd(problem, fixedrankflag) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Apply automatic differentiation to computing the Euclidean gradient

 function autogradfunc = autograd(problem)
 function autogradfunc = autograd(problem, fixedrankflag)

 Returns an AcceleratedFunction or a function handle which can be used to 
 compute Euclidean gradients. See https://ch.mathworks.com/help/
 deeplearning/ref/deep.acceleratedfunction.html for more descriptions 
 about AcceleratedFunction.

 Note: to evaluate the Euclidean gradient of a certain point x(x should be
 of type dlarray), call dfeval(autogradfunc,x) instead of autogradfunc(x).

 See also: <a href="manoptAD.html" class="code" title="function problem = manoptAD(problem, flag)">manoptAD</a>, <a href="egradcompute.html" class="code" title="function egrad = egradcompute(problem, x, complexflag)">egradcompute</a>, <a href="costgradcompute.html" class="code" title="function [cost, grad] = costgradcompute(problem, x, complexflag)">costgradcompute</a></pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="findA_anchors.html" class="code" title="function A = findA_anchors(problem)">findA_anchors</a>	Find the indices of the anchors for the anchoredrotationsfactory</li><li><a href="../../manopt/autodiff/functions_AD/creal.html" class="code" title="function Xreal = creal(X)">creal</a>	Extracts the real part of x</li><li><a href="../../manopt/autodiff/functions_AD/iscstruct.html" class="code" title="function flag = iscstruct(A)">iscstruct</a>	</li></ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="manoptAD.html" class="code" title="function problem = manoptAD(problem, flag)">manoptAD</a>	Preprocess automatic differentiation for a manopt problem structure</li></ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="#_sub1" class="code">function [y, egrad] = autogradfuncinternal(x)</a></li><li><a href="#_sub2" class="code">function [g1, egrad] = autogradfuncinternelfixedrankembedded(x, A, B)</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function autogradfunc = autograd(problem, fixedrankflag)</a>
0002 <span class="comment">% Apply automatic differentiation to computing the Euclidean gradient</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% function autogradfunc = autograd(problem)</span>
0005 <span class="comment">% function autogradfunc = autograd(problem, fixedrankflag)</span>
0006 <span class="comment">%</span>
0007 <span class="comment">% Returns an AcceleratedFunction or a function handle which can be used to</span>
0008 <span class="comment">% compute Euclidean gradients. See https://ch.mathworks.com/help/</span>
0009 <span class="comment">% deeplearning/ref/deep.acceleratedfunction.html for more descriptions</span>
0010 <span class="comment">% about AcceleratedFunction.</span>
0011 <span class="comment">%</span>
0012 <span class="comment">% Note: to evaluate the Euclidean gradient of a certain point x(x should be</span>
0013 <span class="comment">% of type dlarray), call dfeval(autogradfunc,x) instead of autogradfunc(x).</span>
0014 <span class="comment">%</span>
0015 <span class="comment">% See also: manoptAD, egradcompute, costgradcompute</span>
0016 
0017 <span class="comment">% This file is part of Manopt: www.manopt.org.</span>
0018 <span class="comment">% Original author: Xiaowen Jiang, Aug. 31, 2021.</span>
0019 <span class="comment">% Contributors: Nicolas Boumal</span>
0020 <span class="comment">% Change log:</span>
0021 <span class="comment">%</span>
0022 <span class="comment">% To do: Add AD to fixedTTrankfactory, fixedranktensorembeddedfactory</span>
0023 <span class="comment">% and the product manifold which contains fixedrankembeddedfactory</span>
0024 <span class="comment">% or anchoredrotationsfactory</span>
0025     
0026     <span class="comment">% Check availability</span>
0027     assert(isfield(problem,<span class="string">'M'</span>) &amp;&amp; isfield(problem,<span class="string">'cost'</span>),<span class="keyword">...</span>
0028     <span class="string">'problem structure must contain the fields M and cost.'</span>);
0029     assert(exist(<span class="string">'dlarray'</span>, <span class="string">'file'</span>) == 2, [<span class="string">'Deep learning tool box is '</span><span class="keyword">...</span><span class="comment"> </span>
0030     <span class="string">'needed for automatic differentiation'</span>])
0031     
0032     <span class="comment">% Set fixedrankflag to false if the manifold struct is not</span>
0033     <span class="comment">% fixed(multilinear)-rank matrices or tensors with an embedded geometry</span>
0034     <span class="comment">% or tensors of fixed Tensor Train (TT) rank</span>
0035     <span class="keyword">if</span> ~exist(<span class="string">'fixedrankflag'</span>, <span class="string">'var'</span>)|| isempty(fixedrankflag)
0036         fixedrankflag = false;
0037     <span class="keyword">end</span>
0038 
0039     <span class="comment">% Obtain the euclidean gradient function via AD</span>
0040     costfunction = problem.cost;
0041     <span class="comment">% Set fixedrankflag to true if the manifold is fixed-rank matrices with</span>
0042     <span class="comment">% an embedded geometry. The other two cases are not implemented yet.</span>
0043     <span class="keyword">if</span> fixedrankflag
0044         <span class="comment">% AcceleratedFunction can lead to a slow down in this case</span>
0045         autogradfunc = @(x,A,B) <a href="#_sub2" class="code" title="subfunction [g1, egrad] = autogradfuncinternelfixedrankembedded(x, A, B)">autogradfuncinternelfixedrankembedded</a>(x,A,B);
0046     <span class="keyword">else</span>
0047         func = @<a href="#_sub1" class="code" title="subfunction [y, egrad] = autogradfuncinternal(x)">autogradfuncinternal</a>;
0048         <span class="comment">% accelerate</span>
0049         <span class="keyword">try</span>
0050             autogradfunc = dlaccelerate(func); <span class="comment">% Introduced in Matlab 2021a</span>
0051             clearCache(autogradfunc);
0052         <span class="keyword">catch</span>
0053             warning(<span class="string">'manopt:dlaccelerate'</span>, <span class="keyword">...</span>
0054                     [<span class="string">'Function dlaccelerate is not available:\nPlease '</span> <span class="keyword">...</span>
0055                      <span class="string">'upgrade to Matlab 2021a or later and the latest deep\nlearning '</span> <span class="keyword">...</span>
0056                      <span class="string">'toolbox version if possible.\nMeanwhile, auto-diff '</span> <span class="keyword">...</span>
0057                      <span class="string">'may be somewhat slower.\n The hessian is not available as well.\n'</span> <span class="keyword">...</span>
0058                      <span class="string">'To disable this warning: warning(''off'', ''manopt:dlaccelerate'')'</span>]);
0059             autogradfunc = func;
0060         <span class="keyword">end</span>
0061     <span class="keyword">end</span>
0062     
0063     <span class="comment">% define Euclidean gradient function</span>
0064     <a name="_sub1" href="#_subfunctions" class="code">function [y, egrad] = autogradfuncinternal(x)</a>
0065             
0066         y = costfunction(x);
0067         <span class="comment">% In case that the user forgot to take the real part of the cost</span>
0068         <span class="comment">% when dealing with complex problems with Matlab R2021a or earlier,</span>
0069         <span class="comment">% take the real part for AD</span>
0070         <span class="keyword">if</span> <a href="../../manopt/autodiff/functions_AD/iscstruct.html" class="code" title="function flag = iscstruct(A)">iscstruct</a>(y)
0071             y = <a href="../../manopt/autodiff/functions_AD/creal.html" class="code" title="function Xreal = creal(X)">creal</a>(y);
0072         <span class="keyword">end</span>
0073         
0074         <span class="comment">% Call dlgradient to compute the Euclidean gradient. by default,</span>
0075         <span class="comment">% 'RetainData' and 'EnableHigherDerivatives' are set to false</span>
0076         egrad = dlgradient(y, x);
0077         
0078         <span class="comment">% in case that the user is optimizing over anchoredrotationsfactory</span>
0079         <span class="comment">% egrad of anchors with indices in A should be zero</span>
0080         problem_name = problem.M.name();
0081         <span class="keyword">if</span> (contains(problem_name,<span class="string">'Product rotations manifold'</span>) &amp;&amp;<span class="keyword">...</span><span class="comment">, </span>
0082             contains(problem_name,<span class="string">'anchors'</span>) &amp;&amp;<span class="keyword">...</span><span class="comment">,</span>
0083             ~startsWith(problem_name,<span class="string">'Product manifold'</span>))
0084             A = <a href="findA_anchors.html" class="code" title="function A = findA_anchors(problem)">findA_anchors</a>(problem);
0085             egrad(:, :, A) = 0;
0086         <span class="keyword">end</span>
0087     <span class="keyword">end</span>
0088     
0089     <span class="comment">% fixedrankembeddedfactory part</span>
0090     <span class="comment">% obtain the product of egrad and V and the product of egrad</span>
0091     <span class="comment">% transpose and U by differentiating g1 and g2 w.r.t A and B</span>
0092     <a name="_sub2" href="#_subfunctions" class="code">function [g1, egrad] = autogradfuncinternelfixedrankembedded(x, A, B)</a>
0093         X1.U = A; X1.S = eye(size(x.S,1)); X1.V = x.V;
0094         X2.U = x.U; X2.S = eye(size(x.S,1)); X2.V = B;
0095         g1 = costfunction(X1); g2 = costfunction(X2);
0096         egrad.A = dlgradient(g1,A);  egrad.B = dlgradient(g2,B);
0097     <span class="keyword">end</span>
0098     
0099 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Fri 30-Sep-2022 13:18:25 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>