<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of rlbfgs</title>
  <meta name="keywords" content="rlbfgs">
  <meta name="description" content="Riemannian limited memory BFGS solver for smooth objective functions.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../index.html">Home</a> &gt;  <a href="#">manopt</a> &gt; <a href="#">solvers</a> &gt; <a href="index.html">bfgs</a> &gt; rlbfgs.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../index.html"><img alt="<" border="0" src="../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for manopt\solvers\bfgs&nbsp;<img alt=">" border="0" src="../../../right.png"></a></td></tr></table>-->

<h1>rlbfgs
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>Riemannian limited memory BFGS solver for smooth objective functions.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>function [x, cost, info, options] = rlbfgs(problem, x0, options) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Riemannian limited memory BFGS solver for smooth objective functions.
 
 function [x, cost, info, options] = rlbfgs(problem)
 function [x, cost, info, options] = rlbfgs(problem, x0)
 function [x, cost, info, options] = rlbfgs(problem, x0, options)
 function [x, cost, info, options] = rlbfgs(problem, [], options)


 This is a Riemannian limited memory BFGS solver (quasi-Newton method), 
 which aims to minimize the cost function in the given problem structure.
 It requires access to the gradient of the cost function.

 Parameter options.memory can be used to specify the number of iterations
 the algorithm remembers and uses to approximate the inverse Hessian of
 the cost. Default value is 30.
 For unlimited memory, set options.memory = Inf.


 For a description of the algorithm and theorems offering convergence
 guarantees, see the references below.

 The initial iterate is x0 if it is provided. Otherwise, a random point on
 the manifold is picked. To specify options whilst not specifying an
 initial iterate, give x0 as [] (the empty matrix).

 The two outputs 'x' and 'cost' are the last reached point on the manifold
 and its cost.
 
 The output 'info' is a struct-array which contains information about the
 iterations:
   iter (integer)
       The iteration number. The initial guess is 0.
    cost (double)
       The corresponding cost value.
    gradnorm (double)
       The (Riemannian) norm of the gradient.
    time (double)
       The total elapsed time in seconds to reach the corresponding cost.
    stepsize (double)
       The size of the step from the previous to the new iterate.
   accepted (Boolean)
       true if step is accepted in the cautious update. 0 otherwise.
   And possibly additional information logged by options.statsfun.
 For example, type [info.gradnorm] to obtain a vector of the successive
 gradient norms reached at each iteration.

 The options structure is used to overwrite the default values. All
 options have a default value and are hence optional. To force an option
 value, pass an options structure with a field options.optionname, where
 optionname is one of the following and the default value is indicated
 between parentheses:

   tolgradnorm (1e-6)
       The algorithm terminates if the norm of the gradient drops below
       this. For well-scaled problems, a rule of thumb is that you can
       expect to reduce the gradient norm by 8 orders of magnitude
       (sqrt(eps)) compared to the gradient norm at a &quot;typical&quot; point (a
       rough initial iterate for example). Further decrease is sometimes
       possible, but inexact floating point arithmetic will eventually
       limit the final accuracy. If tolgradnorm is set too low, the
       algorithm may end up iterating forever (or at least until another
       stopping criterion triggers).
   maxiter (1000)
       The algorithm terminates if maxiter iterations were executed.
   maxtime (Inf)
       The algorithm terminates if maxtime seconds elapsed.
   minstepsize (1e-10)
     The minimum norm of the tangent vector that points from the current
     point to the next point. If the norm is less than minstepsize, the 
     program will terminate.
   memory (30)
     The number of previous iterations the program remembers. This is used 
     to approximate the inverse Hessian at the current point. Because of
     difficulty of maintaining a representation of operators in terms of
     coordinates, a recursive method is used. The number of steps in the
     recursion is at most options.memory. This parameter can take any
     integer value &gt;= 0, or Inf, which is taken to be options.maxiter. If
     options.maxiter has value Inf, then it will take value 10000 and a
     warning will be displayed.
   strict_inc_func (@(t) 1e-4*t)
     The Cautious step needs a real function that has value 0 at t = 0,
     and  is strictly increasing. See details in Wen Huang's paper
     &quot;A Riemannian BFGS Method without Differentiated Retraction for 
     Nonconvex Optimization Problems&quot;
   statsfun (none)
       Function handle to a function that will be called after each
       iteration to provide the opportunity to log additional statistics.
       They will be returned in the info struct. See the generic Manopt
       documentation about solvers for further information. statsfun is
       called with the point x that was reached last.
   stopfun (none)
       Function handle to a function that will be called at each iteration
       to provide the opportunity to specify additional stopping criteria.
       See the generic Manopt documentation about solvers for further
       information.
   verbosity (2)
       Integer number used to tune the amount of output the algorithm
       generates during execution (mostly as text in the command window).
       The higher, the more output. 0 means silent. 3 and above includes a
       display of the options structure at the beginning of the execution.
   debug (false)
       Set to true to allow the algorithm to perform additional
       computations for debugging purposes. If a debugging test fails, you
       will be informed of it, usually via the command window. Be aware
       that these additional computations appear in the algorithm timings
       too, and may interfere with operations such as counting the number
       of cost evaluations, etc. (the debug calls get storedb too).
   storedepth (2)
       Maximum number of different points x of the manifold for which a
       store structure will be kept in memory in the storedb for caching.
       If memory usage is an issue, you may try to lower this number.
       Profiling may then help to investigate if a performance hit was
       incurred as a result.


 Please cite the Manopt paper as well as the research paper:
 @InBook{Huang2016,
   title     = {A {R}iemannian {BFGS} Method for Nonconvex Optimization Problems},
   author    = {Huang, W. and Absil, P.-A. and Gallivan, K.A.},
   year      = {2016},
   publisher = {Springer International Publishing},
   editor    = {Karas{\&quot;o}zen, B{\&quot;u}lent and Manguo{\u{g}}lu, Murat and Tezer-Sezgin, M{\&quot;u}nevver and G{\&quot;o}ktepe, Serdar and U{\u{g}}ur, {\&quot;O}m{\&quot;u}r},
   address   = {Cham},
   booktitle = {Numerical Mathematics and Advanced Applications ENUMATH 2015},
   pages     = {627--634},
   doi       = {10.1007/978-3-319-39929-4_60}
 }

 We point out that, at the moment, this implementation of RLBFGS can be
 slower than the implementation in ROPTLIB by Wen Huang et al. referenced
 above. For the purpose of comparing to their work, please use their
 implementation.</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>	</li><li><a href="../../../manopt/core/applyStatsfun.html" class="code" title="function stats = applyStatsfun(problem, x, storedb, key, options, stats)">applyStatsfun</a>	Apply the statsfun function to a stats structure (for solvers).</li><li><a href="../../../manopt/core/canGetApproxGradient.html" class="code" title="function candoit = canGetApproxGradient(problem)">canGetApproxGradient</a>	Checks whether an approximate gradient can be computed for this problem.</li><li><a href="../../../manopt/core/canGetCost.html" class="code" title="function candoit = canGetCost(problem)">canGetCost</a>	Checks whether the cost function can be computed for a problem structure.</li><li><a href="../../../manopt/core/canGetGradient.html" class="code" title="function candoit = canGetGradient(problem)">canGetGradient</a>	Checks whether the gradient can be computed for a problem structure.</li><li><a href="../../../manopt/core/canGetLinesearch.html" class="code" title="function candoit = canGetLinesearch(problem)">canGetLinesearch</a>	Checks whether the problem structure can give a line-search a hint.</li><li><a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>	Computes the cost function and the gradient at x in one call if possible.</li><li><a href="../../../manopt/core/getGlobalDefaults.html" class="code" title="function opts = getGlobalDefaults()">getGlobalDefaults</a>	Returns a structure with default option values for Manopt.</li><li><a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>	Merges two options structures with one having precedence over the other.</li><li><a href="../../../manopt/core/stoppingcriterion.html" class="code" title="function [stop, reason] = stoppingcriterion(problem, x, options, info, last)">stoppingcriterion</a>	Checks for standard stopping criteria, as a helper to solvers.</li><li><a href="../../../manopt/solvers/gradientapproximations/approxgradientFD.html" class="code" title="function gradfun = approxgradientFD(problem, options)">approxgradientFD</a>	Gradient approx. fnctn handle based on finite differences of the cost.</li><li><a href="../../../manopt/solvers/linesearch/linesearch_hint.html" class="code" title="function [stepsize, newx, newkey, lsstats] =linesearch_hint(problem, x, d, f0, df0, options, storedb, key)">linesearch_hint</a>	Armijo line-search based on the line-search hint in the problem structure.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../examples/positive_definite_karcher_mean.html" class="code" title="function X = positive_definite_karcher_mean(A)">positive_definite_karcher_mean</a>	Computes a Karcher mean of a collection of positive definite matrices.</li><li><a href="../../../examples/thomson_problem.html" class="code" title="function X = thomson_problem(n, d)">thomson_problem</a>	Simple attempt at computing n well distributed points on a sphere in R^d.</li><li><a href="../../../examples/using_counters.html" class="code" title="function using_counters()">using_counters</a>	Manopt example on how to use counters during optimization. Typical uses,</li></ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="#_sub1" class="code">function stats = savestats()</a></li><li><a href="#_sub2" class="code">function dir = getDirection(M, xCur, xCurGradient, sHistory, yHistory,</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [x, cost, info, options] = rlbfgs(problem, x0, options)</a>
0002 <span class="comment">% Riemannian limited memory BFGS solver for smooth objective functions.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% function [x, cost, info, options] = rlbfgs(problem)</span>
0005 <span class="comment">% function [x, cost, info, options] = rlbfgs(problem, x0)</span>
0006 <span class="comment">% function [x, cost, info, options] = rlbfgs(problem, x0, options)</span>
0007 <span class="comment">% function [x, cost, info, options] = rlbfgs(problem, [], options)</span>
0008 <span class="comment">%</span>
0009 <span class="comment">%</span>
0010 <span class="comment">% This is a Riemannian limited memory BFGS solver (quasi-Newton method),</span>
0011 <span class="comment">% which aims to minimize the cost function in the given problem structure.</span>
0012 <span class="comment">% It requires access to the gradient of the cost function.</span>
0013 <span class="comment">%</span>
0014 <span class="comment">% Parameter options.memory can be used to specify the number of iterations</span>
0015 <span class="comment">% the algorithm remembers and uses to approximate the inverse Hessian of</span>
0016 <span class="comment">% the cost. Default value is 30.</span>
0017 <span class="comment">% For unlimited memory, set options.memory = Inf.</span>
0018 <span class="comment">%</span>
0019 <span class="comment">%</span>
0020 <span class="comment">% For a description of the algorithm and theorems offering convergence</span>
0021 <span class="comment">% guarantees, see the references below.</span>
0022 <span class="comment">%</span>
0023 <span class="comment">% The initial iterate is x0 if it is provided. Otherwise, a random point on</span>
0024 <span class="comment">% the manifold is picked. To specify options whilst not specifying an</span>
0025 <span class="comment">% initial iterate, give x0 as [] (the empty matrix).</span>
0026 <span class="comment">%</span>
0027 <span class="comment">% The two outputs 'x' and 'cost' are the last reached point on the manifold</span>
0028 <span class="comment">% and its cost.</span>
0029 <span class="comment">%</span>
0030 <span class="comment">% The output 'info' is a struct-array which contains information about the</span>
0031 <span class="comment">% iterations:</span>
0032 <span class="comment">%   iter (integer)</span>
0033 <span class="comment">%       The iteration number. The initial guess is 0.</span>
0034 <span class="comment">%    cost (double)</span>
0035 <span class="comment">%       The corresponding cost value.</span>
0036 <span class="comment">%    gradnorm (double)</span>
0037 <span class="comment">%       The (Riemannian) norm of the gradient.</span>
0038 <span class="comment">%    time (double)</span>
0039 <span class="comment">%       The total elapsed time in seconds to reach the corresponding cost.</span>
0040 <span class="comment">%    stepsize (double)</span>
0041 <span class="comment">%       The size of the step from the previous to the new iterate.</span>
0042 <span class="comment">%   accepted (Boolean)</span>
0043 <span class="comment">%       true if step is accepted in the cautious update. 0 otherwise.</span>
0044 <span class="comment">%   And possibly additional information logged by options.statsfun.</span>
0045 <span class="comment">% For example, type [info.gradnorm] to obtain a vector of the successive</span>
0046 <span class="comment">% gradient norms reached at each iteration.</span>
0047 <span class="comment">%</span>
0048 <span class="comment">% The options structure is used to overwrite the default values. All</span>
0049 <span class="comment">% options have a default value and are hence optional. To force an option</span>
0050 <span class="comment">% value, pass an options structure with a field options.optionname, where</span>
0051 <span class="comment">% optionname is one of the following and the default value is indicated</span>
0052 <span class="comment">% between parentheses:</span>
0053 <span class="comment">%</span>
0054 <span class="comment">%   tolgradnorm (1e-6)</span>
0055 <span class="comment">%       The algorithm terminates if the norm of the gradient drops below</span>
0056 <span class="comment">%       this. For well-scaled problems, a rule of thumb is that you can</span>
0057 <span class="comment">%       expect to reduce the gradient norm by 8 orders of magnitude</span>
0058 <span class="comment">%       (sqrt(eps)) compared to the gradient norm at a &quot;typical&quot; point (a</span>
0059 <span class="comment">%       rough initial iterate for example). Further decrease is sometimes</span>
0060 <span class="comment">%       possible, but inexact floating point arithmetic will eventually</span>
0061 <span class="comment">%       limit the final accuracy. If tolgradnorm is set too low, the</span>
0062 <span class="comment">%       algorithm may end up iterating forever (or at least until another</span>
0063 <span class="comment">%       stopping criterion triggers).</span>
0064 <span class="comment">%   maxiter (1000)</span>
0065 <span class="comment">%       The algorithm terminates if maxiter iterations were executed.</span>
0066 <span class="comment">%   maxtime (Inf)</span>
0067 <span class="comment">%       The algorithm terminates if maxtime seconds elapsed.</span>
0068 <span class="comment">%   minstepsize (1e-10)</span>
0069 <span class="comment">%     The minimum norm of the tangent vector that points from the current</span>
0070 <span class="comment">%     point to the next point. If the norm is less than minstepsize, the</span>
0071 <span class="comment">%     program will terminate.</span>
0072 <span class="comment">%   memory (30)</span>
0073 <span class="comment">%     The number of previous iterations the program remembers. This is used</span>
0074 <span class="comment">%     to approximate the inverse Hessian at the current point. Because of</span>
0075 <span class="comment">%     difficulty of maintaining a representation of operators in terms of</span>
0076 <span class="comment">%     coordinates, a recursive method is used. The number of steps in the</span>
0077 <span class="comment">%     recursion is at most options.memory. This parameter can take any</span>
0078 <span class="comment">%     integer value &gt;= 0, or Inf, which is taken to be options.maxiter. If</span>
0079 <span class="comment">%     options.maxiter has value Inf, then it will take value 10000 and a</span>
0080 <span class="comment">%     warning will be displayed.</span>
0081 <span class="comment">%   strict_inc_func (@(t) 1e-4*t)</span>
0082 <span class="comment">%     The Cautious step needs a real function that has value 0 at t = 0,</span>
0083 <span class="comment">%     and  is strictly increasing. See details in Wen Huang's paper</span>
0084 <span class="comment">%     &quot;A Riemannian BFGS Method without Differentiated Retraction for</span>
0085 <span class="comment">%     Nonconvex Optimization Problems&quot;</span>
0086 <span class="comment">%   statsfun (none)</span>
0087 <span class="comment">%       Function handle to a function that will be called after each</span>
0088 <span class="comment">%       iteration to provide the opportunity to log additional statistics.</span>
0089 <span class="comment">%       They will be returned in the info struct. See the generic Manopt</span>
0090 <span class="comment">%       documentation about solvers for further information. statsfun is</span>
0091 <span class="comment">%       called with the point x that was reached last.</span>
0092 <span class="comment">%   stopfun (none)</span>
0093 <span class="comment">%       Function handle to a function that will be called at each iteration</span>
0094 <span class="comment">%       to provide the opportunity to specify additional stopping criteria.</span>
0095 <span class="comment">%       See the generic Manopt documentation about solvers for further</span>
0096 <span class="comment">%       information.</span>
0097 <span class="comment">%   verbosity (2)</span>
0098 <span class="comment">%       Integer number used to tune the amount of output the algorithm</span>
0099 <span class="comment">%       generates during execution (mostly as text in the command window).</span>
0100 <span class="comment">%       The higher, the more output. 0 means silent. 3 and above includes a</span>
0101 <span class="comment">%       display of the options structure at the beginning of the execution.</span>
0102 <span class="comment">%   debug (false)</span>
0103 <span class="comment">%       Set to true to allow the algorithm to perform additional</span>
0104 <span class="comment">%       computations for debugging purposes. If a debugging test fails, you</span>
0105 <span class="comment">%       will be informed of it, usually via the command window. Be aware</span>
0106 <span class="comment">%       that these additional computations appear in the algorithm timings</span>
0107 <span class="comment">%       too, and may interfere with operations such as counting the number</span>
0108 <span class="comment">%       of cost evaluations, etc. (the debug calls get storedb too).</span>
0109 <span class="comment">%   storedepth (2)</span>
0110 <span class="comment">%       Maximum number of different points x of the manifold for which a</span>
0111 <span class="comment">%       store structure will be kept in memory in the storedb for caching.</span>
0112 <span class="comment">%       If memory usage is an issue, you may try to lower this number.</span>
0113 <span class="comment">%       Profiling may then help to investigate if a performance hit was</span>
0114 <span class="comment">%       incurred as a result.</span>
0115 <span class="comment">%</span>
0116 <span class="comment">%</span>
0117 <span class="comment">% Please cite the Manopt paper as well as the research paper:</span>
0118 <span class="comment">% @InBook{Huang2016,</span>
0119 <span class="comment">%   title     = {A {R}iemannian {BFGS} Method for Nonconvex Optimization Problems},</span>
0120 <span class="comment">%   author    = {Huang, W. and Absil, P.-A. and Gallivan, K.A.},</span>
0121 <span class="comment">%   year      = {2016},</span>
0122 <span class="comment">%   publisher = {Springer International Publishing},</span>
0123 <span class="comment">%   editor    = {Karas{\&quot;o}zen, B{\&quot;u}lent and Manguo{\u{g}}lu, Murat and Tezer-Sezgin, M{\&quot;u}nevver and G{\&quot;o}ktepe, Serdar and U{\u{g}}ur, {\&quot;O}m{\&quot;u}r},</span>
0124 <span class="comment">%   address   = {Cham},</span>
0125 <span class="comment">%   booktitle = {Numerical Mathematics and Advanced Applications ENUMATH 2015},</span>
0126 <span class="comment">%   pages     = {627--634},</span>
0127 <span class="comment">%   doi       = {10.1007/978-3-319-39929-4_60}</span>
0128 <span class="comment">% }</span>
0129 <span class="comment">%</span>
0130 <span class="comment">% We point out that, at the moment, this implementation of RLBFGS can be</span>
0131 <span class="comment">% slower than the implementation in ROPTLIB by Wen Huang et al. referenced</span>
0132 <span class="comment">% above. For the purpose of comparing to their work, please use their</span>
0133 <span class="comment">% implementation.</span>
0134 <span class="comment">%</span>
0135 
0136 
0137 <span class="comment">% This file is part of Manopt: www.manopt.org.</span>
0138 <span class="comment">% Original author: Changshuo Liu, July 19, 2017.</span>
0139 <span class="comment">% Contributors: Nicolas Boumal</span>
0140 <span class="comment">% Change log:</span>
0141 <span class="comment">%</span>
0142 <span class="comment">%   Nov. 27, 2017 (Wen Huang):</span>
0143 <span class="comment">%       Changed the default strict_inc_func to @(t) 1e-4*t from @(t) t.</span>
0144 <span class="comment">%</span>
0145 <span class="comment">%   Jan. 18, 2018 (NB):</span>
0146 <span class="comment">%       Corrected a bug related to the way the line search hint was defined</span>
0147 <span class="comment">%       by default.</span>
0148 <span class="comment">%</span>
0149 <span class="comment">%   Aug. 2, 2018 (NB):</span>
0150 <span class="comment">%       Using the new storedb.remove features to keep storedb lean, and</span>
0151 <span class="comment">%       reduced the default value of storedepth from 30 to 2 as a result.</span>
0152 
0153 
0154     <span class="comment">% Verify that the problem description is sufficient for the solver.</span>
0155     <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetCost.html" class="code" title="function candoit = canGetCost(problem)">canGetCost</a>(problem)
0156         warning(<span class="string">'manopt:getCost'</span>, <span class="keyword">...</span>
0157                 <span class="string">'No cost provided. The algorithm will likely abort.'</span>);
0158     <span class="keyword">end</span>
0159     <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetGradient.html" class="code" title="function candoit = canGetGradient(problem)">canGetGradient</a>(problem) &amp;&amp; ~<a href="../../../manopt/core/canGetApproxGradient.html" class="code" title="function candoit = canGetApproxGradient(problem)">canGetApproxGradient</a>(problem)
0160         <span class="comment">% Note: we do not give a warning if an approximate gradient is</span>
0161         <span class="comment">% explicitly given in the problem description, as in that case the user</span>
0162         <span class="comment">% seems to be aware of the issue.</span>
0163         warning(<span class="string">'manopt:getGradient:approx'</span>, <span class="keyword">...</span>
0164            [<span class="string">'No gradient provided. Using an FD approximation instead (slow).\n'</span> <span class="keyword">...</span>
0165             <span class="string">'It may be necessary to increase options.tolgradnorm.\n'</span> <span class="keyword">...</span>
0166             <span class="string">'To disable this warning: warning(''off'', ''manopt:getGradient:approx'')'</span>]);
0167         problem.approxgrad = <a href="../../../manopt/solvers/gradientapproximations/approxgradientFD.html" class="code" title="function gradfun = approxgradientFD(problem, options)">approxgradientFD</a>(problem);
0168     <span class="keyword">end</span>
0169     
0170     <span class="comment">% This solver uses linesearch_hint as a line search algorithm. By</span>
0171     <span class="comment">% default, try a step size of 1, so that if the BFGS approximation of</span>
0172     <span class="comment">% the Hessian (or inverse Hessian) is good, then the iteration is close</span>
0173     <span class="comment">% to a Newton step.</span>
0174     <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetLinesearch.html" class="code" title="function candoit = canGetLinesearch(problem)">canGetLinesearch</a>(problem)
0175         problem.linesearch = @(x, d) 1;
0176     <span class="keyword">end</span>
0177     
0178     <span class="comment">% Local defaults for the program</span>
0179     localdefaults.minstepsize = 1e-10;
0180     localdefaults.maxiter = 1000;
0181     localdefaults.tolgradnorm = 1e-6;
0182     localdefaults.memory = 30;
0183     localdefaults.strict_inc_func = @(t) 1e-4*t;
0184     localdefaults.ls_max_steps = 25;
0185     localdefaults.storedepth = 2;
0186     
0187     <span class="comment">% Merge global and local defaults, then merge w/ user options, if any.</span>
0188     localdefaults = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>(<a href="../../../manopt/core/getGlobalDefaults.html" class="code" title="function opts = getGlobalDefaults()">getGlobalDefaults</a>(), localdefaults);
0189     <span class="keyword">if</span> ~exist(<span class="string">'options'</span>, <span class="string">'var'</span>) || isempty(options)
0190         options = struct();
0191     <span class="keyword">end</span>
0192     options = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>(localdefaults, options);
0193     
0194     <span class="comment">% To make sure memory in range [0, Inf)</span>
0195     options.memory = max(options.memory, 0);
0196     <span class="keyword">if</span> options.memory == Inf
0197         <span class="keyword">if</span> isinf(options.maxiter)
0198             options.memory = 10000;
0199             warning(<span class="string">'rlbfgs:memory'</span>, [<span class="string">'options.memory and options.maxiter'</span> <span class="keyword">...</span>
0200               <span class="string">' are both Inf; options.memory has been changed to 10000.'</span>]);
0201         <span class="keyword">else</span>
0202             options.memory = options.maxiter;
0203         <span class="keyword">end</span>
0204     <span class="keyword">end</span>
0205     
0206     M = problem.M;
0207     
0208     
0209     timetic = tic();
0210     
0211     
0212     <span class="comment">% Create a random starting point if no starting point is provided.</span>
0213     <span class="keyword">if</span> ~exist(<span class="string">'x0'</span>, <span class="string">'var'</span>)|| isempty(x0)
0214         xCur = M.rand();
0215     <span class="keyword">else</span>
0216         xCur = x0;
0217     <span class="keyword">end</span>
0218     
0219     <span class="comment">% Create a store database and get a key for the current x</span>
0220     storedb = <a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>(options.storedepth);
0221     key = storedb.getNewKey();
0222     
0223     <span class="comment">% __________Initialization of variables______________</span>
0224     <span class="comment">% Number of iterations since the last restart</span>
0225     k = 0;  
0226     <span class="comment">% Total number of BFGS iterations</span>
0227     iter = 0; 
0228     
0229     <span class="comment">% This cell stores step vectors which point from x_{t} to x_{t+1} for t</span>
0230     <span class="comment">% indexing the last iterations, capped at options.memory.</span>
0231     <span class="comment">% That is, it stores up to options.memory of the most recent step</span>
0232     <span class="comment">% vectors. However, the implementation below does not need step vectors</span>
0233     <span class="comment">% in their respective tangent spaces at x_{t}'s. Rather, it requires</span>
0234     <span class="comment">% them transported to the current point's tangent space by vector</span>
0235     <span class="comment">% tranport. For details regarding the requirements on the the vector</span>
0236     <span class="comment">% tranport, see the reference paper by Huang et al.</span>
0237     <span class="comment">% In this implementation, those step vectors are iteratively</span>
0238     <span class="comment">% transported to the current point's tangent space after every</span>
0239     <span class="comment">% iteration. Thus, at every iteration, vectors in sHistory are in the</span>
0240     <span class="comment">% current point's tangent space.</span>
0241     sHistory = cell(1, options.memory);
0242     
0243     <span class="comment">% This cell stores the differences for latest t's of the gradient at</span>
0244     <span class="comment">% x_{t+1} and the gradient at x_{t}, transported to x_{t+1}'s tangent</span>
0245     <span class="comment">% space. The memory is also capped at options.memory.</span>
0246     yHistory = cell(1, options.memory);
0247     
0248     <span class="comment">% rhoHistory{t} stores the reciprocal of the inner product between</span>
0249     <span class="comment">% sHistory{t} and yHistory{t}.</span>
0250     rhoHistory = cell(1, options.memory);
0251     
0252     <span class="comment">% Scaling of direction given by getDirection for acceptable step</span>
0253     alpha = 1; 
0254     
0255     <span class="comment">% Scaling of initial matrix, Barzilai-Borwein.</span>
0256     scaleFactor = 1;
0257     
0258     <span class="comment">% Norm of the step</span>
0259     stepsize = 1;
0260     
0261     <span class="comment">% Stores whether the step is accepted by the cautious update check.</span>
0262     accepted = true;
0263     
0264     <span class="comment">% Query the cost function and its gradient</span>
0265     [xCurCost, xCurGradient] = <a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>(problem, xCur, storedb, key);
0266     
0267     xCurGradNorm = M.norm(xCur, xCurGradient);
0268     
0269     <span class="comment">% Line-search statistics for recording in info.</span>
0270     lsstats = [];
0271     
0272     <span class="comment">% Flag to control restarting scheme to avoid infinite loops (see below)</span>
0273     ultimatum = false;
0274     
0275     <span class="comment">% Save stats in a struct array info, and preallocate.</span>
0276     stats = <a href="#_sub1" class="code" title="subfunction stats = savestats()">savestats</a>();
0277     info(1) = stats;
0278     info(min(10000, options.maxiter+1)).iter = [];
0279     
0280     <span class="keyword">if</span> options.verbosity &gt;= 2
0281         fprintf(<span class="string">' iter                   cost val            grad. norm           alpha\n'</span>);
0282     <span class="keyword">end</span>
0283     
0284     <span class="comment">% Main iteration</span>
0285     <span class="keyword">while</span> true
0286 
0287         <span class="comment">% Display iteration information</span>
0288         <span class="keyword">if</span> options.verbosity &gt;= 2
0289         fprintf(<span class="string">'%5d    %+.16e        %.8e      %.4e\n'</span>, <span class="keyword">...</span>
0290                 iter, xCurCost, xCurGradNorm, alpha);
0291         <span class="keyword">end</span>
0292         
0293         <span class="comment">% Start timing this iteration</span>
0294         timetic = tic();
0295         
0296         <span class="comment">% Run standard stopping criterion checks</span>
0297         [stop, reason] = <a href="../../../manopt/core/stoppingcriterion.html" class="code" title="function [stop, reason] = stoppingcriterion(problem, x, options, info, last)">stoppingcriterion</a>(problem, xCur, options, <span class="keyword">...</span>
0298                                            info, iter+1);
0299         
0300         <span class="comment">% If none triggered, run specific stopping criterion check</span>
0301         <span class="keyword">if</span> ~stop 
0302             <span class="keyword">if</span> stats.stepsize &lt; options.minstepsize
0303                 <span class="comment">% To avoid infinite loop and to push the search further</span>
0304                 <span class="comment">% in case BFGS approximation of Hessian is off towards</span>
0305                 <span class="comment">% the end, we erase the memory by setting k = 0;</span>
0306                 <span class="comment">% In this way, it starts off like a steepest descent.</span>
0307                 <span class="comment">% If even steepest descent does not work, then it is</span>
0308                 <span class="comment">% hopeless and we will terminate.</span>
0309                 <span class="keyword">if</span> ~ultimatum
0310                     <span class="keyword">if</span> options.verbosity &gt;= 2
0311                         fprintf([<span class="string">'stepsize is too small, restarting '</span> <span class="keyword">...</span>
0312                             <span class="string">'the bfgs procedure at the current point.\n'</span>]);
0313                     <span class="keyword">end</span>
0314                     k = 0;
0315                     ultimatum = true;
0316                 <span class="keyword">else</span>
0317                     stop = true;
0318                     reason = sprintf([<span class="string">'Last stepsize smaller than '</span>  <span class="keyword">...</span>
0319                         <span class="string">'minimum allowed; options.minstepsize = %g.'</span>], <span class="keyword">...</span>
0320                         options.minstepsize);
0321                 <span class="keyword">end</span>
0322             <span class="keyword">else</span>
0323                 <span class="comment">% We are not in trouble: lift the ultimatum if it was on.</span>
0324                 ultimatum = false;
0325             <span class="keyword">end</span>
0326         <span class="keyword">end</span>  
0327         
0328         <span class="keyword">if</span> stop
0329             <span class="keyword">if</span> options.verbosity &gt;= 1
0330                 fprintf([reason <span class="string">'\n'</span>]);
0331             <span class="keyword">end</span>
0332             <span class="keyword">break</span>;
0333         <span class="keyword">end</span>
0334 
0335         
0336         <span class="comment">% Compute BFGS direction</span>
0337         p = <a href="#_sub2" class="code" title="subfunction dir = getDirection(M, xCur, xCurGradient, sHistory, yHistory, ">getDirection</a>(M, xCur, xCurGradient, sHistory,<span class="keyword">...</span>
0338                 yHistory, rhoHistory, scaleFactor, min(k, options.memory));
0339 
0340         <span class="comment">% Execute line-search</span>
0341         [stepsize, xNext, newkey, lsstats] = <span class="keyword">...</span>
0342             <a href="../../../manopt/solvers/linesearch/linesearch_hint.html" class="code" title="function [stepsize, newx, newkey, lsstats] =linesearch_hint(problem, x, d, f0, df0, options, storedb, key)">linesearch_hint</a>(problem, xCur, p, xCurCost, <span class="keyword">...</span>
0343                             M.inner(xCur, xCurGradient, p), <span class="keyword">...</span>
0344                             options, storedb, key);
0345         
0346         <span class="comment">% Record the BFGS step-multiplier alpha which was effectively</span>
0347         <span class="comment">% selected. Toward convergence, we hope to see alpha = 1.</span>
0348         alpha = stepsize/M.norm(xCur, p);
0349         step = M.lincomb(xCur, alpha, p);
0350         
0351         
0352         <span class="comment">% Query cost and gradient at the candidate new point.</span>
0353         [xNextCost, xNextGrad] = <a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>(problem, xNext, storedb, newkey);
0354         
0355         <span class="comment">% Compute sk and yk</span>
0356         sk = M.transp(xCur, xNext, step);
0357         yk = M.lincomb(xNext, 1, xNextGrad, <span class="keyword">...</span>
0358                              -1, M.transp(xCur, xNext, xCurGradient));
0359 
0360         <span class="comment">% Computation of the BFGS step is invariant under scaling of sk and</span>
0361         <span class="comment">% yk by a common factor. For numerical reasons, we scale sk and yk</span>
0362         <span class="comment">% so that sk is a unit norm vector.</span>
0363         norm_sk = M.norm(xNext, sk);
0364         sk = M.lincomb(xNext, 1/norm_sk, sk);
0365         yk = M.lincomb(xNext, 1/norm_sk, yk);
0366         
0367         inner_sk_yk = M.inner(xNext, sk, yk);
0368         inner_sk_sk = M.norm(xNext, sk)^2;    <span class="comment">% ensures nonnegativity</span>
0369         
0370         
0371         <span class="comment">% If the cautious step is accepted (which is the intended</span>
0372         <span class="comment">% behavior), we record sk, yk and rhok and need to do some</span>
0373         <span class="comment">% housekeeping. If the cautious step is rejected, these are not</span>
0374         <span class="comment">% recorded. In all cases, xNext is the next iterate: the notion of</span>
0375         <span class="comment">% accept/reject here is limited to whether or not we keep track of</span>
0376         <span class="comment">% sk, yk, rhok to update the BFGS operator.</span>
0377         cap = options.strict_inc_func(xCurGradNorm);
0378         <span class="keyword">if</span> inner_sk_sk ~= 0 &amp;&amp; (inner_sk_yk / inner_sk_sk) &gt;= cap
0379             
0380             accepted = true;
0381             
0382             rhok = 1/inner_sk_yk;
0383             
0384             scaleFactor = inner_sk_yk / M.norm(xNext, yk)^2;
0385             
0386             <span class="comment">% Time to store the vectors sk, yk and the scalar rhok.</span>
0387             <span class="comment">% Remember: we need to transport all vectors to the most</span>
0388             <span class="comment">% current tangent space.</span>
0389             
0390             <span class="comment">% If we are out of memory</span>
0391             <span class="keyword">if</span> k &gt;= options.memory
0392                 
0393                 <span class="comment">% sk and yk are saved from 1 to the end with the most</span>
0394                 <span class="comment">% current recorded to the rightmost hand side of the cells</span>
0395                 <span class="comment">% that are occupied. When memory is full, do a shift so</span>
0396                 <span class="comment">% that the rightmost is earliest and replace it with the</span>
0397                 <span class="comment">% most recent sk, yk.</span>
0398                 <span class="keyword">for</span>  i = 2 : options.memory
0399                     sHistory{i} = M.transp(xCur, xNext, sHistory{i});
0400                     yHistory{i} = M.transp(xCur, xNext, yHistory{i});
0401                 <span class="keyword">end</span>
0402                 <span class="keyword">if</span> options.memory &gt; 1
0403                     sHistory = sHistory([2:<span class="keyword">end</span>, 1]);
0404                     yHistory = yHistory([2:<span class="keyword">end</span>, 1]);
0405                     rhoHistory = rhoHistory([2:end 1]);
0406                 <span class="keyword">end</span>
0407                 <span class="keyword">if</span> options.memory &gt; 0
0408                     sHistory{options.memory} = sk;
0409                     yHistory{options.memory} = yk;
0410                     rhoHistory{options.memory} = rhok;
0411                 <span class="keyword">end</span>
0412                 
0413             <span class="comment">% If we are not out of memory</span>
0414             <span class="keyword">else</span>
0415                 
0416                 <span class="keyword">for</span> i = 1:k
0417                     sHistory{i} = M.transp(xCur, xNext, sHistory{i});
0418                     yHistory{i} = M.transp(xCur, xNext, yHistory{i});
0419                 <span class="keyword">end</span>
0420                 sHistory{k+1} = sk;
0421                 yHistory{k+1} = yk;
0422                 rhoHistory{k+1} = rhok;
0423                 
0424             <span class="keyword">end</span>
0425             
0426             k = k + 1;
0427             
0428         <span class="comment">% The cautious step is rejected: we do not store sk, yk, rhok but</span>
0429         <span class="comment">% we still need to transport stored vectors to the new tangent</span>
0430         <span class="comment">% space.</span>
0431         <span class="keyword">else</span>
0432             
0433             accepted = false;
0434             
0435             <span class="keyword">for</span>  i = 1 : min(k, options.memory)
0436                 sHistory{i} = M.transp(xCur, xNext, sHistory{i});
0437                 yHistory{i} = M.transp(xCur, xNext, yHistory{i});
0438             <span class="keyword">end</span>
0439             
0440         <span class="keyword">end</span>
0441         
0442         <span class="comment">% Update variables to new iterate.</span>
0443         storedb.removefirstifdifferent(key, newkey);
0444         xCur = xNext;
0445         key = newkey;
0446         xCurGradient = xNextGrad;
0447         xCurGradNorm = M.norm(xNext, xNextGrad);
0448         xCurCost = xNextCost;
0449         
0450         <span class="comment">% iter is the number of iterations we have accomplished.</span>
0451         iter = iter + 1;
0452         
0453         <span class="comment">% Make sure we don't use too much memory for the store database</span>
0454         <span class="comment">% (this is independent from the BFGS memory.)</span>
0455         storedb.purge();
0456         
0457         
0458         <span class="comment">% Log statistics for freshly executed iteration</span>
0459         stats = <a href="#_sub1" class="code" title="subfunction stats = savestats()">savestats</a>();
0460         info(iter+1) = stats; 
0461         
0462     <span class="keyword">end</span>
0463 
0464     
0465     <span class="comment">% Housekeeping before we return</span>
0466     info = info(1:iter+1);
0467     x = xCur;
0468     cost = xCurCost;
0469 
0470     <span class="keyword">if</span> options.verbosity &gt;= 1
0471         fprintf(<span class="string">'Total time is %f [s] (excludes statsfun)\n'</span>, <span class="keyword">...</span>
0472                 info(end).time);
0473     <span class="keyword">end</span>
0474 
0475     
0476     <span class="comment">% Routine in charge of collecting the current iteration stats</span>
0477     <a name="_sub1" href="#_subfunctions" class="code">function stats = savestats()</a>
0478         stats.iter = iter;
0479         stats.cost = xCurCost;
0480         stats.gradnorm = xCurGradNorm;
0481         <span class="keyword">if</span> iter == 0
0482             stats.stepsize = NaN;
0483             stats.time = toc(timetic);
0484             stats.accepted = NaN;
0485         <span class="keyword">else</span>
0486             stats.stepsize = stepsize;
0487             stats.time = info(iter).time + toc(timetic);
0488             stats.accepted = accepted;
0489         <span class="keyword">end</span>
0490         stats.linesearch = lsstats;
0491         stats = <a href="../../../manopt/core/applyStatsfun.html" class="code" title="function stats = applyStatsfun(problem, x, storedb, key, options, stats)">applyStatsfun</a>(problem, xCur, storedb, key, options, stats);
0492     <span class="keyword">end</span>
0493 
0494 <span class="keyword">end</span>
0495 
0496 
0497 
0498 
0499 <span class="comment">% BFGS step, see Wen's paper for details. This functon takes in a tangent</span>
0500 <span class="comment">% vector g, and applies an approximate inverse Hessian P to it to get Pg.</span>
0501 <span class="comment">% Then, -Pg is returned.</span>
0502 <span class="comment">%</span>
0503 <span class="comment">% Theory requires the vector transport to be isometric and to satisfy the</span>
0504 <span class="comment">% locking condition (see paper), but these properties do not seem to be</span>
0505 <span class="comment">% crucial in practice. If your manifold provides M.isotransp, it may be</span>
0506 <span class="comment">% good to do M.transp = M.isotransp; after loading M with a factory.</span>
0507 <span class="comment">%</span>
0508 <span class="comment">% This implementation operates in the tangent space of the most recent</span>
0509 <span class="comment">% point since all vectors in sHistory and yHistory have been transported</span>
0510 <span class="comment">% there.</span>
0511 <a name="_sub2" href="#_subfunctions" class="code">function dir = getDirection(M, xCur, xCurGradient, sHistory, yHistory, </a><span class="keyword">...</span>
0512                             rhoHistory, scaleFactor, k)
0513     
0514     q = xCurGradient;
0515     
0516     inner_s_q = zeros(1, k);
0517     
0518     <span class="keyword">for</span> i = k : -1 : 1
0519         inner_s_q(1, i) = rhoHistory{i} * M.inner(xCur, sHistory{i}, q);
0520         q = M.lincomb(xCur, 1, q, -inner_s_q(1, i), yHistory{i});
0521     <span class="keyword">end</span>
0522     
0523     r = M.lincomb(xCur, scaleFactor, q);
0524     
0525     <span class="keyword">for</span> i = 1 : k
0526          omega = rhoHistory{i} * M.inner(xCur, yHistory{i}, r);
0527          r = M.lincomb(xCur, 1, r, inner_s_q(1, i)-omega, sHistory{i});
0528     <span class="keyword">end</span>
0529     
0530     dir = M.lincomb(xCur, -1, r);
0531     
0532 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Mon 10-Sep-2018 11:48:06 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>