<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of rlbfgs</title>
  <meta name="keywords" content="rlbfgs">
  <meta name="description" content="Riemannian limited memory BFGS solver for smooth objective functions.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../index.html">Home</a> &gt;  <a href="#">manopt</a> &gt; <a href="#">solvers</a> &gt; <a href="index.html">bfgs</a> &gt; rlbfgs.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../index.html"><img alt="<" border="0" src="../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for manopt\solvers\bfgs&nbsp;<img alt=">" border="0" src="../../../right.png"></a></td></tr></table>-->

<h1>rlbfgs
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>Riemannian limited memory BFGS solver for smooth objective functions.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>function [x, cost, info, options] = rlbfgs(problem, x0, options) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Riemannian limited memory BFGS solver for smooth objective functions.

 function [x, cost, info, options] = rlbfgs(problem)
 function [x, cost, info, options] = rlbfgs(problem, x0)
 function [x, cost, info, options] = rlbfgs(problem, x0, options)
 function [x, cost, info, options] = rlbfgs(problem, [], options)


 This is a Riemannian limited memory BFGS solver (quasi-Newton method), 
 which aims to minimize the cost function in the given problem structure.
 It requires access to the gradient of the cost function.

 Parameter options.memory can be used to specify the number of iterations
 the algorithm remembers and uses to approximate the inverse Hessian of
 the cost. Default value is 30.
 For unlimited memory, set options.memory = Inf.


 For a description of the algorithm and theorems offering convergence
 guarantees, see the references below.

 The initial iterate is x0 if it is provided. Otherwise, a random point on
 the manifold is picked. To specify options whilst not specifying an
 initial iterate, give x0 as [] (the empty matrix).

 The two outputs 'x' and 'cost' are the last reached point on the manifold
 and its cost. 
 
 The output 'info' is a struct-array which contains information about the
 iterations:
   iter (integer)
       The iteration number. The initial guess is 0.
    cost (double)
       The corresponding cost value.
    gradnorm (double)
       The (Riemannian) norm of the gradient.
    time (double)
       The total elapsed time in seconds to reach the corresponding cost.
    stepsize (double)
       The size of the step from the previous to the new iterate.
   accepted (Boolean)
       true if step is accepted in the cautious update. 0 otherwise.
   And possibly additional information logged by options.statsfun.
 For example, type [info.gradnorm] to obtain a vector of the successive
 gradient norms reached at each iteration.

 The options structure is used to overwrite the default values. All
 options have a default value and are hence optional. To force an option
 value, pass an options structure with a field options.optionname, where
 optionname is one of the following and the default value is indicated
 between parentheses:

   tolgradnorm (1e-6)
       The algorithm terminates if the norm of the gradient drops below
       this. For well-scaled problems, a rule of thumb is that you can
       expect to reduce the gradient norm by 8 orders of magnitude
       (sqrt(eps)) compared to the gradient norm at a &quot;typical&quot; point (a
       rough initial iterate for example). Further decrease is sometimes
       possible, but inexact floating point arithmetic will eventually
       limit the final accuracy. If tolgradnorm is set too low, the
       algorithm may end up iterating forever (or at least until another
       stopping criterion triggers).
   maxiter (1000)
       The algorithm terminates if maxiter iterations were executed.
   maxtime (Inf)
       The algorithm terminates if maxtime seconds elapsed.
   minstepsize (1e-10)
     The minimum norm of the tangent vector that points from the current
     point to the next point. If the norm is less than minstepsize, the 
     program will terminate.
   memory (30)
     The number of previous iterations the program remembers. This is used 
     to approximate the inverse Hessian at the current point. Because of
     difficulty of maintaining a representation of operators in terms of
     coordinates, a recursive method is used. The number of steps in the
     recursion is at most options.memory. This parameter can take any
     integer value &gt;= 0, or Inf, which is taken to be options.maxiter. If
     options.maxiter has value Inf, then it will take value 10000 and a
     warning will be displayed.
   linesearch (@linesearch_hint)
       Function handle to a line search function. The options structure is
       passed to the line search too, so you can pass it parameters. See
       each line search's documentation for info.
       By default, the intial multiplier tried is alpha = 1. This can be
       changed with options.linesearch: see help of linesearch_hint.
   strict_inc_func (@(t) t)
     The Cautious step needs a real function that has value 0 at t = 0,
     and  is strictly increasing. See details in Wen Huang's paper
     &quot;A Riemannian BFGS Method without Differentiated Retraction for 
     Nonconvex Optimization Problems&quot;
   statsfun (none)
       Function handle to a function that will be called after each
       iteration to provide the opportunity to log additional statistics.
       They will be returned in the info struct. See the generic Manopt
       documentation about solvers for further information. statsfun is
       called with the point x that was reached last.
   stopfun (none)
       Function handle to a function that will be called at each iteration
       to provide the opportunity to specify additional stopping criteria.
       See the generic Manopt documentation about solvers for further
       information.
   verbosity (2)
       Integer number used to tune the amount of output the algorithm
       generates during execution (mostly as text in the command window).
       The higher, the more output. 0 means silent. 3 and above includes a
       display of the options structure at the beginning of the execution.
   debug (false)
       Set to true to allow the algorithm to perform additional
       computations for debugging purposes. If a debugging test fails, you
       will be informed of it, usually via the command window. Be aware
       that these additional computations appear in the algorithm timings
       too, and may interfere with operations such as counting the number
       of cost evaluations, etc. (the debug calls get storedb too).
   storedepth (30)
       Maximum number of different points x of the manifold for which a
       store structure will be kept in memory in the storedb. If the
       caching features of Manopt are not used, this is irrelevant. If
       memory usage is an issue, you may try to lower this number.
       Profiling may then help to investigate if a performance hit was
       incurred as a result.


 Please cite the Manopt paper as well as the research paper:
 @InBook{Huang2016,
   title     = {A {R}iemannian {BFGS} Method for Nonconvex Optimization Problems},
   author    = {Huang, W. and Absil, P.-A. and Gallivan, K.A.},
   year      = {2016},
   publisher = {Springer International Publishing},
   editor    = {Karas{\&quot;o}zen, B{\&quot;u}lent and Manguo{\u{g}}lu, Murat and Tezer-Sezgin, M{\&quot;u}nevver and G{\&quot;o}ktepe, Serdar and U{\u{g}}ur, {\&quot;O}m{\&quot;u}r},
   address   = {Cham},
   booktitle = {Numerical Mathematics and Advanced Applications ENUMATH 2015},
   pages     = {627--634},
   doi       = {10.1007/978-3-319-39929-4_60}
 }</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>	</li><li><a href="../../../manopt/core/applyStatsfun.html" class="code" title="function stats = applyStatsfun(problem, x, storedb, key, options, stats)">applyStatsfun</a>	Apply the statsfun function to a stats structure (for solvers).</li><li><a href="../../../manopt/core/canGetApproxGradient.html" class="code" title="function candoit = canGetApproxGradient(problem)">canGetApproxGradient</a>	Checks whether an approximate gradient can be computed for this problem.</li><li><a href="../../../manopt/core/canGetCost.html" class="code" title="function candoit = canGetCost(problem)">canGetCost</a>	Checks whether the cost function can be computed for a problem structure.</li><li><a href="../../../manopt/core/canGetGradient.html" class="code" title="function candoit = canGetGradient(problem)">canGetGradient</a>	Checks whether the gradient can be computed for a problem structure.</li><li><a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>	Computes the cost function and the gradient at x in one call if possible.</li><li><a href="../../../manopt/core/getGlobalDefaults.html" class="code" title="function opts = getGlobalDefaults()">getGlobalDefaults</a>	Returns a structure with default option values for Manopt.</li><li><a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts1, opts2)">mergeOptions</a>	Merges two options structures with one having precedence over the other.</li><li><a href="../../../manopt/core/stoppingcriterion.html" class="code" title="function [stop, reason] = stoppingcriterion(problem, x, options, info, last)">stoppingcriterion</a>	Checks for standard stopping criteria, as a helper to solvers.</li><li><a href="../../../manopt/solvers/gradientapproximations/approxgradientFD.html" class="code" title="function gradfun = approxgradientFD(problem, options)">approxgradientFD</a>	Gradient approx. fnctn handle based on finite differences of the cost.</li><li><a href="../../../manopt/solvers/linesearch/linesearch_hint.html" class="code" title="function [stepsize, newx, newkey, lsstats] =linesearch_hint(problem, x, d, f0, df0, options, storedb, key)">linesearch_hint</a>	Armijo line-search based on the line-search hint in the problem structure.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../examples/positive_definite_karcher_mean.html" class="code" title="function X = positive_definite_karcher_mean(A)">positive_definite_karcher_mean</a>	Computes a Karcher mean of a collection of positive definite matrices.</li><li><a href="../../../examples/thomson_problem.html" class="code" title="function X = thomson_problem(n, d)">thomson_problem</a>	Simple attempt at computing n well distributed points on a sphere in R^d.</li></ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="#_sub1" class="code">function stats = savestats()</a></li><li><a href="#_sub2" class="code">function dir = getDirection(M, xCur, xCurGradient, sHistory, yHistory,</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [x, cost, info, options] = rlbfgs(problem, x0, options)</a>
0002 <span class="comment">% Riemannian limited memory BFGS solver for smooth objective functions.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% function [x, cost, info, options] = rlbfgs(problem)</span>
0005 <span class="comment">% function [x, cost, info, options] = rlbfgs(problem, x0)</span>
0006 <span class="comment">% function [x, cost, info, options] = rlbfgs(problem, x0, options)</span>
0007 <span class="comment">% function [x, cost, info, options] = rlbfgs(problem, [], options)</span>
0008 <span class="comment">%</span>
0009 <span class="comment">%</span>
0010 <span class="comment">% This is a Riemannian limited memory BFGS solver (quasi-Newton method),</span>
0011 <span class="comment">% which aims to minimize the cost function in the given problem structure.</span>
0012 <span class="comment">% It requires access to the gradient of the cost function.</span>
0013 <span class="comment">%</span>
0014 <span class="comment">% Parameter options.memory can be used to specify the number of iterations</span>
0015 <span class="comment">% the algorithm remembers and uses to approximate the inverse Hessian of</span>
0016 <span class="comment">% the cost. Default value is 30.</span>
0017 <span class="comment">% For unlimited memory, set options.memory = Inf.</span>
0018 <span class="comment">%</span>
0019 <span class="comment">%</span>
0020 <span class="comment">% For a description of the algorithm and theorems offering convergence</span>
0021 <span class="comment">% guarantees, see the references below.</span>
0022 <span class="comment">%</span>
0023 <span class="comment">% The initial iterate is x0 if it is provided. Otherwise, a random point on</span>
0024 <span class="comment">% the manifold is picked. To specify options whilst not specifying an</span>
0025 <span class="comment">% initial iterate, give x0 as [] (the empty matrix).</span>
0026 <span class="comment">%</span>
0027 <span class="comment">% The two outputs 'x' and 'cost' are the last reached point on the manifold</span>
0028 <span class="comment">% and its cost.</span>
0029 <span class="comment">%</span>
0030 <span class="comment">% The output 'info' is a struct-array which contains information about the</span>
0031 <span class="comment">% iterations:</span>
0032 <span class="comment">%   iter (integer)</span>
0033 <span class="comment">%       The iteration number. The initial guess is 0.</span>
0034 <span class="comment">%    cost (double)</span>
0035 <span class="comment">%       The corresponding cost value.</span>
0036 <span class="comment">%    gradnorm (double)</span>
0037 <span class="comment">%       The (Riemannian) norm of the gradient.</span>
0038 <span class="comment">%    time (double)</span>
0039 <span class="comment">%       The total elapsed time in seconds to reach the corresponding cost.</span>
0040 <span class="comment">%    stepsize (double)</span>
0041 <span class="comment">%       The size of the step from the previous to the new iterate.</span>
0042 <span class="comment">%   accepted (Boolean)</span>
0043 <span class="comment">%       true if step is accepted in the cautious update. 0 otherwise.</span>
0044 <span class="comment">%   And possibly additional information logged by options.statsfun.</span>
0045 <span class="comment">% For example, type [info.gradnorm] to obtain a vector of the successive</span>
0046 <span class="comment">% gradient norms reached at each iteration.</span>
0047 <span class="comment">%</span>
0048 <span class="comment">% The options structure is used to overwrite the default values. All</span>
0049 <span class="comment">% options have a default value and are hence optional. To force an option</span>
0050 <span class="comment">% value, pass an options structure with a field options.optionname, where</span>
0051 <span class="comment">% optionname is one of the following and the default value is indicated</span>
0052 <span class="comment">% between parentheses:</span>
0053 <span class="comment">%</span>
0054 <span class="comment">%   tolgradnorm (1e-6)</span>
0055 <span class="comment">%       The algorithm terminates if the norm of the gradient drops below</span>
0056 <span class="comment">%       this. For well-scaled problems, a rule of thumb is that you can</span>
0057 <span class="comment">%       expect to reduce the gradient norm by 8 orders of magnitude</span>
0058 <span class="comment">%       (sqrt(eps)) compared to the gradient norm at a &quot;typical&quot; point (a</span>
0059 <span class="comment">%       rough initial iterate for example). Further decrease is sometimes</span>
0060 <span class="comment">%       possible, but inexact floating point arithmetic will eventually</span>
0061 <span class="comment">%       limit the final accuracy. If tolgradnorm is set too low, the</span>
0062 <span class="comment">%       algorithm may end up iterating forever (or at least until another</span>
0063 <span class="comment">%       stopping criterion triggers).</span>
0064 <span class="comment">%   maxiter (1000)</span>
0065 <span class="comment">%       The algorithm terminates if maxiter iterations were executed.</span>
0066 <span class="comment">%   maxtime (Inf)</span>
0067 <span class="comment">%       The algorithm terminates if maxtime seconds elapsed.</span>
0068 <span class="comment">%   minstepsize (1e-10)</span>
0069 <span class="comment">%     The minimum norm of the tangent vector that points from the current</span>
0070 <span class="comment">%     point to the next point. If the norm is less than minstepsize, the</span>
0071 <span class="comment">%     program will terminate.</span>
0072 <span class="comment">%   memory (30)</span>
0073 <span class="comment">%     The number of previous iterations the program remembers. This is used</span>
0074 <span class="comment">%     to approximate the inverse Hessian at the current point. Because of</span>
0075 <span class="comment">%     difficulty of maintaining a representation of operators in terms of</span>
0076 <span class="comment">%     coordinates, a recursive method is used. The number of steps in the</span>
0077 <span class="comment">%     recursion is at most options.memory. This parameter can take any</span>
0078 <span class="comment">%     integer value &gt;= 0, or Inf, which is taken to be options.maxiter. If</span>
0079 <span class="comment">%     options.maxiter has value Inf, then it will take value 10000 and a</span>
0080 <span class="comment">%     warning will be displayed.</span>
0081 <span class="comment">%   linesearch (@linesearch_hint)</span>
0082 <span class="comment">%       Function handle to a line search function. The options structure is</span>
0083 <span class="comment">%       passed to the line search too, so you can pass it parameters. See</span>
0084 <span class="comment">%       each line search's documentation for info.</span>
0085 <span class="comment">%       By default, the intial multiplier tried is alpha = 1. This can be</span>
0086 <span class="comment">%       changed with options.linesearch: see help of linesearch_hint.</span>
0087 <span class="comment">%   strict_inc_func (@(t) t)</span>
0088 <span class="comment">%     The Cautious step needs a real function that has value 0 at t = 0,</span>
0089 <span class="comment">%     and  is strictly increasing. See details in Wen Huang's paper</span>
0090 <span class="comment">%     &quot;A Riemannian BFGS Method without Differentiated Retraction for</span>
0091 <span class="comment">%     Nonconvex Optimization Problems&quot;</span>
0092 <span class="comment">%   statsfun (none)</span>
0093 <span class="comment">%       Function handle to a function that will be called after each</span>
0094 <span class="comment">%       iteration to provide the opportunity to log additional statistics.</span>
0095 <span class="comment">%       They will be returned in the info struct. See the generic Manopt</span>
0096 <span class="comment">%       documentation about solvers for further information. statsfun is</span>
0097 <span class="comment">%       called with the point x that was reached last.</span>
0098 <span class="comment">%   stopfun (none)</span>
0099 <span class="comment">%       Function handle to a function that will be called at each iteration</span>
0100 <span class="comment">%       to provide the opportunity to specify additional stopping criteria.</span>
0101 <span class="comment">%       See the generic Manopt documentation about solvers for further</span>
0102 <span class="comment">%       information.</span>
0103 <span class="comment">%   verbosity (2)</span>
0104 <span class="comment">%       Integer number used to tune the amount of output the algorithm</span>
0105 <span class="comment">%       generates during execution (mostly as text in the command window).</span>
0106 <span class="comment">%       The higher, the more output. 0 means silent. 3 and above includes a</span>
0107 <span class="comment">%       display of the options structure at the beginning of the execution.</span>
0108 <span class="comment">%   debug (false)</span>
0109 <span class="comment">%       Set to true to allow the algorithm to perform additional</span>
0110 <span class="comment">%       computations for debugging purposes. If a debugging test fails, you</span>
0111 <span class="comment">%       will be informed of it, usually via the command window. Be aware</span>
0112 <span class="comment">%       that these additional computations appear in the algorithm timings</span>
0113 <span class="comment">%       too, and may interfere with operations such as counting the number</span>
0114 <span class="comment">%       of cost evaluations, etc. (the debug calls get storedb too).</span>
0115 <span class="comment">%   storedepth (30)</span>
0116 <span class="comment">%       Maximum number of different points x of the manifold for which a</span>
0117 <span class="comment">%       store structure will be kept in memory in the storedb. If the</span>
0118 <span class="comment">%       caching features of Manopt are not used, this is irrelevant. If</span>
0119 <span class="comment">%       memory usage is an issue, you may try to lower this number.</span>
0120 <span class="comment">%       Profiling may then help to investigate if a performance hit was</span>
0121 <span class="comment">%       incurred as a result.</span>
0122 <span class="comment">%</span>
0123 <span class="comment">%</span>
0124 <span class="comment">% Please cite the Manopt paper as well as the research paper:</span>
0125 <span class="comment">% @InBook{Huang2016,</span>
0126 <span class="comment">%   title     = {A {R}iemannian {BFGS} Method for Nonconvex Optimization Problems},</span>
0127 <span class="comment">%   author    = {Huang, W. and Absil, P.-A. and Gallivan, K.A.},</span>
0128 <span class="comment">%   year      = {2016},</span>
0129 <span class="comment">%   publisher = {Springer International Publishing},</span>
0130 <span class="comment">%   editor    = {Karas{\&quot;o}zen, B{\&quot;u}lent and Manguo{\u{g}}lu, Murat and Tezer-Sezgin, M{\&quot;u}nevver and G{\&quot;o}ktepe, Serdar and U{\u{g}}ur, {\&quot;O}m{\&quot;u}r},</span>
0131 <span class="comment">%   address   = {Cham},</span>
0132 <span class="comment">%   booktitle = {Numerical Mathematics and Advanced Applications ENUMATH 2015},</span>
0133 <span class="comment">%   pages     = {627--634},</span>
0134 <span class="comment">%   doi       = {10.1007/978-3-319-39929-4_60}</span>
0135 <span class="comment">% }</span>
0136 <span class="comment">%</span>
0137 
0138 
0139 <span class="comment">% This file is part of Manopt: www.manopt.org.</span>
0140 <span class="comment">% Original author: Changshuo Liu, July 19, 2017.</span>
0141 <span class="comment">% Contributors: Nicolas Boumal</span>
0142 <span class="comment">% Change log:</span>
0143 
0144 
0145     <span class="comment">% Verify that the problem description is sufficient for the solver.</span>
0146     <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetCost.html" class="code" title="function candoit = canGetCost(problem)">canGetCost</a>(problem)
0147         warning(<span class="string">'manopt:getCost'</span>, <span class="keyword">...</span>
0148             <span class="string">'No cost provided. The algorithm will likely abort.'</span>);
0149     <span class="keyword">end</span>
0150     <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetGradient.html" class="code" title="function candoit = canGetGradient(problem)">canGetGradient</a>(problem) &amp;&amp; ~<a href="../../../manopt/core/canGetApproxGradient.html" class="code" title="function candoit = canGetApproxGradient(problem)">canGetApproxGradient</a>(problem)
0151         <span class="comment">% Note: we do not give a warning if an approximate gradient is</span>
0152         <span class="comment">% explicitly given in the problem description, as in that case the user</span>
0153         <span class="comment">% seems to be aware of the issue.</span>
0154         warning(<span class="string">'manopt:getGradient:approx'</span>, <span class="keyword">...</span>
0155            [<span class="string">'No gradient provided. Using an FD approximation instead (slow).\n'</span> <span class="keyword">...</span>
0156             <span class="string">'It may be necessary to increase options.tolgradnorm.\n'</span> <span class="keyword">...</span>
0157             <span class="string">'To disable this warning: warning(''off'', ''manopt:getGradient:approx'')'</span>]);
0158         problem.approxgrad = <a href="../../../manopt/solvers/gradientapproximations/approxgradientFD.html" class="code" title="function gradfun = approxgradientFD(problem, options)">approxgradientFD</a>(problem);
0159     <span class="keyword">end</span>
0160     
0161     <span class="comment">% Local defaults for the program</span>
0162     localdefaults.minstepsize = 1e-10;
0163     localdefaults.maxiter = 1000;
0164     localdefaults.tolgradnorm = 1e-6;
0165     localdefaults.memory = 30;
0166     localdefaults.strict_inc_func = @(t) t;
0167     localdefaults.ls_max_steps  = 25;
0168     localdefaults.storedepth = 30;
0169     localdefaults.linesearch = @<a href="../../../manopt/solvers/linesearch/linesearch_hint.html" class="code" title="function [stepsize, newx, newkey, lsstats] =linesearch_hint(problem, x, d, f0, df0, options, storedb, key)">linesearch_hint</a>;
0170     
0171     <span class="comment">% Merge global and local defaults, then merge w/ user options, if any.</span>
0172     localdefaults = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts1, opts2)">mergeOptions</a>(<a href="../../../manopt/core/getGlobalDefaults.html" class="code" title="function opts = getGlobalDefaults()">getGlobalDefaults</a>(), localdefaults);
0173     <span class="keyword">if</span> ~exist(<span class="string">'options'</span>, <span class="string">'var'</span>) || isempty(options)
0174         options = struct();
0175     <span class="keyword">end</span>
0176     options = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts1, opts2)">mergeOptions</a>(localdefaults, options);
0177     
0178     <span class="comment">% To make sure memory in range [0, Inf)</span>
0179     options.memory = max(options.memory, 0);
0180     <span class="keyword">if</span> options.memory == Inf
0181         <span class="keyword">if</span> isinf(options.maxiter)
0182             options.memory = 10000;
0183             warning(<span class="string">'rlbfgs:memory'</span>, [<span class="string">'options.memory and options.maxiter'</span> <span class="keyword">...</span>
0184               <span class="string">' are both Inf; options.memory has been changed to 10000.'</span>]);
0185         <span class="keyword">else</span>
0186             options.memory = options.maxiter;
0187         <span class="keyword">end</span>
0188     <span class="keyword">end</span>
0189     
0190     M = problem.M;
0191     
0192     <span class="comment">% Create a random starting point if no starting point is provided.</span>
0193     <span class="keyword">if</span> ~exist(<span class="string">'x0'</span>, <span class="string">'var'</span>)|| isempty(x0)
0194         xCur = M.rand(); 
0195     <span class="keyword">else</span>
0196         xCur = x0;
0197     <span class="keyword">end</span>
0198     
0199     timetic = tic();
0200     
0201     <span class="comment">% Create a store database and get a key for the current x</span>
0202     storedb = <a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>(options.storedepth);
0203     key = storedb.getNewKey();
0204     
0205     <span class="comment">% __________Initialization of variables______________</span>
0206     <span class="comment">% Number of iterations since the last restart</span>
0207     k = 0;  
0208     <span class="comment">% Total number of BFGS iterations</span>
0209     iter = 0; 
0210     
0211     <span class="comment">% This cell stores step vectors which point from x_{t} to x_{t+1} for t</span>
0212     <span class="comment">% indexing the last iterations, capped at options.memory.</span>
0213     <span class="comment">% That is, it stores up to options.memory of the most recent step</span>
0214     <span class="comment">% vectors. However, the implementation below does not need step vectors</span>
0215     <span class="comment">% in their respective tangent spaces at x_{t}'s. Rather, it requires</span>
0216     <span class="comment">% them transported to the current point's tangent space by vector</span>
0217     <span class="comment">% tranport. For details regarding the requirements on the the vector</span>
0218     <span class="comment">% tranport, see the reference paper by Huang et al.</span>
0219     <span class="comment">% In this implementation, those step vectors are iteratively</span>
0220     <span class="comment">% transported to the current point's tangent space after every</span>
0221     <span class="comment">% iteration. Thus, at every iteration, vectors in sHistory are in the</span>
0222     <span class="comment">% current point's tangent space.</span>
0223     sHistory = cell(1, options.memory);
0224     
0225     <span class="comment">% This cell stores the differences for latest t's of the gradient at</span>
0226     <span class="comment">% x_{t+1} and the gradient at x_{t}, transported to x_{t+1}'s tangent</span>
0227     <span class="comment">% space. The memory is also capped at options.memory.</span>
0228     yHistory = cell(1, options.memory);
0229     
0230     <span class="comment">% rhoHistory{t} stores the reciprocal of the inner product between</span>
0231     <span class="comment">% sHistory{t} and yHistory{t}.</span>
0232     rhoHistory = cell(1, options.memory);
0233     
0234     <span class="comment">% Scaling of direction given by getDirection for acceptable step</span>
0235     alpha = 1; 
0236     
0237     <span class="comment">% Scaling of initial matrix, Barzilai-Borwein.</span>
0238     scaleFactor = 1;
0239     
0240     <span class="comment">% Norm of the step</span>
0241     stepsize = 1;
0242     
0243     <span class="comment">% Stores whether the step is accepted by the cautious update check.</span>
0244     accepted = true;
0245     
0246     <span class="comment">% Query the cost function and its gradient</span>
0247     [xCurCost, xCurGradient] = <a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>(problem, xCur, storedb, key);
0248     
0249     xCurGradNorm = M.norm(xCur, xCurGradient);
0250     
0251     <span class="comment">% Line-search statistics for recording in info.</span>
0252     lsstats = [];
0253     
0254     <span class="comment">% Flag to control restarting scheme to avoid infinite loops (see below)</span>
0255     ultimatum = false;
0256     
0257     <span class="comment">% Save stats in a struct array info, and preallocate.</span>
0258     stats = <a href="#_sub1" class="code" title="subfunction stats = savestats()">savestats</a>();
0259     info(1) = stats;
0260     info(min(10000, options.maxiter+1)).iter = [];
0261     
0262     <span class="keyword">if</span> options.verbosity &gt;= 2
0263         fprintf(<span class="string">' iter                   cost val            grad. norm           alpha\n'</span>);
0264     <span class="keyword">end</span>
0265     
0266     <span class="comment">% Main iteration</span>
0267     <span class="keyword">while</span> true
0268 
0269         <span class="comment">% Display iteration information</span>
0270         <span class="keyword">if</span> options.verbosity &gt;= 2
0271         fprintf(<span class="string">'%5d    %+.16e        %.8e      %.4e\n'</span>, <span class="keyword">...</span>
0272                 iter, xCurCost, xCurGradNorm, alpha);
0273         <span class="keyword">end</span>
0274         
0275         <span class="comment">% Start timing this iteration</span>
0276         timetic = tic();
0277         
0278         <span class="comment">% Run standard stopping criterion checks</span>
0279         [stop, reason] = <a href="../../../manopt/core/stoppingcriterion.html" class="code" title="function [stop, reason] = stoppingcriterion(problem, x, options, info, last)">stoppingcriterion</a>(problem, xCur, options, <span class="keyword">...</span>
0280                                            info, iter+1);
0281         
0282         <span class="comment">% If none triggered, run specific stopping criterion check</span>
0283         <span class="keyword">if</span> ~stop 
0284             <span class="keyword">if</span> stats.stepsize &lt; options.minstepsize
0285                 <span class="comment">% To avoid infinite loop and to push the search further</span>
0286                 <span class="comment">% in case BFGS approximation of Hessian is off towards</span>
0287                 <span class="comment">% the end, we erase the memory by setting k = 0;</span>
0288                 <span class="comment">% In this way, it starts off like a steepest descent.</span>
0289                 <span class="comment">% If even steepest descent does not work, then it is</span>
0290                 <span class="comment">% hopeless and we will terminate.</span>
0291                 <span class="keyword">if</span> ~ultimatum
0292                     <span class="keyword">if</span> options.verbosity &gt;= 2
0293                         fprintf([<span class="string">'stepsize is too small, restarting '</span> <span class="keyword">...</span>
0294                             <span class="string">'the bfgs procedure at the current point.\n'</span>]);
0295                     <span class="keyword">end</span>
0296                     k = 0;
0297                     ultimatum = true;
0298                 <span class="keyword">else</span>
0299                     stop = true;
0300                     reason = sprintf([<span class="string">'Last stepsize smaller than '</span>  <span class="keyword">...</span>
0301                         <span class="string">'minimum allowed; options.minstepsize = %g.'</span>], <span class="keyword">...</span>
0302                         options.minstepsize);
0303                 <span class="keyword">end</span>
0304             <span class="keyword">else</span>
0305                 <span class="comment">% We are not in trouble: lift the ultimatum if it was on.</span>
0306                 ultimatum = false;
0307             <span class="keyword">end</span>
0308         <span class="keyword">end</span>  
0309         
0310         <span class="keyword">if</span> stop
0311             <span class="keyword">if</span> options.verbosity &gt;= 1
0312                 fprintf([reason <span class="string">'\n'</span>]);
0313             <span class="keyword">end</span>
0314             <span class="keyword">break</span>;
0315         <span class="keyword">end</span>
0316 
0317         
0318         <span class="comment">% Compute BFGS direction</span>
0319         p = <a href="#_sub2" class="code" title="subfunction dir = getDirection(M, xCur, xCurGradient, sHistory, yHistory, ">getDirection</a>(M, xCur, xCurGradient, sHistory,<span class="keyword">...</span>
0320                 yHistory, rhoHistory, scaleFactor, min(k, options.memory));
0321 
0322         <span class="comment">% Execute line-search</span>
0323         [stepsize, xNext, newkey, lsstats] = <span class="keyword">...</span>
0324             <a href="../../../manopt/solvers/linesearch/linesearch_hint.html" class="code" title="function [stepsize, newx, newkey, lsstats] =linesearch_hint(problem, x, d, f0, df0, options, storedb, key)">linesearch_hint</a>(problem, xCur, p, xCurCost, <span class="keyword">...</span>
0325                             M.inner(xCur, xCurGradient, p), <span class="keyword">...</span>
0326                             options, storedb, key);
0327         
0328         <span class="comment">% Record the BFGS step-multiplier alpha which as effectively</span>
0329         <span class="comment">% selected. Toward convergence, we hope to see alpha = 1.</span>
0330         alpha = stepsize/M.norm(xCur, p);
0331         step = M.lincomb(xCur, alpha, p);
0332         
0333         
0334         <span class="comment">% Query cost and gradient at the candidate new point.</span>
0335         [xNextCost, xNextGrad] = <a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>(problem, xNext, storedb, newkey);
0336         
0337         <span class="comment">% Compute sk and yk</span>
0338         sk = M.transp(xCur, xNext, step);
0339         yk = M.lincomb(xNext, 1, xNextGrad, <span class="keyword">...</span>
0340                              -1, M.transp(xCur, xNext, xCurGradient));
0341 
0342         <span class="comment">% Computation of the BFGS step is invariant under scaling of sk and</span>
0343         <span class="comment">% yk by a common factor. For numerical reasons, we scale sk and yk</span>
0344         <span class="comment">% so that sk is a unit norm vector.</span>
0345         norm_sk = M.norm(xNext, sk);
0346         sk = M.lincomb(xNext, 1/norm_sk, sk);
0347         yk = M.lincomb(xNext, 1/norm_sk, yk);
0348         
0349         inner_sk_yk = M.inner(xNext, sk, yk);
0350         inner_sk_sk = M.norm(xNext, sk)^2;    <span class="comment">% ensures nonnegativity</span>
0351         
0352         
0353         <span class="comment">% If the cautious step is accepted (which is the intended</span>
0354         <span class="comment">% behavior), we record sk, yk and rhok and need to do some</span>
0355         <span class="comment">% housekeeping. If the cautious step is rejected, these are not</span>
0356         <span class="comment">% recorded. In all cases, xNext is the next iterate: the notion of</span>
0357         <span class="comment">% accept/reject here is limited to whether or not we keep track of</span>
0358         <span class="comment">% sk, yk, rhok to update the BFGS operator.</span>
0359         cap = options.strict_inc_func(xCurGradNorm);
0360         <span class="keyword">if</span> inner_sk_sk ~= 0 &amp;&amp; (inner_sk_yk / inner_sk_sk) &gt;= cap
0361             
0362             accepted = true;
0363             
0364             rhok = 1/inner_sk_yk;
0365             
0366             scaleFactor = inner_sk_yk / M.norm(xNext, yk)^2;
0367             
0368             <span class="comment">% Time to store the vectors sk, yk and the scalar rhok.</span>
0369             <span class="comment">% Remember: we need to transport all vectors to the most</span>
0370             <span class="comment">% current tangent space.</span>
0371             
0372             <span class="comment">% If we are out of memory</span>
0373             <span class="keyword">if</span> k &gt;= options.memory
0374                 
0375                 <span class="comment">% sk and yk are saved from 1 to the end with the most</span>
0376                 <span class="comment">% current recorded to the rightmost hand side of the cells</span>
0377                 <span class="comment">% that are occupied. When memory is full, do a shift so</span>
0378                 <span class="comment">% that the rightmost is earliest and replace it with the</span>
0379                 <span class="comment">% most recent sk, yk.</span>
0380                 <span class="keyword">for</span>  i = 2 : options.memory
0381                     sHistory{i} = M.transp(xCur, xNext, sHistory{i});
0382                     yHistory{i} = M.transp(xCur, xNext, yHistory{i});
0383                 <span class="keyword">end</span>
0384                 <span class="keyword">if</span> options.memory &gt; 1
0385                     sHistory = sHistory([2:<span class="keyword">end</span>, 1]);
0386                     yHistory = yHistory([2:<span class="keyword">end</span>, 1]);
0387                     rhoHistory = rhoHistory([2:end 1]);
0388                 <span class="keyword">end</span>
0389                 <span class="keyword">if</span> options.memory &gt; 0
0390                     sHistory{options.memory} = sk;
0391                     yHistory{options.memory} = yk;
0392                     rhoHistory{options.memory} = rhok;
0393                 <span class="keyword">end</span>
0394                 
0395             <span class="comment">% If we are not out of memory</span>
0396             <span class="keyword">else</span>
0397                 
0398                 <span class="keyword">for</span>  i = 1:k
0399                     sHistory{i} = M.transp(xCur, xNext, sHistory{i});
0400                     yHistory{i} = M.transp(xCur, xNext, yHistory{i});
0401                 <span class="keyword">end</span>
0402                 sHistory{k+1} = sk;
0403                 yHistory{k+1} = yk;
0404                 rhoHistory{k+1} = rhok;
0405                 
0406             <span class="keyword">end</span>
0407             
0408             k = k + 1;
0409             
0410         <span class="comment">% The cautious step is rejected: we do not store sk, yk, rhok but</span>
0411         <span class="comment">% we still need to transport stored vectors to the new tangent</span>
0412         <span class="comment">% space.</span>
0413         <span class="keyword">else</span>
0414             
0415             accepted = false;
0416             
0417             <span class="keyword">for</span>  i = 1 : min(k, options.memory)
0418                 sHistory{i} = M.transp(xCur, xNext, sHistory{i});
0419                 yHistory{i} = M.transp(xCur, xNext, yHistory{i});
0420             <span class="keyword">end</span>
0421             
0422         <span class="keyword">end</span>
0423         
0424         <span class="comment">% Update variables to new iterate</span>
0425         iter = iter + 1;
0426         xCur = xNext;
0427         key = newkey;
0428         xCurGradient = xNextGrad;
0429         xCurGradNorm = M.norm(xNext, xNextGrad);
0430         xCurCost = xNextCost;
0431         
0432         
0433         <span class="comment">% Make sure we don't use too much memory for the store database</span>
0434         <span class="comment">% (this is independent from the BFGS memory.)</span>
0435         storedb.purge();
0436         
0437         
0438         <span class="comment">% Log statistics for freshly executed iteration</span>
0439         stats = <a href="#_sub1" class="code" title="subfunction stats = savestats()">savestats</a>();
0440         info(iter+1) = stats; 
0441         
0442     <span class="keyword">end</span>
0443 
0444     
0445     <span class="comment">% Housekeeping before we return</span>
0446     info = info(1:iter+1);
0447     x = xCur;
0448     cost = xCurCost;
0449 
0450     <span class="keyword">if</span> options.verbosity &gt;= 1
0451         fprintf(<span class="string">'Total time is %f [s] (excludes statsfun)\n'</span>, <span class="keyword">...</span>
0452                 info(end).time);
0453     <span class="keyword">end</span>
0454 
0455     
0456     <span class="comment">% Routine in charge of collecting the current iteration stats</span>
0457     <a name="_sub1" href="#_subfunctions" class="code">function stats = savestats()</a>
0458         stats.iter = iter;
0459         stats.cost = xCurCost;
0460         stats.gradnorm = xCurGradNorm;
0461         <span class="keyword">if</span> iter == 0
0462             stats.stepsize = NaN;
0463             stats.time = toc(timetic);
0464             stats.accepted = NaN;
0465         <span class="keyword">else</span>
0466             stats.stepsize = stepsize;
0467             stats.time = info(iter).time + toc(timetic);
0468             stats.accepted = accepted;
0469         <span class="keyword">end</span>
0470         stats.linesearch = lsstats;
0471         stats = <a href="../../../manopt/core/applyStatsfun.html" class="code" title="function stats = applyStatsfun(problem, x, storedb, key, options, stats)">applyStatsfun</a>(problem, xCur, storedb, key, options, stats);
0472     <span class="keyword">end</span>
0473 
0474 <span class="keyword">end</span>
0475 
0476 
0477 
0478 
0479 <span class="comment">% BFGS step, see Wen's paper for details. This functon takes in a tangent</span>
0480 <span class="comment">% vector g, and applies an approximate inverse Hessian P to it to get Pg.</span>
0481 <span class="comment">% Then, -Pg is returned.</span>
0482 <span class="comment">%</span>
0483 <span class="comment">% Theory requires the vector transport to be isometric and to satisfy the</span>
0484 <span class="comment">% locking condition (see paper), but these properties do not seem to be</span>
0485 <span class="comment">% crucial in practice. If your manifold provides M.isotransp, it may be</span>
0486 <span class="comment">% good to do M.transp = M.isotransp; after loading M with a factory.</span>
0487 <span class="comment">%</span>
0488 <span class="comment">% This implementation operates in the tangent space of the most recent</span>
0489 <span class="comment">% point since all vectors in sHistory and yHistory have been transported</span>
0490 <span class="comment">% there.</span>
0491 <a name="_sub2" href="#_subfunctions" class="code">function dir = getDirection(M, xCur, xCurGradient, sHistory, yHistory, </a><span class="keyword">...</span>
0492                             rhoHistory, scaleFactor, k)
0493     
0494     q = xCurGradient;
0495     
0496     inner_s_q = zeros(1, k);
0497     
0498     <span class="keyword">for</span> i = k : -1 : 1
0499         inner_s_q(1, i) = rhoHistory{i} * M.inner(xCur, sHistory{i}, q);
0500         q = M.lincomb(xCur, 1, q, -inner_s_q(1, i), yHistory{i});
0501     <span class="keyword">end</span>
0502     
0503     r = M.lincomb(xCur, scaleFactor, q);
0504     
0505     <span class="keyword">for</span> i = 1 : k
0506          omega = rhoHistory{i} * M.inner(xCur, yHistory{i}, r);
0507          r = M.lincomb(xCur, 1, r, inner_s_q(1, i)-omega, sHistory{i});
0508     <span class="keyword">end</span>
0509     
0510     dir = M.lincomb(xCur, -1, r);
0511     
0512 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Fri 08-Sep-2017 12:43:19 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>