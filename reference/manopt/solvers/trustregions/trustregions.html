<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of trustregions</title>
  <meta name="keywords" content="trustregions">
  <meta name="description" content="Riemannian trust-regions solver for optimization on manifolds.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../index.html">Home</a> &gt;  <a href="#">manopt</a> &gt; <a href="#">solvers</a> &gt; <a href="index.html">trustregions</a> &gt; trustregions.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../index.html"><img alt="<" border="0" src="../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for manopt\solvers\trustregions&nbsp;<img alt=">" border="0" src="../../../right.png"></a></td></tr></table>-->

<h1>trustregions
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>Riemannian trust-regions solver for optimization on manifolds.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>function [x, cost, info, options] = trustregions(problem, x, options) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Riemannian trust-regions solver for optimization on manifolds.

 function [x, cost, info, options] = trustregions(problem)
 function [x, cost, info, options] = trustregions(problem, x0)
 function [x, cost, info, options] = trustregions(problem, x0, options)
 function [x, cost, info, options] = trustregions(problem, [], options)

 This is the Riemannian Trust-Region solver for Manopt, named RTR.
 This solver tries to minimize the cost function described in the problem
 structure. It requires the availability of the cost function and of its
 gradient. It issues calls for the Hessian.

 If no Hessian nor approximation for it is provided, an approximation of
 Hessian-vector products is computed with finite differences of gradients.

 If no gradient is provided, an approximation of the gradient is computed,
 but this can be slow for manifolds of high dimension.

 At each iteration a subproblem is solved using a trust-region subproblem
 (TRS) solver. The default is @trs_tCG_cached. This one (and some others)
 use the preconditioner if one is supplied.

 For a description of the algorithm and theorems offering convergence
 guarantees, see the references below. Documentation for this solver is
 available online, but may be outdated.
 http://www.manopt.org/solver_documentation_trustregions.html


 The initial iterate is x0 if it is provided. Otherwise, a random point on
 the manifold is picked. To specify options whilst not specifying an
 initial iterate, give x0 as [] (the empty matrix).

 The two outputs 'x' and 'cost' are the last reached point on the manifold
 and its cost. Notice that x is not necessarily the best reached point,
 because this solver is not forced to be a descent method. In particular,
 very close to convergence, it is sometimes preferable to accept very
 slight increases in the cost value (on the order of the machine epsilon)
 in the process of reaching fine convergence. Other than that, the cost
 function value does decrease monotonically with iterations.
 
 The output 'info' is a struct-array which contains information about the
 iterations:
   iter (integer)
       The (outer) iteration number, or number of steps considered
       (whether accepted or rejected). The initial guess is 0.
   cost (double)
       The corresponding cost value.
   gradnorm (double)
       The (Riemannian) norm of the gradient.
   time (double)
       The total elapsed time in seconds to reach the corresponding cost.
   rho (double)
       The performance ratio for the iterate.
   rhonum, rhoden (double)
       Regularized numerator and denominator of the performance ratio:
       rho = rhonum/rhoden. See options.rho_regularization.
   accepted (boolean)
       Whether the proposed iterate was accepted or not.
   stepsize (double)
       The (Riemannian) norm of the vector returned by the inner solver
       and which is retracted to obtain the proposed next iterate. If
       accepted = true for the corresponding iterate, this is the size of
       the step from the previous to the new iterate. If accepted is
       false, the step was not executed and this is the size of the
       rejected step.
   Delta (double)
       The trust-region radius at the outer iteration.
   limitedbyTR (boolean)
       true if the subproblemsolver was limited by the trust-region
       radius (a boundary solution was returned).
   And possibly additional information logged by the subproblemsolver or
   by options.statsfun.
 For example, type [info.gradnorm] to obtain a vector of the successive
 gradient norms reached at each (outer) iteration.

 The options structure is used to overwrite the default values. All
 options have a default value and are hence optional. To force an option
 value, pass an options structure with a field options.optionname, where
 optionname is one of the following and the default value is indicated
 between parentheses:

   tolgradnorm (1e-6)
       The algorithm terminates if the norm of the gradient drops below
       this. For well-scaled problems, a rule of thumb is that you can
       expect to reduce the gradient norm by 8 orders of magnitude
       (sqrt(eps)) compared to the gradient norm at a &quot;typical&quot; point (a
       rough initial iterate for example). Further decrease is sometimes
       possible, but inexact floating point arithmetic limits the final
       accuracy. If tolgradnorm is set too low, the algorithm may end up
       iterating forever (or until another stopping criterion triggers).
   maxiter (1000)
       The algorithm terminates after at most maxiter (outer) iterations.
   maxtime (Inf)
       The algorithm terminates if maxtime seconds elapsed.
   miniter (0)
       Minimum number of outer iterations: this overrides all other
       stopping criteria. Can be helpful to escape saddle points.
   Delta_bar (problem.M.typicaldist() or sqrt(problem.M.dim()))
       Maximum trust-region radius. If you specify this parameter but not
       Delta0, then Delta0 is set to 1/8 times this parameter.
   Delta0 (Delta_bar/8)
       Initial trust-region radius. If you observe a long plateau at the
       beginning of the convergence plot (gradient norm vs iteration), it
       may pay off to try to tune this parameter to shorten the plateau.
       You should not set this parameter without setting Delta_bar too (at
       a larger value).
   subproblemsolver (@trs_tCG_cached)
       Function handle to a subproblem solver. The subproblem solver also
       sees this options structure, so that parameters can be passed to it
       through here as well. Built-in solvers include:
           trs_tCG_cached
           trs_tCG
           trs_gep
       Note that trs_gep solves the subproblem exactly which may be slow.
       It is included mainly for prototyping or for solving the subproblem
       exactly in low dimensional subspaces.
   rho_prime (0.1)
       Accept/reject threshold : if rho is at least rho_prime, the outer
       iteration is accepted. Otherwise, it is rejected. In case it is
       rejected, the trust-region radius will have been decreased.
       To ensure this, rho_prime &gt;= 0 must be strictly smaller than 1/4.
       If rho_prime is negative, the algorithm is not guaranteed to
       produce monotonically decreasing cost values. It is strongly
       recommended to set rho_prime &gt; 0, to aid convergence.
   rho_regularization (1e3)
       Close to convergence, evaluating the performance ratio rho is
       numerically challenging. Meanwhile, close to convergence, the
       quadratic model should be a good fit and the steps should be
       accepted. Regularization lets rho go to 1 as the model decrease and
       the actual decrease go to zero. Set this option to zero to disable
       regularization (not recommended). See in-code for the specifics.
       When this is not zero, it may happen that the iterates produced are
       not monotonically improving the cost when very close to
       convergence. This is because the corrected cost improvement could
       change sign if it is negative but very small.
   statsfun (none)
       Function handle to a function that is called after each iteration
       to provide the opportunity to log additional statistics.
       They are returned in the info struct. See the generic Manopt
       documentation about solvers for further information. statsfun is
       called with the point x that was reached last, after the
       accept/reject decision. See comment below.
   stopfun (none)
       Function handle to a function that is called at each iteration to
       provide the opportunity to specify additional stopping criteria.
       See the generic Manopt documentation about solvers for further
       information.
   verbosity (2)
       Integer number used to tune the amount of output the algorithm logs
       during execution (mostly as text in the command window).
       The higher, the more output. 0 means silent. 3 and above includes a
       display of the options structure at the beginning of the execution.
   debug (false)
       Set to true to allow the algorithm to perform additional
       computations for debugging purposes. If a debugging test fails, you
       will be informed of it, usually via the command window. Be aware
       that these additional computations appear in the algorithm timings
       too, and may interfere with operations such as counting the number
       of cost evaluations, etc. The debug calls get storedb too.
   storedepth (2)
       Maximum number of different points x of the manifold for which a
       store structure may be kept in memory in the storedb for caching.
       If memory usage is an issue, you may try to lower this number.
       Profiling or manopt counters may then help to investigate if a
       performance hit was incurred as a result.
   hook (none)
       A function handle which allows the user to change the current point
       x at the beginning of each iteration, before the stopping criterion
       is evaluated. See applyHook for help on how to use this option.

 Notice that statsfun is called with the point x that was reached last,
 after the accept/reject decision. Hence: if the step was accepted, we get
 that new x, with a store which only saw the call for the cost and for the
 gradient. If the step was rejected, we get the same x as previously, with
 the store structure containing everything that was computed at that point
 (possibly including previous rejects at that same point). Hence, statsfun
 should not be used in conjunction with the store to count operations for
 example. Instead, you should use manopt counters: see statscounters.


 Please cite the Manopt paper as well as the research paper:
     @Article{genrtr,
       Title    = {Trust-region methods on {Riemannian} manifolds},
       Author   = {Absil, P.-A. and Baker, C. G. and Gallivan, K. A.},
       Journal  = {Foundations of Computational Mathematics},
       Year     = {2007},
       Number   = {3},
       Pages    = {303--330},
       Volume   = {7},
       Doi      = {10.1007/s10208-005-0179-9}
     }

 See also: steepestdescent conjugategradient manopt/examples</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>	</li><li><a href="../../../manopt/core/applyHook.html" class="code" title="function [newx, newkey, info, hooked] = applyHook(problem, x, storedb, key, options, info, last)">applyHook</a>	Apply the hook function to possibly replace the current x (for solvers).</li><li><a href="../../../manopt/core/applyStatsfun.html" class="code" title="function stats = applyStatsfun(problem, x, storedb, key, options, stats)">applyStatsfun</a>	Apply the statsfun function to a stats structure (for solvers).</li><li><a href="../../../manopt/core/canGetApproxGradient.html" class="code" title="function candoit = canGetApproxGradient(problem)">canGetApproxGradient</a>	Checks whether an approximate gradient can be computed for this problem.</li><li><a href="../../../manopt/core/canGetApproxHessian.html" class="code" title="function candoit = canGetApproxHessian(problem)">canGetApproxHessian</a>	Checks whether an approximate Hessian can be computed for this problem.</li><li><a href="../../../manopt/core/canGetCost.html" class="code" title="function candoit = canGetCost(problem)">canGetCost</a>	Checks whether the cost function can be computed for a problem structure.</li><li><a href="../../../manopt/core/canGetGradient.html" class="code" title="function candoit = canGetGradient(problem)">canGetGradient</a>	Checks whether the gradient can be computed for a problem structure.</li><li><a href="../../../manopt/core/canGetHessian.html" class="code" title="function candoit = canGetHessian(problem)">canGetHessian</a>	Checks whether the Hessian can be computed for a problem structure.</li><li><a href="../../../manopt/core/getCost.html" class="code" title="function cost = getCost(problem, x, storedb, key)">getCost</a>	Computes the cost function at x.</li><li><a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>	Computes the cost function and the gradient at x in one call if possible.</li><li><a href="../../../manopt/core/getDirectionalDerivative.html" class="code" title="function diff = getDirectionalDerivative(problem, x, d, storedb, key)">getDirectionalDerivative</a>	Computes the directional derivative of the cost function at x along d.</li><li><a href="../../../manopt/core/getGlobalDefaults.html" class="code" title="function opts = getGlobalDefaults()">getGlobalDefaults</a>	Returns a structure with default option values for Manopt.</li><li><a href="../../../manopt/core/getGradient.html" class="code" title="function grad = getGradient(problem, x, storedb, key)">getGradient</a>	Computes the gradient of the cost function at x.</li><li><a href="../../../manopt/core/getHessian.html" class="code" title="function hess = getHessian(problem, x, d, storedb, key)">getHessian</a>	Computes the Hessian of the cost function at x along d.</li><li><a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>	Merges two options structures with one having precedence over the other.</li><li><a href="../../../manopt/core/stoppingcriterion.html" class="code" title="function [stop, reason] = stoppingcriterion(problem, x, options, info, last)">stoppingcriterion</a>	Checks for standard stopping criteria, as a helper to solvers.</li><li><a href="../../../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/disp.html" class="code" title="function disp( x, name )">disp</a>	DISP Display TT/MPS tensor.</li><li><a href="../../../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS_block/disp.html" class="code" title="function disp( x, name )">disp</a>	DISP Display TT/MPS block-mu tensor.</li><li><a href="../../../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS_op/disp.html" class="code" title="function disp( x, name )">disp</a>	DISP Display TT/MPS operator.</li><li><a href="../../../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS_op_laplace/disp.html" class="code" title="function disp( x, name )">disp</a>	DISP Display TT/MPS operator.</li><li><a href="../../../manopt/solvers/gradientapproximations/approxgradientFD.html" class="code" title="function gradfun = approxgradientFD(problem, options)">approxgradientFD</a>	Gradient approx. fnctn handle based on finite differences of the cost.</li><li><a href="../../../manopt/solvers/hessianapproximations/approxhessianFD.html" class="code" title="function hessfun = approxhessianFD(problem, options)">approxhessianFD</a>	Hessian approx. fnctn handle based on finite differences of the gradient.</li><li><a href="trs_tCG_cached.html" class="code" title="function trsoutput = trs_tCG_cached(problem, trsinput, options, storedb, key)">trs_tCG_cached</a>	Truncated (Steihaug-Toint) Conjugate-Gradient method with caching.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../examples/dominant_invariant_subspace.html" class="code" title="function [X, info] = dominant_invariant_subspace(A, p)">dominant_invariant_subspace</a>	Returns an orthonormal basis of the dominant invariant p-subspace of A.</li><li><a href="../../../examples/dominant_invariant_subspace_complex.html" class="code" title="function [X, info] = dominant_invariant_subspace_complex(A, p)">dominant_invariant_subspace_complex</a>	Returns a unitary basis of the dominant invariant p-subspace of A.</li><li><a href="../../../examples/doubly_stochastic_denoising.html" class="code" title="function doubly_stochastic_denoising()">doubly_stochastic_denoising</a>	Find a doubly stochastic matrix closest to a given matrix, in Frobenius norm.</li><li><a href="../../../examples/elliptope_SDP.html" class="code" title="function [Y, problem, S] = elliptope_SDP(A, p, Y0)">elliptope_SDP</a>	Solver for semidefinite programs (SDP's) with unit diagonal constraints.</li><li><a href="../../../examples/elliptope_SDP_complex.html" class="code" title="function [Y, problem, S] = elliptope_SDP_complex(A, p, Y0)">elliptope_SDP_complex</a>	Solver for complex semidefinite programs (SDP's) with unit diagonal.</li><li><a href="../../../examples/essential_svd.html" class="code" title="function essential_svd">essential_svd</a>	Sample solution of an optimization problem on the essential manifold.</li><li><a href="../../../examples/generalized_eigenvalue_computation.html" class="code" title="function [Xsol, Ssol] = generalized_eigenvalue_computation(A, B, p)">generalized_eigenvalue_computation</a>	Returns orthonormal basis of the dominant invariant p-subspace of B^-1 A.</li><li><a href="../../../examples/generalized_procrustes.html" class="code" title="function [A, R] = generalized_procrustes(A_measure)">generalized_procrustes</a>	Rotationally align clouds of points (generalized Procrustes problem)</li><li><a href="../../../examples/low_rank_dist_completion.html" class="code" title="function [Y, infos, problem_description] =  low_rank_dist_completion(problem_description)">low_rank_dist_completion</a>	Perform low-rank distance matrix completion w/ automatic rank detection.</li><li><a href="../../../examples/low_rank_matrix_completion.html" class="code" title="function low_rank_matrix_completion()">low_rank_matrix_completion</a>	Given partial observation of a low rank matrix, attempts to complete it.</li><li><a href="../../../examples/low_rank_tensor_completion.html" class="code" title="function low_rank_tensor_completion()">low_rank_tensor_completion</a>	Given partial observation of a low rank tensor, attempts to complete it.</li><li><a href="../../../examples/low_rank_tensor_completion_TT.html" class="code" title="function low_rank_tensor_completion_TT()">low_rank_tensor_completion_TT</a>	Example file for the manifold encoded in fixedTTrankfactory.</li><li><a href="../../../examples/low_rank_tensor_completion_embedded.html" class="code" title="function low_rank_tensor_completion_embedded()">low_rank_tensor_completion_embedded</a>	Given partial observation of a low rank tensor (possibly including noise),</li><li><a href="../../../examples/maxcut.html" class="code" title="function [x, cutvalue, cutvalue_upperbound, Y] = maxcut(L, r)">maxcut</a>	Algorithm to (try to) compute a maximum cut of a graph, via SDP approach.</li><li><a href="../../../examples/radio_interferometric_calibration.html" class="code" title="function xsol = radio_interferometric_calibration(N, K)">radio_interferometric_calibration</a>	Returns the gain matrices of N stations with K receivers.</li><li><a href="../../../examples/robust_pca.html" class="code" title="function [U, cost] = robust_pca(X, d)">robust_pca</a>	Computes a robust version of PCA (principal component analysis) on data.</li><li><a href="../../../examples/shapefit_smoothed.html" class="code" title="function [T_hub, T_lsq, T_cvx] = shapefit_smoothed(V, J)">shapefit_smoothed</a>	ShapeFit formulation for sensor network localization from pair directions</li><li><a href="../../../examples/sparse_pca.html" class="code" title="function [Z, P, X, A] = sparse_pca(A, m, gamma)">sparse_pca</a>	Sparse principal component analysis based on optimization over Stiefel.</li><li><a href="../../../examples/truncated_svd.html" class="code" title="function [U, S, V, info] = truncated_svd(A, p)">truncated_svd</a>	Returns an SVD decomposition of A truncated to rank p.</li><li><a href="../../../examples/using_counters.html" class="code" title="function using_counters()">using_counters</a>	Manopt example on how to use counters during optimization. Typical uses,</li><li><a href="../../../examples/using_gpu.html" class="code" title="function using_gpu()">using_gpu</a>	Manopt example on how to use GPU with manifold factories that allow it.</li><li><a href="../../../manopt/autodiff/basic_examples_AD/basic_example_AD.html" class="code" title="function basic_example_AD()">basic_example_AD</a>	A basic example that shows how to apply automatic differentiation to</li><li><a href="../../../manopt/autodiff/basic_examples_AD/complex_example_AD.html" class="code" title="function complex_example_AD()">complex_example_AD</a>	A basic example that shows how to define the cost funtion for</li><li><a href="../../../manopt/autodiff/basic_examples_AD/complextest_AD1.html" class="code" title="function complextest_AD1()">complextest_AD1</a>	Test AD for a complex optimization problem on a product manifold (struct)</li><li><a href="../../../manopt/autodiff/basic_examples_AD/complextest_AD2.html" class="code" title="function complextest_AD2()">complextest_AD2</a>	Test AD for a complex optimization problem on a power manifold (cell)</li><li><a href="../../../manopt/autodiff/basic_examples_AD/complextest_AD3.html" class="code" title="function complextest_AD3()">complextest_AD3</a>	Test AD for a complex optimization problem on a manifold which is stored</li><li><a href="../../../manopt/autodiff/basic_examples_AD/realtest_AD1.html" class="code" title="function realtest_AD1()">realtest_AD1</a>	Test AD for a real optimization problem on a product manifold (struct)</li><li><a href="../../../manopt/autodiff/basic_examples_AD/realtest_AD2.html" class="code" title="function realtest_AD2()">realtest_AD2</a>	Test AD for a real optimization problem on a power manifold (cell)</li><li><a href="../../../manopt/autodiff/basic_examples_AD/realtest_AD3.html" class="code" title="function realtest_AD3()">realtest_AD3</a>	Test AD for a real optimization problem on a manifold which is stored in</li><li><a href="../../../manopt/autodiff/basic_examples_AD/using_gpu_AD.html" class="code" title="function using_gpu_AD()">using_gpu_AD</a>	Manopt example on how to use GPU to compute the egrad and the ehess via AD.</li><li><a href="../../../manopt/manifolds/ttfixedrank/TTeMPS_1.1/examples/linearsystem_compare.html" class="code" title="">linearsystem_compare</a>	Example code for the algorithms described in</li><li><a href="../../../manopt/solvers/neldermead/centroid.html" class="code" title="function y = centroid(M, x)">centroid</a>	Attempts the computation of a centroid of a set of points on a manifold.</li><li><a href="../../../manopt/solvers/preconditioners/preconhessiansolve.html" class="code" title="function preconfun = preconhessiansolve(problem, options)">preconhessiansolve</a>	Preconditioner based on the inverse Hessian, by solving linear systems.</li><li><a href="../../../manopt/tools/hessianextreme.html" class="code" title="function [y, lambda, info] = hessianextreme(problem, x, side, y0, options, storedb, key)">hessianextreme</a>	Compute an extreme eigenvector / eigenvalue of the Hessian of a problem.</li><li><a href="../../../manopt/tools/manoptsolve.html" class="code" title="function [x, cost, info, options] = manoptsolve(problem, x0, options)">manoptsolve</a>	Gateway helper function to call a Manopt solver, chosen in the options.</li></ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="#_sub1" class="code">function stats = savestats(problem, x, storedb, key, options, k, fx,</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [x, cost, info, options] = trustregions(problem, x, options)</a>
0002 <span class="comment">% Riemannian trust-regions solver for optimization on manifolds.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% function [x, cost, info, options] = trustregions(problem)</span>
0005 <span class="comment">% function [x, cost, info, options] = trustregions(problem, x0)</span>
0006 <span class="comment">% function [x, cost, info, options] = trustregions(problem, x0, options)</span>
0007 <span class="comment">% function [x, cost, info, options] = trustregions(problem, [], options)</span>
0008 <span class="comment">%</span>
0009 <span class="comment">% This is the Riemannian Trust-Region solver for Manopt, named RTR.</span>
0010 <span class="comment">% This solver tries to minimize the cost function described in the problem</span>
0011 <span class="comment">% structure. It requires the availability of the cost function and of its</span>
0012 <span class="comment">% gradient. It issues calls for the Hessian.</span>
0013 <span class="comment">%</span>
0014 <span class="comment">% If no Hessian nor approximation for it is provided, an approximation of</span>
0015 <span class="comment">% Hessian-vector products is computed with finite differences of gradients.</span>
0016 <span class="comment">%</span>
0017 <span class="comment">% If no gradient is provided, an approximation of the gradient is computed,</span>
0018 <span class="comment">% but this can be slow for manifolds of high dimension.</span>
0019 <span class="comment">%</span>
0020 <span class="comment">% At each iteration a subproblem is solved using a trust-region subproblem</span>
0021 <span class="comment">% (TRS) solver. The default is @trs_tCG_cached. This one (and some others)</span>
0022 <span class="comment">% use the preconditioner if one is supplied.</span>
0023 <span class="comment">%</span>
0024 <span class="comment">% For a description of the algorithm and theorems offering convergence</span>
0025 <span class="comment">% guarantees, see the references below. Documentation for this solver is</span>
0026 <span class="comment">% available online, but may be outdated.</span>
0027 <span class="comment">% http://www.manopt.org/solver_documentation_trustregions.html</span>
0028 <span class="comment">%</span>
0029 <span class="comment">%</span>
0030 <span class="comment">% The initial iterate is x0 if it is provided. Otherwise, a random point on</span>
0031 <span class="comment">% the manifold is picked. To specify options whilst not specifying an</span>
0032 <span class="comment">% initial iterate, give x0 as [] (the empty matrix).</span>
0033 <span class="comment">%</span>
0034 <span class="comment">% The two outputs 'x' and 'cost' are the last reached point on the manifold</span>
0035 <span class="comment">% and its cost. Notice that x is not necessarily the best reached point,</span>
0036 <span class="comment">% because this solver is not forced to be a descent method. In particular,</span>
0037 <span class="comment">% very close to convergence, it is sometimes preferable to accept very</span>
0038 <span class="comment">% slight increases in the cost value (on the order of the machine epsilon)</span>
0039 <span class="comment">% in the process of reaching fine convergence. Other than that, the cost</span>
0040 <span class="comment">% function value does decrease monotonically with iterations.</span>
0041 <span class="comment">%</span>
0042 <span class="comment">% The output 'info' is a struct-array which contains information about the</span>
0043 <span class="comment">% iterations:</span>
0044 <span class="comment">%   iter (integer)</span>
0045 <span class="comment">%       The (outer) iteration number, or number of steps considered</span>
0046 <span class="comment">%       (whether accepted or rejected). The initial guess is 0.</span>
0047 <span class="comment">%   cost (double)</span>
0048 <span class="comment">%       The corresponding cost value.</span>
0049 <span class="comment">%   gradnorm (double)</span>
0050 <span class="comment">%       The (Riemannian) norm of the gradient.</span>
0051 <span class="comment">%   time (double)</span>
0052 <span class="comment">%       The total elapsed time in seconds to reach the corresponding cost.</span>
0053 <span class="comment">%   rho (double)</span>
0054 <span class="comment">%       The performance ratio for the iterate.</span>
0055 <span class="comment">%   rhonum, rhoden (double)</span>
0056 <span class="comment">%       Regularized numerator and denominator of the performance ratio:</span>
0057 <span class="comment">%       rho = rhonum/rhoden. See options.rho_regularization.</span>
0058 <span class="comment">%   accepted (boolean)</span>
0059 <span class="comment">%       Whether the proposed iterate was accepted or not.</span>
0060 <span class="comment">%   stepsize (double)</span>
0061 <span class="comment">%       The (Riemannian) norm of the vector returned by the inner solver</span>
0062 <span class="comment">%       and which is retracted to obtain the proposed next iterate. If</span>
0063 <span class="comment">%       accepted = true for the corresponding iterate, this is the size of</span>
0064 <span class="comment">%       the step from the previous to the new iterate. If accepted is</span>
0065 <span class="comment">%       false, the step was not executed and this is the size of the</span>
0066 <span class="comment">%       rejected step.</span>
0067 <span class="comment">%   Delta (double)</span>
0068 <span class="comment">%       The trust-region radius at the outer iteration.</span>
0069 <span class="comment">%   limitedbyTR (boolean)</span>
0070 <span class="comment">%       true if the subproblemsolver was limited by the trust-region</span>
0071 <span class="comment">%       radius (a boundary solution was returned).</span>
0072 <span class="comment">%   And possibly additional information logged by the subproblemsolver or</span>
0073 <span class="comment">%   by options.statsfun.</span>
0074 <span class="comment">% For example, type [info.gradnorm] to obtain a vector of the successive</span>
0075 <span class="comment">% gradient norms reached at each (outer) iteration.</span>
0076 <span class="comment">%</span>
0077 <span class="comment">% The options structure is used to overwrite the default values. All</span>
0078 <span class="comment">% options have a default value and are hence optional. To force an option</span>
0079 <span class="comment">% value, pass an options structure with a field options.optionname, where</span>
0080 <span class="comment">% optionname is one of the following and the default value is indicated</span>
0081 <span class="comment">% between parentheses:</span>
0082 <span class="comment">%</span>
0083 <span class="comment">%   tolgradnorm (1e-6)</span>
0084 <span class="comment">%       The algorithm terminates if the norm of the gradient drops below</span>
0085 <span class="comment">%       this. For well-scaled problems, a rule of thumb is that you can</span>
0086 <span class="comment">%       expect to reduce the gradient norm by 8 orders of magnitude</span>
0087 <span class="comment">%       (sqrt(eps)) compared to the gradient norm at a &quot;typical&quot; point (a</span>
0088 <span class="comment">%       rough initial iterate for example). Further decrease is sometimes</span>
0089 <span class="comment">%       possible, but inexact floating point arithmetic limits the final</span>
0090 <span class="comment">%       accuracy. If tolgradnorm is set too low, the algorithm may end up</span>
0091 <span class="comment">%       iterating forever (or until another stopping criterion triggers).</span>
0092 <span class="comment">%   maxiter (1000)</span>
0093 <span class="comment">%       The algorithm terminates after at most maxiter (outer) iterations.</span>
0094 <span class="comment">%   maxtime (Inf)</span>
0095 <span class="comment">%       The algorithm terminates if maxtime seconds elapsed.</span>
0096 <span class="comment">%   miniter (0)</span>
0097 <span class="comment">%       Minimum number of outer iterations: this overrides all other</span>
0098 <span class="comment">%       stopping criteria. Can be helpful to escape saddle points.</span>
0099 <span class="comment">%   Delta_bar (problem.M.typicaldist() or sqrt(problem.M.dim()))</span>
0100 <span class="comment">%       Maximum trust-region radius. If you specify this parameter but not</span>
0101 <span class="comment">%       Delta0, then Delta0 is set to 1/8 times this parameter.</span>
0102 <span class="comment">%   Delta0 (Delta_bar/8)</span>
0103 <span class="comment">%       Initial trust-region radius. If you observe a long plateau at the</span>
0104 <span class="comment">%       beginning of the convergence plot (gradient norm vs iteration), it</span>
0105 <span class="comment">%       may pay off to try to tune this parameter to shorten the plateau.</span>
0106 <span class="comment">%       You should not set this parameter without setting Delta_bar too (at</span>
0107 <span class="comment">%       a larger value).</span>
0108 <span class="comment">%   subproblemsolver (@trs_tCG_cached)</span>
0109 <span class="comment">%       Function handle to a subproblem solver. The subproblem solver also</span>
0110 <span class="comment">%       sees this options structure, so that parameters can be passed to it</span>
0111 <span class="comment">%       through here as well. Built-in solvers include:</span>
0112 <span class="comment">%           trs_tCG_cached</span>
0113 <span class="comment">%           trs_tCG</span>
0114 <span class="comment">%           trs_gep</span>
0115 <span class="comment">%       Note that trs_gep solves the subproblem exactly which may be slow.</span>
0116 <span class="comment">%       It is included mainly for prototyping or for solving the subproblem</span>
0117 <span class="comment">%       exactly in low dimensional subspaces.</span>
0118 <span class="comment">%   rho_prime (0.1)</span>
0119 <span class="comment">%       Accept/reject threshold : if rho is at least rho_prime, the outer</span>
0120 <span class="comment">%       iteration is accepted. Otherwise, it is rejected. In case it is</span>
0121 <span class="comment">%       rejected, the trust-region radius will have been decreased.</span>
0122 <span class="comment">%       To ensure this, rho_prime &gt;= 0 must be strictly smaller than 1/4.</span>
0123 <span class="comment">%       If rho_prime is negative, the algorithm is not guaranteed to</span>
0124 <span class="comment">%       produce monotonically decreasing cost values. It is strongly</span>
0125 <span class="comment">%       recommended to set rho_prime &gt; 0, to aid convergence.</span>
0126 <span class="comment">%   rho_regularization (1e3)</span>
0127 <span class="comment">%       Close to convergence, evaluating the performance ratio rho is</span>
0128 <span class="comment">%       numerically challenging. Meanwhile, close to convergence, the</span>
0129 <span class="comment">%       quadratic model should be a good fit and the steps should be</span>
0130 <span class="comment">%       accepted. Regularization lets rho go to 1 as the model decrease and</span>
0131 <span class="comment">%       the actual decrease go to zero. Set this option to zero to disable</span>
0132 <span class="comment">%       regularization (not recommended). See in-code for the specifics.</span>
0133 <span class="comment">%       When this is not zero, it may happen that the iterates produced are</span>
0134 <span class="comment">%       not monotonically improving the cost when very close to</span>
0135 <span class="comment">%       convergence. This is because the corrected cost improvement could</span>
0136 <span class="comment">%       change sign if it is negative but very small.</span>
0137 <span class="comment">%   statsfun (none)</span>
0138 <span class="comment">%       Function handle to a function that is called after each iteration</span>
0139 <span class="comment">%       to provide the opportunity to log additional statistics.</span>
0140 <span class="comment">%       They are returned in the info struct. See the generic Manopt</span>
0141 <span class="comment">%       documentation about solvers for further information. statsfun is</span>
0142 <span class="comment">%       called with the point x that was reached last, after the</span>
0143 <span class="comment">%       accept/reject decision. See comment below.</span>
0144 <span class="comment">%   stopfun (none)</span>
0145 <span class="comment">%       Function handle to a function that is called at each iteration to</span>
0146 <span class="comment">%       provide the opportunity to specify additional stopping criteria.</span>
0147 <span class="comment">%       See the generic Manopt documentation about solvers for further</span>
0148 <span class="comment">%       information.</span>
0149 <span class="comment">%   verbosity (2)</span>
0150 <span class="comment">%       Integer number used to tune the amount of output the algorithm logs</span>
0151 <span class="comment">%       during execution (mostly as text in the command window).</span>
0152 <span class="comment">%       The higher, the more output. 0 means silent. 3 and above includes a</span>
0153 <span class="comment">%       display of the options structure at the beginning of the execution.</span>
0154 <span class="comment">%   debug (false)</span>
0155 <span class="comment">%       Set to true to allow the algorithm to perform additional</span>
0156 <span class="comment">%       computations for debugging purposes. If a debugging test fails, you</span>
0157 <span class="comment">%       will be informed of it, usually via the command window. Be aware</span>
0158 <span class="comment">%       that these additional computations appear in the algorithm timings</span>
0159 <span class="comment">%       too, and may interfere with operations such as counting the number</span>
0160 <span class="comment">%       of cost evaluations, etc. The debug calls get storedb too.</span>
0161 <span class="comment">%   storedepth (2)</span>
0162 <span class="comment">%       Maximum number of different points x of the manifold for which a</span>
0163 <span class="comment">%       store structure may be kept in memory in the storedb for caching.</span>
0164 <span class="comment">%       If memory usage is an issue, you may try to lower this number.</span>
0165 <span class="comment">%       Profiling or manopt counters may then help to investigate if a</span>
0166 <span class="comment">%       performance hit was incurred as a result.</span>
0167 <span class="comment">%   hook (none)</span>
0168 <span class="comment">%       A function handle which allows the user to change the current point</span>
0169 <span class="comment">%       x at the beginning of each iteration, before the stopping criterion</span>
0170 <span class="comment">%       is evaluated. See applyHook for help on how to use this option.</span>
0171 <span class="comment">%</span>
0172 <span class="comment">% Notice that statsfun is called with the point x that was reached last,</span>
0173 <span class="comment">% after the accept/reject decision. Hence: if the step was accepted, we get</span>
0174 <span class="comment">% that new x, with a store which only saw the call for the cost and for the</span>
0175 <span class="comment">% gradient. If the step was rejected, we get the same x as previously, with</span>
0176 <span class="comment">% the store structure containing everything that was computed at that point</span>
0177 <span class="comment">% (possibly including previous rejects at that same point). Hence, statsfun</span>
0178 <span class="comment">% should not be used in conjunction with the store to count operations for</span>
0179 <span class="comment">% example. Instead, you should use manopt counters: see statscounters.</span>
0180 <span class="comment">%</span>
0181 <span class="comment">%</span>
0182 <span class="comment">% Please cite the Manopt paper as well as the research paper:</span>
0183 <span class="comment">%     @Article{genrtr,</span>
0184 <span class="comment">%       Title    = {Trust-region methods on {Riemannian} manifolds},</span>
0185 <span class="comment">%       Author   = {Absil, P.-A. and Baker, C. G. and Gallivan, K. A.},</span>
0186 <span class="comment">%       Journal  = {Foundations of Computational Mathematics},</span>
0187 <span class="comment">%       Year     = {2007},</span>
0188 <span class="comment">%       Number   = {3},</span>
0189 <span class="comment">%       Pages    = {303--330},</span>
0190 <span class="comment">%       Volume   = {7},</span>
0191 <span class="comment">%       Doi      = {10.1007/s10208-005-0179-9}</span>
0192 <span class="comment">%     }</span>
0193 <span class="comment">%</span>
0194 <span class="comment">% See also: steepestdescent conjugategradient manopt/examples</span>
0195 
0196 <span class="comment">% An explicit, general listing of this algorithm, with preconditioning,</span>
0197 <span class="comment">% can be found in the following paper:</span>
0198 <span class="comment">%     @Article{boumal2015lowrank,</span>
0199 <span class="comment">%       Title   = {Low-rank matrix completion via preconditioned optimization on the {G}rassmann manifold},</span>
0200 <span class="comment">%       Author  = {Boumal, N. and Absil, P.-A.},</span>
0201 <span class="comment">%       Journal = {Linear Algebra and its Applications},</span>
0202 <span class="comment">%       Year    = {2015},</span>
0203 <span class="comment">%       Pages   = {200--239},</span>
0204 <span class="comment">%       Volume  = {475},</span>
0205 <span class="comment">%       Doi     = {10.1016/j.laa.2015.02.027},</span>
0206 <span class="comment">%     }</span>
0207 
0208 <span class="comment">% When the Hessian is not specified, it is approximated with</span>
0209 <span class="comment">% finite-differences of the gradient. The resulting method is called</span>
0210 <span class="comment">% RTR-FD. Some convergence theory for it is available in this paper:</span>
0211 <span class="comment">% @incollection{boumal2015rtrfd</span>
0212 <span class="comment">%    author={Boumal, N.},</span>
0213 <span class="comment">%    title={Riemannian trust regions with finite-difference Hessian approximations are globally convergent},</span>
0214 <span class="comment">%    year={2015},</span>
0215 <span class="comment">%    booktitle={Geometric Science of Information}</span>
0216 <span class="comment">% }</span>
0217 
0218 
0219 <span class="comment">% This file is part of Manopt: www.manopt.org.</span>
0220 <span class="comment">% This code is an adaptation to Manopt of the original GenRTR code:</span>
0221 <span class="comment">% RTR - Riemannian Trust-Region</span>
0222 <span class="comment">% (c) 2004-2007, P.-A. Absil, C. G. Baker, K. A. Gallivan</span>
0223 <span class="comment">% Florida State University</span>
0224 <span class="comment">% School of Computational Science</span>
0225 <span class="comment">% (http://www.math.fsu.edu/~cbaker/GenRTR/?page=download)</span>
0226 <span class="comment">% See accompanying license file.</span>
0227 <span class="comment">% The adaptation was executed by Nicolas Boumal.</span>
0228 <span class="comment">%</span>
0229 <span class="comment">%</span>
0230 <span class="comment">% Change log:</span>
0231 <span class="comment">%</span>
0232 <span class="comment">%   NB April 3, 2013:</span>
0233 <span class="comment">%       tCG now returns the Hessian along the returned direction eta, so</span>
0234 <span class="comment">%       that we do not compute that Hessian redundantly: some savings at</span>
0235 <span class="comment">%       each iteration. Similarly, if the useRand flag is on, we spare an</span>
0236 <span class="comment">%       extra Hessian computation at each outer iteration too, owing to</span>
0237 <span class="comment">%       some modifications in the Cauchy point section of the code specific</span>
0238 <span class="comment">%       to useRand = true.</span>
0239 <span class="comment">%</span>
0240 <span class="comment">%   NB Aug. 22, 2013:</span>
0241 <span class="comment">%       This function is now Octave compatible. The transition called for</span>
0242 <span class="comment">%       two changes which would otherwise not be advisable. (1) tic/toc is</span>
0243 <span class="comment">%       now used as is, as opposed to the safer way:</span>
0244 <span class="comment">%       t = tic(); elapsed = toc(t);</span>
0245 <span class="comment">%       And (2), the (formerly inner) function savestats was moved outside</span>
0246 <span class="comment">%       the main function to not be nested anymore. This is arguably less</span>
0247 <span class="comment">%       elegant, but Octave does not (and likely will not) support nested</span>
0248 <span class="comment">%       functions.</span>
0249 <span class="comment">%</span>
0250 <span class="comment">%   NB Dec. 2, 2013:</span>
0251 <span class="comment">%       The in-code documentation was largely revised and expanded.</span>
0252 <span class="comment">%</span>
0253 <span class="comment">%   NB Dec. 2, 2013:</span>
0254 <span class="comment">%       The former heuristic which triggered when rhonum was very small and</span>
0255 <span class="comment">%       forced rho = 1 has been replaced by a smoother heuristic which</span>
0256 <span class="comment">%       consists in regularizing rhonum and rhoden before computing their</span>
0257 <span class="comment">%       ratio. It is tunable via options.rho_regularization. Furthermore,</span>
0258 <span class="comment">%       the solver now detects if tCG did not obtain a model decrease</span>
0259 <span class="comment">%       (which is theoretically impossible but may happen because of</span>
0260 <span class="comment">%       numerical errors and/or because of a nonlinear/nonsymmetric Hessian</span>
0261 <span class="comment">%       operator, which is the case for finite difference approximations).</span>
0262 <span class="comment">%       When such an anomaly is detected, the step is rejected and the</span>
0263 <span class="comment">%       trust region radius is decreased.</span>
0264 <span class="comment">%       Feb. 18, 2015 note: this is less useful now, as tCG now guarantees</span>
0265 <span class="comment">%       model decrease even for the finite difference approximation of the</span>
0266 <span class="comment">%       Hessian. It is still useful in case of numerical errors, but this</span>
0267 <span class="comment">%       is less stringent.</span>
0268 <span class="comment">%</span>
0269 <span class="comment">%   NB Dec. 3, 2013:</span>
0270 <span class="comment">%       The stepsize is now registered at each iteration, at a small</span>
0271 <span class="comment">%       additional cost. The defaults for Delta_bar and Delta0 are better</span>
0272 <span class="comment">%       defined. Setting Delta_bar in the options will automatically set</span>
0273 <span class="comment">%       Delta0 accordingly. In Manopt 1.0.4, the defaults for these options</span>
0274 <span class="comment">%       were not treated appropriately because of an incorrect use of the</span>
0275 <span class="comment">%       isfield() built-in function.</span>
0276 <span class="comment">%</span>
0277 <span class="comment">%   NB Feb. 18, 2015:</span>
0278 <span class="comment">%       Added some comments. Also, Octave now supports safe tic/toc usage,</span>
0279 <span class="comment">%       so we reverted the changes to use that again (see Aug. 22, 2013 log</span>
0280 <span class="comment">%       entry).</span>
0281 <span class="comment">%</span>
0282 <span class="comment">%   NB April 3, 2015:</span>
0283 <span class="comment">%       Works with the new StoreDB class system.</span>
0284 <span class="comment">%</span>
0285 <span class="comment">%   NB April 8, 2015:</span>
0286 <span class="comment">%       No Hessian warning if approximate Hessian explicitly available.</span>
0287 <span class="comment">%</span>
0288 <span class="comment">%   NB Nov. 1, 2016:</span>
0289 <span class="comment">%       Now uses approximate gradient via finite differences if need be.</span>
0290 <span class="comment">%</span>
0291 <span class="comment">%   NB Aug. 2, 2018:</span>
0292 <span class="comment">%       Using storedb.remove() to keep the cache lean, which allowed to</span>
0293 <span class="comment">%       reduce storedepth to 2 from 20 (by default).</span>
0294 <span class="comment">%</span>
0295 <span class="comment">%   NB July 19, 2020:</span>
0296 <span class="comment">%       Added support for options.hook.</span>
0297 <span class="comment">%</span>
0298 <span class="comment">%   VL Aug. 17, 2022:</span>
0299 <span class="comment">%       Refactored code to use various subproblem solvers with a new input</span>
0300 <span class="comment">%       output pattern. Modified how information about iterations is</span>
0301 <span class="comment">%       printed to accomodate new subproblem solvers. Moved all useRand and</span>
0302 <span class="comment">%       cauchy logic to trs_tCG. Options pertaining to tCG are still</span>
0303 <span class="comment">%       available but have moved to that file. Made trs_tCG_cached default.</span>
0304 
0305 
0306 <span class="comment">% Verify that the problem description is sufficient for the solver.</span>
0307 
0308 <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetCost.html" class="code" title="function candoit = canGetCost(problem)">canGetCost</a>(problem)
0309     warning(<span class="string">'manopt:getCost'</span>, <span class="keyword">...</span>
0310             <span class="string">'No cost provided. The algorithm will likely abort.'</span>);  
0311 <span class="keyword">end</span>
0312 <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetGradient.html" class="code" title="function candoit = canGetGradient(problem)">canGetGradient</a>(problem) &amp;&amp; ~<a href="../../../manopt/core/canGetApproxGradient.html" class="code" title="function candoit = canGetApproxGradient(problem)">canGetApproxGradient</a>(problem)
0313     <span class="comment">% Note: we do not give a warning if an approximate gradient is</span>
0314     <span class="comment">% explicitly given in the problem description, as in that case the user</span>
0315     <span class="comment">% seems to be aware of the issue.</span>
0316     warning(<span class="string">'manopt:getGradient:approx'</span>, <span class="keyword">...</span>
0317            [<span class="string">'No gradient provided. Using FD approximation (slow).\n'</span> <span class="keyword">...</span>
0318             <span class="string">'It may be necessary to increase options.tolgradnorm.\n'</span> <span class="keyword">...</span>
0319             <span class="string">'To disable this warning: '</span> <span class="keyword">...</span>
0320             <span class="string">'warning(''off'', ''manopt:getGradient:approx'')'</span>]);
0321     problem.approxgrad = <a href="../../../manopt/solvers/gradientapproximations/approxgradientFD.html" class="code" title="function gradfun = approxgradientFD(problem, options)">approxgradientFD</a>(problem);
0322 <span class="keyword">end</span>
0323 <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetHessian.html" class="code" title="function candoit = canGetHessian(problem)">canGetHessian</a>(problem) &amp;&amp; ~<a href="../../../manopt/core/canGetApproxHessian.html" class="code" title="function candoit = canGetApproxHessian(problem)">canGetApproxHessian</a>(problem)
0324     <span class="comment">% Note: we do not give a warning if an approximate Hessian is</span>
0325     <span class="comment">% explicitly given in the problem description, as in that case the user</span>
0326     <span class="comment">% seems to be aware of the issue.</span>
0327     warning(<span class="string">'manopt:getHessian:approx'</span>, <span class="keyword">...</span>
0328            [<span class="string">'No Hessian provided. Using FD approximation.\n'</span> <span class="keyword">...</span>
0329             <span class="string">'To disable this warning: '</span> <span class="keyword">...</span>
0330             <span class="string">'warning(''off'', ''manopt:getHessian:approx'')'</span>]);
0331     problem.approxhess = <a href="../../../manopt/solvers/hessianapproximations/approxhessianFD.html" class="code" title="function hessfun = approxhessianFD(problem, options)">approxhessianFD</a>(problem);
0332 <span class="keyword">end</span>
0333 
0334 <span class="comment">% Set local defaults here</span>
0335 localdefaults.verbosity = 2;
0336 localdefaults.maxtime = inf;
0337 localdefaults.miniter = 0;
0338 localdefaults.maxiter = 1000;
0339 localdefaults.rho_prime = 0.1;
0340 localdefaults.rho_regularization = 1e3;
0341 localdefaults.subproblemsolver = @<a href="trs_tCG_cached.html" class="code" title="function trsoutput = trs_tCG_cached(problem, trsinput, options, storedb, key)">trs_tCG_cached</a>;
0342 localdefaults.tolgradnorm = 1e-6;
0343 
0344 <span class="comment">% Merge global and local defaults, then merge w/ user options, if any.</span>
0345 localdefaults = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>(<a href="../../../manopt/core/getGlobalDefaults.html" class="code" title="function opts = getGlobalDefaults()">getGlobalDefaults</a>(), localdefaults);
0346 <span class="keyword">if</span> ~exist(<span class="string">'options'</span>, <span class="string">'var'</span>) || isempty(options)
0347     options = struct();
0348 <span class="keyword">end</span>
0349 options = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>(localdefaults, options);
0350 
0351 M = problem.M;
0352 
0353 <span class="comment">% If no initial point x is given by the user, generate one at random.</span>
0354 <span class="keyword">if</span> ~exist(<span class="string">'x'</span>, <span class="string">'var'</span>) || isempty(x)
0355     x = M.rand();
0356 <span class="keyword">end</span>
0357 
0358 <span class="comment">% Set default Delta_bar and Delta0 separately to deal with additional</span>
0359 <span class="comment">% logic: if Delta_bar is provided but not Delta0, let Delta0 automatically</span>
0360 <span class="comment">% be some fraction of the provided Delta_bar.</span>
0361 <span class="keyword">if</span> ~isfield(options, <span class="string">'Delta_bar'</span>)
0362     <span class="keyword">if</span> isfield(M, <span class="string">'typicaldist'</span>)
0363         options.Delta_bar = M.typicaldist();
0364     <span class="keyword">else</span>
0365         options.Delta_bar = sqrt(M.dim());
0366     <span class="keyword">end</span> 
0367 <span class="keyword">end</span>
0368 <span class="keyword">if</span> ~isfield(options, <span class="string">'Delta0'</span>)
0369     options.Delta0 = options.Delta_bar / 8;
0370 <span class="keyword">end</span>
0371 
0372 <span class="comment">% Check some option values</span>
0373 assert(options.rho_prime &lt; 1/4, <span class="keyword">...</span>
0374         <span class="string">'options.rho_prime must be strictly smaller than 1/4.'</span>);
0375 assert(options.Delta_bar &gt; 0, <span class="keyword">...</span>
0376         <span class="string">'options.Delta_bar must be positive.'</span>);
0377 assert(options.Delta0 &gt; 0 &amp;&amp; options.Delta0 &lt;= options.Delta_bar, <span class="keyword">...</span>
0378         <span class="string">'options.Delta0 must be positive and smaller than Delta_bar.'</span>);
0379 
0380 <span class="comment">% It is sometimes useful to check what the actual option values are.</span>
0381 <span class="keyword">if</span> options.verbosity &gt;= 3
0382     <a href="../../../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/disp.html" class="code" title="function disp( x, name )">disp</a>(options);
0383 <span class="keyword">end</span>
0384 
0385 <span class="comment">% Create a store database and get a key for the current x</span>
0386 storedb = <a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>(options.storedepth);
0387 key = storedb.getNewKey();
0388 
0389 ticstart = tic();
0390 
0391 <span class="comment">%% Initializations</span>
0392 
0393 <span class="comment">% k counts the outer (TR) iterations. The semantic is that k counts the</span>
0394 <span class="comment">% number of iterations fully executed so far.</span>
0395 k = 0;
0396 
0397 <span class="comment">% accept tracks if the proposed step is accepted (true) or declined (false)</span>
0398 accept = true;
0399 
0400 <span class="comment">% Initialize solution and companion measures: f(x), fgrad(x)</span>
0401 [fx, fgradx] = <a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>(problem, x, storedb, key);
0402 norm_grad = M.norm(x, fgradx);
0403 
0404 <span class="comment">% Initialize trust-region radius</span>
0405 Delta = options.Delta0;
0406 
0407 <span class="comment">% Depending on the subproblem solver, different kinds of statistics are</span>
0408 <span class="comment">% logged and displayed. This initial call to the solver tells us ahead of</span>
0409 <span class="comment">% time what to write in the column headers for displayed information, and</span>
0410 <span class="comment">% how to initialize the info struct-array.</span>
0411 trsinfo = options.subproblemsolver([], [], options);
0412 
0413 <span class="comment">% printheader is a string that contains the header for the subproblem</span>
0414 <span class="comment">% solver's printed output.</span>
0415 printheader = trsinfo.printheader;
0416 
0417 <span class="comment">% initstats is a struct of initial values for the stats that the subproblem</span>
0418 <span class="comment">% solver wishes to store.</span>
0419 initstats = trsinfo.initstats;
0420 
0421 stats = <a href="#_sub1" class="code" title="subfunction stats = savestats(problem, x, storedb, key, options, k, fx, ">savestats</a>(problem, x, storedb, key, options, k, fx, norm_grad, <span class="keyword">...</span>
0422                                          Delta, ticstart, initstats);
0423 
0424 info(1) = stats;
0425 info(min(10000, options.maxiter+1)).iter = [];
0426 
0427 <span class="comment">% Display headers, depending on verbosity level, then also the initial row.</span>
0428 <span class="keyword">if</span> options.verbosity == 2
0429     fprintf([<span class="string">'%3s %3s    iter   '</span>, <span class="keyword">...</span>
0430              <span class="string">'%15scost val   %2sgrad. norm   %s\n'</span>], <span class="keyword">...</span>
0431              <span class="string">'   '</span>, <span class="string">'   '</span>, <span class="string">'        '</span>, <span class="string">'  '</span>, printheader);
0432     fprintf([<span class="string">'%3s %3s   %5d   '</span>, <span class="keyword">...</span>
0433              <span class="string">'%+.16e   %12e\n'</span>], <span class="keyword">...</span>
0434              <span class="string">'   '</span>, <span class="string">'   '</span>, k, fx, norm_grad);
0435 <span class="keyword">elseif</span> options.verbosity &gt; 2
0436     fprintf([<span class="string">'%3s %3s    iter   '</span>, <span class="keyword">...</span>
0437              <span class="string">'%15scost val   %2sgrad. norm   %10srho   %4srho_noreg   '</span> <span class="keyword">...</span>
0438              <span class="string">'%7sDelta   %s\n'</span>], <span class="keyword">...</span>
0439              <span class="string">'   '</span>, <span class="string">'   '</span>, <span class="string">'        '</span>, <span class="string">'  '</span>, <span class="string">'         '</span>, <span class="string">'   '</span>, <span class="keyword">...</span>
0440              <span class="string">'       '</span>, printheader);
0441     fprintf([<span class="string">'%3s %3s   %5d   '</span>, <span class="keyword">...</span>
0442              <span class="string">'%+.16e   %12e\n'</span>], <span class="keyword">...</span>
0443              <span class="string">'   '</span>,<span class="string">'   '</span>,k, fx, norm_grad);
0444 <span class="keyword">end</span>
0445 
0446 <span class="comment">% To keep track of consecutive radius changes, so that we can warn the</span>
0447 <span class="comment">% user if it appears necessary.</span>
0448 consecutive_TRplus = 0;
0449 consecutive_TRminus = 0;
0450 
0451 
0452 <span class="comment">% **********************</span>
0453 <span class="comment">% ** Start of TR loop **</span>
0454 <span class="comment">% **********************</span>
0455 <span class="keyword">while</span> true
0456     
0457     <span class="comment">% Start clock for this outer iteration</span>
0458     ticstart = tic();
0459 
0460     <span class="comment">% Apply the hook function if there is one: this allows external code to</span>
0461     <span class="comment">% move x to another point. If the point is changed (indicated by a true</span>
0462     <span class="comment">% value for the boolean 'hooked'), we update our knowledge about x.</span>
0463     [x, key, info, hooked] = <a href="../../../manopt/core/applyHook.html" class="code" title="function [newx, newkey, info, hooked] = applyHook(problem, x, storedb, key, options, info, last)">applyHook</a>(problem, x, storedb, key, <span class="keyword">...</span>
0464                                                        options, info, k+1);
0465     <span class="keyword">if</span> hooked
0466         [fx, fgradx] = <a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>(problem, x, storedb, key);
0467         norm_grad = M.norm(x, fgradx);
0468     <span class="keyword">end</span>
0469     
0470     <span class="comment">% Run standard stopping criterion checks</span>
0471     [stop, reason] = <a href="../../../manopt/core/stoppingcriterion.html" class="code" title="function [stop, reason] = stoppingcriterion(problem, x, options, info, last)">stoppingcriterion</a>(problem, x, options, info, k+1);
0472     
0473     <span class="comment">% Ensure trustregions runs at least options.miniter iterations</span>
0474     <span class="keyword">if</span> k &lt; options.miniter
0475         stop = 0;
0476     <span class="keyword">end</span>
0477     
0478     <span class="keyword">if</span> stop
0479         <span class="keyword">if</span> options.verbosity &gt;= 1
0480             fprintf([reason <span class="string">'\n'</span>]);
0481         <span class="keyword">end</span>
0482         <span class="keyword">break</span>;
0483     <span class="keyword">end</span>
0484 
0485     <span class="keyword">if</span> options.debug &gt; 0
0486         fprintf([repmat(<span class="string">'*'</span>, 1, 98) <span class="string">'\n'</span>]);
0487     <span class="keyword">end</span>
0488 
0489     <span class="comment">% *************************</span>
0490     <span class="comment">% ** Begin TR Subproblem **</span>
0491     <span class="comment">% *************************</span>
0492   
0493     <span class="comment">% Solve TR subproblem with solver specified by options.subproblemsolver</span>
0494     trsinput = struct(<span class="string">'x'</span>, x, <span class="string">'fgradx'</span>, fgradx, <span class="string">'Delta'</span>, Delta, <span class="keyword">...</span>
0495                       <span class="string">'accept'</span>, accept);
0496 
0497     trsoutput = options.subproblemsolver(problem, trsinput, options, <span class="keyword">...</span>
0498                                          storedb, key);
0499     
0500     eta = trsoutput.eta;
0501     Heta = trsoutput.Heta;
0502     limitedbyTR = trsoutput.limitedbyTR;
0503     trsprintstr = trsoutput.printstr;
0504     trsstats = trsoutput.stats;
0505 
0506         
0507     <span class="comment">% This is computed for logging purposes and may be useful for some</span>
0508     <span class="comment">% user-defined stopping criteria.</span>
0509     norm_eta = M.norm(x, eta);
0510     
0511     <span class="keyword">if</span> options.debug &gt; 0
0512         testangle = M.inner(x, eta, fgradx) / (norm_eta*norm_grad);
0513     <span class="keyword">end</span>
0514     
0515 
0516     <span class="comment">% Compute the tentative next iterate (the proposal)</span>
0517     x_prop = M.retr(x, eta);
0518     key_prop = storedb.getNewKey();
0519 
0520     <span class="comment">% Compute the function value of the proposal</span>
0521     fx_prop = <a href="../../../manopt/core/getCost.html" class="code" title="function cost = getCost(problem, x, storedb, key)">getCost</a>(problem, x_prop, storedb, key_prop);
0522 
0523     <span class="comment">% Will we accept the proposal or not?</span>
0524     <span class="comment">% Check the performance of the quadratic model against the actual cost.</span>
0525     rhonum = fx - fx_prop;
0526     vecrho = M.lincomb(x, 1, fgradx, .5, Heta);
0527     rhoden = -M.inner(x, eta, vecrho);
0528     rho_noreg = rhonum/rhoden;
0529     <span class="comment">% rhonum could be anything.</span>
0530     <span class="comment">% rhoden should be nonnegative, as guaranteed by tCG, barring</span>
0531     <span class="comment">% numerical errors.</span>
0532     
0533     <span class="comment">% Heuristic -- added Dec. 2, 2013 (NB) to replace the former heuristic.</span>
0534     <span class="comment">% This heuristic is documented in the book by Conn Gould and Toint on</span>
0535     <span class="comment">% trust-region methods, section 17.4.2.</span>
0536     <span class="comment">% rhonum measures the difference between two numbers. Close to</span>
0537     <span class="comment">% convergence, these two numbers are very close to each other, so</span>
0538     <span class="comment">% that computing their difference is numerically challenging: there may</span>
0539     <span class="comment">% be a significant loss in accuracy. Since the acceptance or rejection</span>
0540     <span class="comment">% of the step is conditioned on the ratio between rhonum and rhoden,</span>
0541     <span class="comment">% large errors in rhonum result in a very large error in rho, hence in</span>
0542     <span class="comment">% erratic acceptance / rejection. Meanwhile, close to convergence,</span>
0543     <span class="comment">% steps are usually trustworthy and we should transition to a Newton-</span>
0544     <span class="comment">% like method, with rho=1 consistently. The heuristic thus shifts both</span>
0545     <span class="comment">% rhonum and rhoden by a small amount such that far from convergence,</span>
0546     <span class="comment">% the shift is irrelevant and close to convergence, the ratio rho goes</span>
0547     <span class="comment">% to 1, effectively promoting acceptance of the step.</span>
0548     <span class="comment">% The rationale is that close to convergence, both rhonum and rhoden</span>
0549     <span class="comment">% are quadratic in the distance between x and x_prop. Thus, when this</span>
0550     <span class="comment">% distance is on the order of sqrt(eps), the value of rhonum and rhoden</span>
0551     <span class="comment">% is on the order of eps, which is indistinguishable from the numerical</span>
0552     <span class="comment">% error, resulting in badly estimated rho's.</span>
0553     <span class="comment">% For abs(fx) &lt; 1, this heuristic is invariant under offsets of f but</span>
0554     <span class="comment">% not under scaling of f. For abs(fx) &gt; 1, the opposite holds. This</span>
0555     <span class="comment">% should not alarm us, as this heuristic only triggers at the very last</span>
0556     <span class="comment">% iterations if very fine convergence is demanded.</span>
0557     rho_reg_offset = max(1, abs(fx)) * eps * options.rho_regularization;
0558     rhonum = rhonum + rho_reg_offset;
0559     rhoden = rhoden + rho_reg_offset;
0560    
0561     <span class="keyword">if</span> options.debug &gt; 0
0562         fprintf(<span class="string">'DBG:     rhonum : %e\n'</span>, rhonum);
0563         fprintf(<span class="string">'DBG:     rhoden : %e\n'</span>, rhoden);
0564     <span class="keyword">end</span>
0565     
0566     <span class="comment">% This is always true if a linear, symmetric operator is used for the</span>
0567     <span class="comment">% Hessian (approximation) and if we had infinite numerical precision.</span>
0568     <span class="comment">% In practice, nonlinear approximations of the Hessian such as the</span>
0569     <span class="comment">% built-in finite difference approximation and finite numerical</span>
0570     <span class="comment">% accuracy can cause the model to increase. In such scenarios, we</span>
0571     <span class="comment">% decide to force a rejection of the step and a reduction of the</span>
0572     <span class="comment">% trust-region radius. We test the sign of the regularized rhoden since</span>
0573     <span class="comment">% the regularization is supposed to capture the accuracy to which</span>
0574     <span class="comment">% rhoden is computed: if rhoden were negative before regularization but</span>
0575     <span class="comment">% not after, that should not be (and is not) detected as a failure.</span>
0576     <span class="comment">%</span>
0577     <span class="comment">% Note (Feb. 17, 2015, NB): the most recent version of trs_tCG already</span>
0578     <span class="comment">% includes a mechanism to ensure model decrease if the Cauchy step</span>
0579     <span class="comment">% attained a decrease (which is theoretically the case under very lax</span>
0580     <span class="comment">% assumptions). This being said, it is always possible that numerical</span>
0581     <span class="comment">% errors will prevent this, so that it is good to keep a safeguard.</span>
0582     <span class="comment">%</span>
0583     <span class="comment">% The current strategy is that, if this should happen, then we reject</span>
0584     <span class="comment">% the step and reduce the trust region radius. This also ensures that</span>
0585     <span class="comment">% the actual cost values are monotonically decreasing.</span>
0586     <span class="comment">%</span>
0587     <span class="comment">% [This bit of code seems not to trigger since trs_tCG already ensures</span>
0588     <span class="comment">%  the model decreases even in the presence of non-linearities; but as</span>
0589     <span class="comment">%  a result the radius is not necessarily decreased. Perhaps we should</span>
0590     <span class="comment">%  change this with the proposed commented line below; needs testing.]</span>
0591     <span class="comment">%</span>
0592     model_decreased = (rhoden &gt;= 0);
0593     
0594     <span class="keyword">if</span> ~model_decreased
0595         trsprintstr = [trsprintstr <span class="string">', model did not decrease'</span>]; <span class="comment">%#ok&lt;AGROW&gt;</span>
0596     <span class="keyword">end</span>
0597     rho = rhonum / rhoden;
0598     
0599     <span class="comment">% Added June 30, 2015 following observation by BM.</span>
0600     <span class="comment">% With this modification, it is guaranteed that a step rejection is</span>
0601     <span class="comment">% always accompanied by a TR reduction. This prevents stagnation in</span>
0602     <span class="comment">% this &quot;corner case&quot; (NaN's really aren't supposed to occur, but it's</span>
0603     <span class="comment">% nice if we can handle them nonetheless).</span>
0604     <span class="keyword">if</span> isnan(rho)
0605         fprintf([<span class="string">'rho is NaN! Forcing a radius decrease. '</span> <span class="keyword">...</span>
0606                  <span class="string">'This should not happen.\n'</span>]);
0607         <span class="keyword">if</span> isnan(fx_prop)
0608             fprintf([<span class="string">'The cost function returned NaN (perhaps the '</span> <span class="keyword">...</span>
0609                      <span class="string">'retraction returned a bad point?)\n'</span>]);
0610         <span class="keyword">else</span>
0611             fprintf(<span class="string">'The cost function did not return a NaN value.\n'</span>);
0612         <span class="keyword">end</span>
0613     <span class="keyword">end</span>
0614    
0615     <span class="keyword">if</span> options.debug &gt; 0
0616         m = @(x, eta) <span class="keyword">...</span>
0617           <a href="../../../manopt/core/getCost.html" class="code" title="function cost = getCost(problem, x, storedb, key)">getCost</a>(problem, x, storedb, key) + <span class="keyword">...</span>
0618           <a href="../../../manopt/core/getDirectionalDerivative.html" class="code" title="function diff = getDirectionalDerivative(problem, x, d, storedb, key)">getDirectionalDerivative</a>(problem, x, eta, storedb, key) + <span class="keyword">...</span>
0619              .5*M.inner(x, <a href="../../../manopt/core/getHessian.html" class="code" title="function hess = getHessian(problem, x, d, storedb, key)">getHessian</a>(problem, x, eta, storedb, key), eta);
0620         zerovec = M.zerovec(x);
0621         actrho = (fx - fx_prop) / (m(x, zerovec) - m(x, eta));
0622         fprintf(<span class="string">'DBG:   new f(x) : %+e\n'</span>, fx_prop);
0623         fprintf(<span class="string">'DBG: actual rho : %e\n'</span>, actrho);
0624         fprintf(<span class="string">'DBG:   used rho : %e\n'</span>, rho);
0625     <span class="keyword">end</span>
0626 
0627     <span class="comment">% Choose the new TR radius based on the model performance</span>
0628     trstr = <span class="string">'   '</span>;
0629     <span class="comment">% If the actual decrease is smaller than 1/4 of the predicted decrease,</span>
0630     <span class="comment">% then reduce the TR radius.</span>
0631     <span class="keyword">if</span> rho &lt; 1/4 || ~model_decreased || isnan(rho)
0632         trstr = <span class="string">'TR-'</span>;
0633         Delta = Delta/4;
0634         consecutive_TRplus = 0;
0635         consecutive_TRminus = consecutive_TRminus + 1;
0636         <span class="keyword">if</span> consecutive_TRminus &gt;= 5 &amp;&amp; options.verbosity &gt;= 2
0637             consecutive_TRminus = -inf;
0638             fprintf([<span class="string">' +++ Detected many consecutive TR- (radius '</span> <span class="keyword">...</span>
0639                      <span class="string">'decreases).\n'</span> <span class="keyword">...</span>
0640                      <span class="string">' +++ Consider dividing options.Delta_bar by 10.\n'</span> <span class="keyword">...</span>
0641                      <span class="string">' +++ Current values: options.Delta_bar = %g and '</span> <span class="keyword">...</span>
0642                      <span class="string">'options.Delta0 = %g.\n'</span>], options.Delta_bar, <span class="keyword">...</span>
0643                                                 options.Delta0);
0644         <span class="keyword">end</span>
0645     <span class="comment">% If the actual decrease is at least 3/4 of the predicted decrease and</span>
0646     <span class="comment">% the trs_tCG (inner solve) hit the TR boundary, increase TR radius.</span>
0647     <span class="comment">% We also keep track of the number of consecutive trust-region radius</span>
0648     <span class="comment">% increases. If there are many, this may indicate the need to adapt the</span>
0649     <span class="comment">% initial and maximum radii.</span>
0650     <span class="keyword">elseif</span> rho &gt; 3/4 &amp;&amp; limitedbyTR
0651         trstr = <span class="string">'TR+'</span>;
0652         Delta = min(2*Delta, options.Delta_bar);
0653         consecutive_TRminus = 0;
0654         consecutive_TRplus = consecutive_TRplus + 1;
0655         <span class="keyword">if</span> consecutive_TRplus &gt;= 5 &amp;&amp; options.verbosity &gt;= 1
0656             consecutive_TRplus = -inf;
0657             fprintf([<span class="string">' +++ Detected many consecutive TR+ (radius '</span> <span class="keyword">...</span>
0658                      <span class="string">'increases).\n'</span> <span class="keyword">...</span>
0659                      <span class="string">' +++ Consider multiplying options.Delta_bar by 10.\n'</span> <span class="keyword">...</span>
0660                      <span class="string">' +++ Current values: options.Delta_bar = %g and '</span> <span class="keyword">...</span>
0661                      <span class="string">'options.Delta0 = %g.\n'</span>], options.Delta_bar, <span class="keyword">...</span>
0662                                                 options.Delta0);
0663         <span class="keyword">end</span>
0664     <span class="keyword">else</span>
0665         <span class="comment">% Otherwise, keep the TR radius constant.</span>
0666         consecutive_TRplus = 0;
0667         consecutive_TRminus = 0;
0668     <span class="keyword">end</span>
0669 
0670     <span class="comment">% Choose to accept or reject the proposed step based on the model</span>
0671     <span class="comment">% performance. Note the strict inequality.</span>
0672     <span class="keyword">if</span> model_decreased &amp;&amp; rho &gt; options.rho_prime
0673         
0674         <span class="comment">% April 17, 2018: a side effect of rho_regularization &gt; 0 is that</span>
0675         <span class="comment">% it can happen that the cost function appears to go up (although</span>
0676         <span class="comment">% only by a small amount) for some accepted steps. We decide to</span>
0677         <span class="comment">% accept this because, numerically, computing the difference</span>
0678         <span class="comment">% between fx_prop and fx is more difficult than computing the</span>
0679         <span class="comment">% improvement in the model, because fx_prop and fx are on the same</span>
0680         <span class="comment">% order of magnitude yet are separated by a very small gap near</span>
0681         <span class="comment">% convergence, whereas the model improvement is computed as a sum</span>
0682         <span class="comment">% of two small terms. As a result, the step which seems bad may</span>
0683         <span class="comment">% turn out to be good, in that it may help reduce the gradient norm</span>
0684         <span class="comment">% for example. This update merely informs the user of this event.</span>
0685         <span class="comment">% In further updates, we could also introduce this as a stopping</span>
0686         <span class="comment">% criterion. It is then important to choose wisely which of x or</span>
0687         <span class="comment">% x_prop should be returned (perhaps the one with smallest</span>
0688         <span class="comment">% gradient?)</span>
0689         <span class="keyword">if</span> fx_prop &gt; fx &amp;&amp; options.verbosity &gt;= 2
0690             fprintf([<span class="string">'Between line above and below, cost function '</span> <span class="keyword">...</span>
0691                      <span class="string">'increased by %.2g (step size: %.2g)\n'</span>], <span class="keyword">...</span>
0692                      fx_prop - fx, norm_eta);
0693         <span class="keyword">end</span>
0694         
0695         accept = true;
0696         accstr = <span class="string">'acc'</span>;
0697         <span class="comment">% We accept the step: no need to keep the old cache.</span>
0698         storedb.removefirstifdifferent(key, key_prop);
0699         x = x_prop;
0700         key = key_prop;
0701         fx = fx_prop;
0702         fgradx = <a href="../../../manopt/core/getGradient.html" class="code" title="function grad = getGradient(problem, x, storedb, key)">getGradient</a>(problem, x, storedb, key);
0703         norm_grad = M.norm(x, fgradx);
0704     <span class="keyword">else</span>
0705         <span class="comment">% We reject the step: no need to keep cache related to the</span>
0706         <span class="comment">% tentative step.</span>
0707         storedb.removefirstifdifferent(key_prop, key);
0708         accept = false;
0709         accstr = <span class="string">'REJ'</span>;
0710     <span class="keyword">end</span>
0711     
0712     <span class="comment">% k is the number of iterations we have accomplished.</span>
0713     k = k + 1;
0714     
0715     <span class="comment">% Make sure we don't use too much memory for the store database.</span>
0716     storedb.purge();
0717 
0718     <span class="comment">% Log statistics for freshly executed iteration.</span>
0719     <span class="comment">% Everything after this in the loop is not accounted for in the timing.</span>
0720     stats = <a href="#_sub1" class="code" title="subfunction stats = savestats(problem, x, storedb, key, options, k, fx, ">savestats</a>(problem, x, storedb, key, options, k, fx, <span class="keyword">...</span>
0721                       norm_grad, Delta, ticstart, trsstats, <span class="keyword">...</span>
0722                       info, rho, rhonum, rhoden, accept, norm_eta, <span class="keyword">...</span>
0723                       limitedbyTR);
0724     info(k+1) = stats;
0725 
0726     <span class="comment">% Display</span>
0727     <span class="keyword">if</span> options.verbosity == 2
0728         fprintf(<span class="string">'%3s %3s   %5d   %+.16e   %12e   %s\n'</span>, <span class="keyword">...</span>
0729                 accstr, trstr, k, fx, norm_grad, trsprintstr);
0730     <span class="keyword">elseif</span> options.verbosity &gt; 2
0731         fprintf([<span class="string">'%3s %3s   %5d   %+.16e   %.6e   %+.6e   '</span> <span class="keyword">...</span>
0732                  <span class="string">'%+.6e   %.6e   %s\n'</span>], <span class="keyword">...</span>
0733                 accstr, trstr, k, fx, norm_grad, rho, <span class="keyword">...</span>
0734                 rho_noreg, Delta, trsprintstr);
0735         <span class="keyword">if</span> options.debug &gt; 0
0736             fprintf(<span class="string">'      Delta : %f          |eta| : %e\n'</span>, <span class="keyword">...</span>
0737                     Delta, norm_eta);
0738         <span class="keyword">end</span>
0739     <span class="keyword">end</span>
0740     <span class="keyword">if</span> options.debug &gt; 0
0741         fprintf(<span class="string">'DBG: cos ang(eta, gradf): %d\n'</span>, testangle);
0742         <span class="keyword">if</span> rho == 0
0743             fprintf(<span class="string">'DBG: rho = 0: likely to hinder convergence.\n'</span>);
0744         <span class="keyword">end</span>
0745     <span class="keyword">end</span>
0746 
0747 <span class="keyword">end</span>  <span class="comment">% of TR loop (counter: k)</span>
0748 
0749 <span class="comment">% Restrict info struct-array to useful part</span>
0750 info = info(1:k+1);
0751 
0752 
0753 <span class="keyword">if</span> options.debug &gt; 0
0754    fprintf([repmat(<span class="string">'*'</span>, 1, 98) <span class="string">'\n'</span>]);
0755 <span class="keyword">end</span>
0756 <span class="keyword">if</span> options.verbosity &gt; 0
0757     fprintf(<span class="string">'Total time is %f [s] (excludes statsfun)\n'</span>, info(end).time);
0758 <span class="keyword">end</span>
0759 
0760 <span class="comment">% Return the best cost reached</span>
0761 cost = fx;
0762 
0763 <span class="keyword">end</span>
0764 
0765 
0766 
0767     
0768 
0769 <span class="comment">% Routine in charge of collecting the current iteration stats</span>
0770 <a name="_sub1" href="#_subfunctions" class="code">function stats = savestats(problem, x, storedb, key, options, k, fx, </a><span class="keyword">...</span>
0771                            norm_grad, Delta, ticstart, trsstats, info, rho, <span class="keyword">...</span>
0772                            rhonum, rhoden, accept, norm_eta, limitedbyTR)
0773     stats.iter = k;
0774     stats.cost = fx;
0775     stats.gradnorm = norm_grad;
0776     stats.Delta = Delta;
0777     <span class="keyword">if</span> k == 0
0778         stats.time = toc(ticstart);
0779         stats.rho = inf;
0780         stats.rhonum = NaN;
0781         stats.rhoden = NaN;
0782         stats.accepted = true;
0783         stats.stepsize = NaN;
0784         stats.limitedbyTR = false;
0785         fields = fieldnames(trsstats);
0786         <span class="keyword">for</span> i = 1 : length(fields)
0787             stats.(fields{i}) = trsstats.(fields{i});
0788         <span class="keyword">end</span>
0789     <span class="keyword">else</span>
0790         stats.time = info(k).time + toc(ticstart);
0791         stats.rho = rho;
0792         stats.rhonum = rhonum;
0793         stats.rhoden = rhoden;
0794         stats.accepted = accept;
0795         stats.stepsize = norm_eta;
0796         stats.limitedbyTR = limitedbyTR;
0797         fields = fieldnames(trsstats);
0798         <span class="keyword">for</span> i = 1 : length(fields)
0799             stats.(fields{i}) = trsstats.(fields{i});
0800         <span class="keyword">end</span>
0801     <span class="keyword">end</span>
0802     
0803     <span class="comment">% See comment about statsfun above: the x and store passed to statsfun</span>
0804     <span class="comment">% are that of the most recently accepted point after the iteration</span>
0805     <span class="comment">% fully executed.</span>
0806     stats = <a href="../../../manopt/core/applyStatsfun.html" class="code" title="function stats = applyStatsfun(problem, x, storedb, key, options, stats)">applyStatsfun</a>(problem, x, storedb, key, options, stats);
0807     
0808 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Fri 30-Sep-2022 13:18:25 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>