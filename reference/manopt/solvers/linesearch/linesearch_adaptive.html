<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of linesearch_adaptive</title>
  <meta name="keywords" content="linesearch_adaptive">
  <meta name="description" content="Adaptive line search algorithm (step size selection) for descent methods.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../index.html">Home</a> &gt;  <a href="#">manopt</a> &gt; <a href="#">solvers</a> &gt; <a href="index.html">linesearch</a> &gt; linesearch_adaptive.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../index.html"><img alt="<" border="0" src="../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for manopt\solvers\linesearch&nbsp;<img alt=">" border="0" src="../../../right.png"></a></td></tr></table>-->

<h1>linesearch_adaptive
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>Adaptive line search algorithm (step size selection) for descent methods.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>function [stepsize, newx, newkey, lsstats] =linesearch_adaptive(problem, x, d, f0, df0, options, storedb, key) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Adaptive line search algorithm (step size selection) for descent methods.

 function [stepsize, newx, newkey, lsstats] = 
        linesearch_adaptive(problem, x, d, f0, df0, options, storedb, key)

 Adaptive linesearch algorithm for descent methods, based on a simple
 backtracking method. Contrary to linesearch.m, this function is not
 invariant under rescaling of the search direction d. These two line
 search methods vary mainly in their strategy to pick the initial step
 size.
 
 Below, the step is constructed as alpha*d, and the step size is the norm
 of that vector, thus: stepsize = alpha*norm_d. The step is executed by
 retracting the vector alpha*d from the current point x, giving newx.

 This line-search may create and maintain a structure called lsmem inside
 storedb.internal. This gives the linesearch the opportunity to remember
 what happened in the previous calls. This is typically used to make a
 first guess at the step size, based on previous events.

 Inputs/Outputs : see help for linesearch

 See also: steepestdescent conjugategradients <a href="linesearch.html" class="code" title="function [stepsize, newx, newkey, lsstats] =linesearch(problem, x, d, f0, df0, options, storedb, key)">linesearch</a></pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>	</li><li><a href="../../../manopt/core/getCost.html" class="code" title="function cost = getCost(problem, x, storedb, key)">getCost</a>	Computes the cost function at x.</li><li><a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts1, opts2)">mergeOptions</a>	Merges two options structures with one having precedence over the other.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../manopt/solvers/conjugategradient/conjugategradient.html" class="code" title="function [x, cost, info, options] = conjugategradient(problem, x, options)">conjugategradient</a>	Conjugate gradient minimization algorithm for Manopt.</li></ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [stepsize, newx, newkey, lsstats] = </a><span class="keyword">...</span>
0002   linesearch_adaptive(problem, x, d, f0, df0, options, storedb, key)
0003 <span class="comment">% Adaptive line search algorithm (step size selection) for descent methods.</span>
0004 <span class="comment">%</span>
0005 <span class="comment">% function [stepsize, newx, newkey, lsstats] =</span>
0006 <span class="comment">%        linesearch_adaptive(problem, x, d, f0, df0, options, storedb, key)</span>
0007 <span class="comment">%</span>
0008 <span class="comment">% Adaptive linesearch algorithm for descent methods, based on a simple</span>
0009 <span class="comment">% backtracking method. Contrary to linesearch.m, this function is not</span>
0010 <span class="comment">% invariant under rescaling of the search direction d. These two line</span>
0011 <span class="comment">% search methods vary mainly in their strategy to pick the initial step</span>
0012 <span class="comment">% size.</span>
0013 <span class="comment">%</span>
0014 <span class="comment">% Below, the step is constructed as alpha*d, and the step size is the norm</span>
0015 <span class="comment">% of that vector, thus: stepsize = alpha*norm_d. The step is executed by</span>
0016 <span class="comment">% retracting the vector alpha*d from the current point x, giving newx.</span>
0017 <span class="comment">%</span>
0018 <span class="comment">% This line-search may create and maintain a structure called lsmem inside</span>
0019 <span class="comment">% storedb.internal. This gives the linesearch the opportunity to remember</span>
0020 <span class="comment">% what happened in the previous calls. This is typically used to make a</span>
0021 <span class="comment">% first guess at the step size, based on previous events.</span>
0022 <span class="comment">%</span>
0023 <span class="comment">% Inputs/Outputs : see help for linesearch</span>
0024 <span class="comment">%</span>
0025 <span class="comment">% See also: steepestdescent conjugategradients linesearch</span>
0026 
0027 <span class="comment">% This file is part of Manopt: www.manopt.org.</span>
0028 <span class="comment">% Original author: Bamdev Mishra, Dec. 30, 2012.</span>
0029 <span class="comment">% Contributors: Nicolas Boumal</span>
0030 <span class="comment">% Change log:</span>
0031 <span class="comment">%</span>
0032 <span class="comment">%   Sept. 13, 2013 (NB) :</span>
0033 <span class="comment">%       The automatic direction reversal feature was removed (it triggered</span>
0034 <span class="comment">%       when df0 &gt; 0). Direction reversal is a decision that needs to be</span>
0035 <span class="comment">%       made by the solver, so it can know about it.</span>
0036 <span class="comment">%</span>
0037 <span class="comment">%    Nov. 7, 2013 (NB) :</span>
0038 <span class="comment">%       The whole function has been recoded to mimick more closely the new</span>
0039 <span class="comment">%       version of linesearch.m. The parameters are available through the</span>
0040 <span class="comment">%       options structure passed to the solver and have the same names and</span>
0041 <span class="comment">%       same meaning as for the base linesearch. The information is logged</span>
0042 <span class="comment">%       more reliably.</span>
0043 <span class="comment">%</span>
0044 <span class="comment">%   April 3, 2015 (NB):</span>
0045 <span class="comment">%       Works with the new StoreDB class system.</span>
0046 <span class="comment">%</span>
0047 <span class="comment">%   April 8, 2015 (NB):</span>
0048 <span class="comment">%       Got rid of lsmem input/output: now maintained in storedb.internal.</span>
0049 
0050 
0051     <span class="comment">% Allow omission of the key, and even of storedb.</span>
0052     <span class="keyword">if</span> ~exist(<span class="string">'key'</span>, <span class="string">'var'</span>)
0053         <span class="keyword">if</span> ~exist(<span class="string">'storedb'</span>, <span class="string">'var'</span>)
0054             storedb = <a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>();
0055         <span class="keyword">end</span>
0056         key = storedb.getNewKey();
0057     <span class="keyword">end</span>
0058 
0059     <span class="comment">% Backtracking default parameters. These can be overwritten in the</span>
0060     <span class="comment">% options structure which is passed to the solver.</span>
0061     default_options.ls_contraction_factor = .5;
0062     default_options.ls_suff_decr = .5;
0063     default_options.ls_max_steps = 10;
0064     default_options.ls_initial_stepsize = 1;
0065     
0066     <span class="keyword">if</span> ~exist(<span class="string">'options'</span>, <span class="string">'var'</span>) || isempty(options)
0067         options = struct();
0068     <span class="keyword">end</span>
0069     options = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts1, opts2)">mergeOptions</a>(default_options, options);
0070     
0071     contraction_factor = options.ls_contraction_factor;
0072     suff_decr = options.ls_suff_decr;
0073     max_ls_steps = options.ls_max_steps;
0074     initial_stepsize = options.ls_initial_stepsize;
0075     
0076     <span class="comment">% Compute the norm of the search direction.</span>
0077     norm_d = problem.M.norm(x, d);
0078     
0079     <span class="comment">% If this is not the first iteration, then lsmem should have been</span>
0080     <span class="comment">% filled with a suggestion for the initial step.</span>
0081     <span class="keyword">if</span> isfield(storedb.internal, <span class="string">'lsmem'</span>)
0082         lsmem = storedb.internal.lsmem;
0083         <span class="keyword">if</span> isfield(lsmem, <span class="string">'init_alpha'</span>)
0084             <span class="comment">% Pick initial step size based on where we were last time,</span>
0085             alpha = lsmem.init_alpha;
0086         <span class="keyword">end</span>
0087     <span class="comment">% Otherwise, fall back to a user supplied suggestion.</span>
0088     <span class="keyword">else</span>
0089         alpha = initial_stepsize / norm_d;
0090     <span class="keyword">end</span>
0091 
0092     <span class="comment">% Make the chosen step and compute the cost there.</span>
0093     newx = problem.M.retr(x, d, alpha);
0094     newkey = storedb.getNewKey();
0095     newf = <a href="../../../manopt/core/getCost.html" class="code" title="function cost = getCost(problem, x, storedb, key)">getCost</a>(problem, newx, storedb, newkey);
0096     cost_evaluations = 1;
0097     
0098     <span class="comment">% Backtrack while the Armijo criterion is not satisfied</span>
0099     <span class="keyword">while</span> newf &gt; f0 + suff_decr*alpha*df0
0100         
0101         <span class="comment">% Reduce the step size,</span>
0102         alpha = contraction_factor * alpha;
0103         
0104         <span class="comment">% and look closer down the line</span>
0105         newx = problem.M.retr(x, d, alpha);
0106         newkey = storedb.getNewKey();
0107         newf = <a href="../../../manopt/core/getCost.html" class="code" title="function cost = getCost(problem, x, storedb, key)">getCost</a>(problem, newx, storedb, newkey);
0108         cost_evaluations = cost_evaluations + 1;
0109         
0110         <span class="comment">% Make sure we don't run out of budget</span>
0111         <span class="keyword">if</span> cost_evaluations &gt;= max_ls_steps
0112             <span class="keyword">break</span>;
0113         <span class="keyword">end</span>
0114         
0115     <span class="keyword">end</span>
0116     
0117     <span class="comment">% If we got here without obtaining a decrease, we reject the step.</span>
0118     <span class="keyword">if</span> newf &gt; f0
0119         alpha = 0;
0120         newx = x;
0121         newkey = key;
0122         newf = f0; <span class="comment">%#ok&lt;NASGU&gt;</span>
0123     <span class="keyword">end</span>
0124     
0125     <span class="comment">% As seen outside this function, stepsize is the size of the vector we</span>
0126     <span class="comment">% retract to make the step from x to newx. Since the step is alpha*d:</span>
0127     stepsize = alpha * norm_d;
0128 
0129     <span class="comment">% Fill lsmem with a suggestion for what the next initial step size</span>
0130     <span class="comment">% trial should be. On average we intend to do only one extra cost</span>
0131     <span class="comment">% evaluation. Notice how the suggestion is not about stepsize but about</span>
0132     <span class="comment">% alpha. This is the reason why this line search is not invariant under</span>
0133     <span class="comment">% rescaling of the search direction d.</span>
0134     <span class="keyword">switch</span> cost_evaluations
0135         <span class="keyword">case</span> 1
0136             <span class="comment">% If things go very well, push your luck.</span>
0137             init_alpha = 2 * alpha;
0138         <span class="keyword">case</span> 2
0139             <span class="comment">% If things go reasonably well, try to keep pace.</span>
0140             init_alpha = alpha;
0141         <span class="keyword">otherwise</span>
0142             <span class="comment">% If we backtracked a lot, the new stepsize is probably quite</span>
0143             <span class="comment">% small: try to recover.</span>
0144             init_alpha = 2 * alpha;
0145     <span class="keyword">end</span>
0146     storedb.internal.lsmem.init_alpha = init_alpha;
0147     
0148     <span class="comment">% Return some statistics also, for possible analysis.</span>
0149     lsstats.costevals = cost_evaluations;
0150     lsstats.stepsize = stepsize;
0151     lsstats.alpha = alpha;
0152     
0153 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Thu 02-Jul-2015 18:56:12 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>