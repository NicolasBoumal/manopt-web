<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of conjugategradient</title>
  <meta name="keywords" content="conjugategradient">
  <meta name="description" content="Conjugate gradient minimization algorithm for Manopt.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../index.html">Home</a> &gt;  <a href="#">manopt</a> &gt; <a href="#">solvers</a> &gt; <a href="index.html">conjugategradient</a> &gt; conjugategradient.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../index.html"><img alt="<" border="0" src="../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for manopt\solvers\conjugategradient&nbsp;<img alt=">" border="0" src="../../../right.png"></a></td></tr></table>-->

<h1>conjugategradient
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>Conjugate gradient minimization algorithm for Manopt.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>function [x, cost, info, options] = conjugategradient(problem, x, options) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Conjugate gradient minimization algorithm for Manopt.

 function [x, cost, info, options] = conjugategradient(problem)
 function [x, cost, info, options] = conjugategradient(problem, x0)
 function [x, cost, info, options] = conjugategradient(problem, x0, options)
 function [x, cost, info, options] = conjugategradient(problem, [], options)

 Apply the conjugate gradient minimization algorithm to the problem
 defined in the problem structure, starting at x0 if it is provided
 (otherwise, at a random point on the manifold). To specify options whilst
 not specifying an initial guess, give x0 as [] (the empty matrix).

 The outputs x and cost are the best reached point on the manifold and its
 cost. The struct-array info contains information about the iterations:
   iter : the iteration number (0 for the initial guess)
   cost : cost value
   time : elapsed time in seconds
   gradnorm : Riemannian norm of the gradient
   stepsize : norm of the last tangent vector retracted
   beta : value of the beta parameter (see options.beta_type)
   linesearch : information logged by options.linesearch
   And possibly additional information logged by options.statsfun.
 For example, type [info.gradnorm] to obtain a vector of the successive
 gradient norms reached.

 The options structure is used to overwrite the default values. All
 options have a default value and are hence optional. To force an option
 value, pass an options structure with a field options.optionname, where
 optionname is one of the following and the default value is indicated
 between parentheses:

   tolgradnorm (1e-6)
       The algorithm terminates if the norm of the gradient drops below this.
   maxiter (1000)
       The algorithm terminates if maxiter iterations have been executed.
   maxtime (Inf)
       The algorithm terminates if maxtime seconds elapsed.
   minstepsize (1e-10)
       The algorithm terminates if the linesearch returns a displacement
       vector (to be retracted) smaller in norm than this value.
   beta_type ('H-S')
       Conjugate gradient beta rule used to construct the new search
       direction, based on a linear combination of the previous search
       direction and the new (preconditioned) gradient. Possible values
       for this parameter are:
           'S-D', 'steep' for beta = 0 (preconditioned steepest descent)
           'F-R' for Fletcher-Reeves's rule
           'P-R' for Polak-Ribiere's modified rule
           'H-S' for Hestenes-Stiefel's modified rule
           'H-Z' for Hager-Zhang's modified rule
           'L-S' for Sato's Liu-Storey rule
       See Hager and Zhang 2006, &quot;A survey of nonlinear conjugate gradient
       methods&quot; for a description of these rules in the Euclidean case and
       for an explanation of how to adapt them to the preconditioned case.
       The adaption to the Riemannian case is straightforward: see in code
       for details. Modified rules take the max between 0 and the computed
       beta value, which provides automatic restart, except for H-Z and L-S 
       which use a different modification. Sato's Liu-Storey rule is 
       described in Sato 2021, &quot;Riemannian conjugate gradient methods: 
       General framework and specific algorithms with convergence analyses&quot;
   orth_value (Inf)
       Following Powell's restart strategy (Math. prog. 1977), restart CG
       (that is, make a -preconditioned- gradient step) if two successive
       -preconditioned- gradients are &quot;too&quot; parallel. See for example
       Hager and Zhang 2006, &quot;A survey of nonlinear conjugate gradient
       methods&quot;, page 12. An infinite value disables this strategy. See in
       code formula for the specific criterion used.
   linesearch (@linesearch_adaptive or @linesearch_hint)
       Function handle to a line search function. The options structure is
       passed to the line search too, so you can pass it parameters. See
       each line search's documentation for info. Another available line
       search in manopt is @linesearch, in /manopt/linesearch/linesearch.m
       If the problem structure includes a line search hint, then the
       default line search used is @linesearch_hint.
   statsfun (none)
       Function handle to a function that will be called after each
       iteration to provide the opportunity to log additional statistics.
       They will be returned in the info struct. See the generic Manopt
       documentation about solvers for further information.
   stopfun (none)
       Function handle to a function that will be called at each iteration
       to provide the opportunity to specify additional stopping criteria.
       See the generic Manopt documentation about solvers for further
       information.
   verbosity (3)
       Integer number used to tune the amount of output the algorithm
       generates during execution (mostly as text in the command window).
       The higher, the more output. 0 means silent.
   storedepth (2)
       Maximum number of different points x of the manifold for which a
       store structure will be kept in memory in the storedb. If the
       caching features of Manopt are not used, this is irrelevant. For
       the CG algorithm, a store depth of 2 should always be sufficient.


 In most of the examples bundled with the toolbox (see link below), the
 solver can be replaced by the present one if need be.

 See also: steepestdescent trustregions manopt/solvers/linesearch manopt/examples</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>	</li><li><a href="../../../manopt/core/applyStatsfun.html" class="code" title="function stats = applyStatsfun(problem, x, storedb, key, options, stats)">applyStatsfun</a>	Apply the statsfun function to a stats structure (for solvers).</li><li><a href="../../../manopt/core/canGetApproxGradient.html" class="code" title="function candoit = canGetApproxGradient(problem)">canGetApproxGradient</a>	Checks whether an approximate gradient can be computed for this problem.</li><li><a href="../../../manopt/core/canGetCost.html" class="code" title="function candoit = canGetCost(problem)">canGetCost</a>	Checks whether the cost function can be computed for a problem structure.</li><li><a href="../../../manopt/core/canGetGradient.html" class="code" title="function candoit = canGetGradient(problem)">canGetGradient</a>	Checks whether the gradient can be computed for a problem structure.</li><li><a href="../../../manopt/core/canGetLinesearch.html" class="code" title="function candoit = canGetLinesearch(problem)">canGetLinesearch</a>	Checks whether the problem structure can give a line-search a hint.</li><li><a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>	Computes the cost function and the gradient at x in one call if possible.</li><li><a href="../../../manopt/core/getGlobalDefaults.html" class="code" title="function opts = getGlobalDefaults()">getGlobalDefaults</a>	Returns a structure with default option values for Manopt.</li><li><a href="../../../manopt/core/getPrecon.html" class="code" title="function Pd = getPrecon(problem, x, d, storedb, key)">getPrecon</a>	Applies the preconditioner for the Hessian of the cost at x along d.</li><li><a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>	Merges two options structures with one having precedence over the other.</li><li><a href="../../../manopt/core/stoppingcriterion.html" class="code" title="function [stop, reason] = stoppingcriterion(problem, x, options, info, last)">stoppingcriterion</a>	Checks for standard stopping criteria, as a helper to solvers.</li><li><a href="../../../manopt/solvers/gradientapproximations/approxgradientFD.html" class="code" title="function gradfun = approxgradientFD(problem, options)">approxgradientFD</a>	Gradient approx. fnctn handle based on finite differences of the cost.</li><li><a href="../../../manopt/solvers/linesearch/linesearch_adaptive.html" class="code" title="function [stepsize, newx, newkey, lsstats] =linesearch_adaptive(problem, x, d, f0, df0, options, storedb, key)">linesearch_adaptive</a>	Adaptive line search algorithm (step size selection) for descent methods.</li><li><a href="../../../manopt/solvers/linesearch/linesearch_hint.html" class="code" title="function [stepsize, newx, newkey, lsstats] =linesearch_hint(problem, x, d, f0, df0, options, storedb, key)">linesearch_hint</a>	Armijo line-search based on the line-search hint in the problem structure.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../examples/doubly_stochastic_denoising.html" class="code" title="function doubly_stochastic_denoising()">doubly_stochastic_denoising</a>	Find a doubly stochastic matrix closest to a given matrix, in Frobenius norm.</li><li><a href="../../../examples/low_rank_matrix_completion.html" class="code" title="function low_rank_matrix_completion()">low_rank_matrix_completion</a>	Given partial observation of a low rank matrix, attempts to complete it.</li><li><a href="../../../examples/low_rank_tensor_completion.html" class="code" title="function low_rank_tensor_completion()">low_rank_tensor_completion</a>	Given partial observation of a low rank tensor, attempts to complete it.</li><li><a href="../../../examples/packing_on_the_sphere.html" class="code" title="function [X, maxdot] = packing_on_the_sphere(d, n, epsilon, X0)">packing_on_the_sphere</a>	Return a set of points spread out on the sphere.</li><li><a href="../../../examples/using_counters.html" class="code" title="function using_counters()">using_counters</a>	Manopt example on how to use counters during optimization. Typical uses,</li><li><a href="../../../manopt/tools/manoptsolve.html" class="code" title="function [x, cost, info, options] = manoptsolve(problem, x0, options)">manoptsolve</a>	Gateway helper function to call a Manopt solver, chosen in the options.</li></ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="#_sub1" class="code">function stats = savestats()</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [x, cost, info, options] = conjugategradient(problem, x, options)</a>
0002 <span class="comment">% Conjugate gradient minimization algorithm for Manopt.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% function [x, cost, info, options] = conjugategradient(problem)</span>
0005 <span class="comment">% function [x, cost, info, options] = conjugategradient(problem, x0)</span>
0006 <span class="comment">% function [x, cost, info, options] = conjugategradient(problem, x0, options)</span>
0007 <span class="comment">% function [x, cost, info, options] = conjugategradient(problem, [], options)</span>
0008 <span class="comment">%</span>
0009 <span class="comment">% Apply the conjugate gradient minimization algorithm to the problem</span>
0010 <span class="comment">% defined in the problem structure, starting at x0 if it is provided</span>
0011 <span class="comment">% (otherwise, at a random point on the manifold). To specify options whilst</span>
0012 <span class="comment">% not specifying an initial guess, give x0 as [] (the empty matrix).</span>
0013 <span class="comment">%</span>
0014 <span class="comment">% The outputs x and cost are the best reached point on the manifold and its</span>
0015 <span class="comment">% cost. The struct-array info contains information about the iterations:</span>
0016 <span class="comment">%   iter : the iteration number (0 for the initial guess)</span>
0017 <span class="comment">%   cost : cost value</span>
0018 <span class="comment">%   time : elapsed time in seconds</span>
0019 <span class="comment">%   gradnorm : Riemannian norm of the gradient</span>
0020 <span class="comment">%   stepsize : norm of the last tangent vector retracted</span>
0021 <span class="comment">%   beta : value of the beta parameter (see options.beta_type)</span>
0022 <span class="comment">%   linesearch : information logged by options.linesearch</span>
0023 <span class="comment">%   And possibly additional information logged by options.statsfun.</span>
0024 <span class="comment">% For example, type [info.gradnorm] to obtain a vector of the successive</span>
0025 <span class="comment">% gradient norms reached.</span>
0026 <span class="comment">%</span>
0027 <span class="comment">% The options structure is used to overwrite the default values. All</span>
0028 <span class="comment">% options have a default value and are hence optional. To force an option</span>
0029 <span class="comment">% value, pass an options structure with a field options.optionname, where</span>
0030 <span class="comment">% optionname is one of the following and the default value is indicated</span>
0031 <span class="comment">% between parentheses:</span>
0032 <span class="comment">%</span>
0033 <span class="comment">%   tolgradnorm (1e-6)</span>
0034 <span class="comment">%       The algorithm terminates if the norm of the gradient drops below this.</span>
0035 <span class="comment">%   maxiter (1000)</span>
0036 <span class="comment">%       The algorithm terminates if maxiter iterations have been executed.</span>
0037 <span class="comment">%   maxtime (Inf)</span>
0038 <span class="comment">%       The algorithm terminates if maxtime seconds elapsed.</span>
0039 <span class="comment">%   minstepsize (1e-10)</span>
0040 <span class="comment">%       The algorithm terminates if the linesearch returns a displacement</span>
0041 <span class="comment">%       vector (to be retracted) smaller in norm than this value.</span>
0042 <span class="comment">%   beta_type ('H-S')</span>
0043 <span class="comment">%       Conjugate gradient beta rule used to construct the new search</span>
0044 <span class="comment">%       direction, based on a linear combination of the previous search</span>
0045 <span class="comment">%       direction and the new (preconditioned) gradient. Possible values</span>
0046 <span class="comment">%       for this parameter are:</span>
0047 <span class="comment">%           'S-D', 'steep' for beta = 0 (preconditioned steepest descent)</span>
0048 <span class="comment">%           'F-R' for Fletcher-Reeves's rule</span>
0049 <span class="comment">%           'P-R' for Polak-Ribiere's modified rule</span>
0050 <span class="comment">%           'H-S' for Hestenes-Stiefel's modified rule</span>
0051 <span class="comment">%           'H-Z' for Hager-Zhang's modified rule</span>
0052 <span class="comment">%           'L-S' for Sato's Liu-Storey rule</span>
0053 <span class="comment">%       See Hager and Zhang 2006, &quot;A survey of nonlinear conjugate gradient</span>
0054 <span class="comment">%       methods&quot; for a description of these rules in the Euclidean case and</span>
0055 <span class="comment">%       for an explanation of how to adapt them to the preconditioned case.</span>
0056 <span class="comment">%       The adaption to the Riemannian case is straightforward: see in code</span>
0057 <span class="comment">%       for details. Modified rules take the max between 0 and the computed</span>
0058 <span class="comment">%       beta value, which provides automatic restart, except for H-Z and L-S</span>
0059 <span class="comment">%       which use a different modification. Sato's Liu-Storey rule is</span>
0060 <span class="comment">%       described in Sato 2021, &quot;Riemannian conjugate gradient methods:</span>
0061 <span class="comment">%       General framework and specific algorithms with convergence analyses&quot;</span>
0062 <span class="comment">%   orth_value (Inf)</span>
0063 <span class="comment">%       Following Powell's restart strategy (Math. prog. 1977), restart CG</span>
0064 <span class="comment">%       (that is, make a -preconditioned- gradient step) if two successive</span>
0065 <span class="comment">%       -preconditioned- gradients are &quot;too&quot; parallel. See for example</span>
0066 <span class="comment">%       Hager and Zhang 2006, &quot;A survey of nonlinear conjugate gradient</span>
0067 <span class="comment">%       methods&quot;, page 12. An infinite value disables this strategy. See in</span>
0068 <span class="comment">%       code formula for the specific criterion used.</span>
0069 <span class="comment">%   linesearch (@linesearch_adaptive or @linesearch_hint)</span>
0070 <span class="comment">%       Function handle to a line search function. The options structure is</span>
0071 <span class="comment">%       passed to the line search too, so you can pass it parameters. See</span>
0072 <span class="comment">%       each line search's documentation for info. Another available line</span>
0073 <span class="comment">%       search in manopt is @linesearch, in /manopt/linesearch/linesearch.m</span>
0074 <span class="comment">%       If the problem structure includes a line search hint, then the</span>
0075 <span class="comment">%       default line search used is @linesearch_hint.</span>
0076 <span class="comment">%   statsfun (none)</span>
0077 <span class="comment">%       Function handle to a function that will be called after each</span>
0078 <span class="comment">%       iteration to provide the opportunity to log additional statistics.</span>
0079 <span class="comment">%       They will be returned in the info struct. See the generic Manopt</span>
0080 <span class="comment">%       documentation about solvers for further information.</span>
0081 <span class="comment">%   stopfun (none)</span>
0082 <span class="comment">%       Function handle to a function that will be called at each iteration</span>
0083 <span class="comment">%       to provide the opportunity to specify additional stopping criteria.</span>
0084 <span class="comment">%       See the generic Manopt documentation about solvers for further</span>
0085 <span class="comment">%       information.</span>
0086 <span class="comment">%   verbosity (3)</span>
0087 <span class="comment">%       Integer number used to tune the amount of output the algorithm</span>
0088 <span class="comment">%       generates during execution (mostly as text in the command window).</span>
0089 <span class="comment">%       The higher, the more output. 0 means silent.</span>
0090 <span class="comment">%   storedepth (2)</span>
0091 <span class="comment">%       Maximum number of different points x of the manifold for which a</span>
0092 <span class="comment">%       store structure will be kept in memory in the storedb. If the</span>
0093 <span class="comment">%       caching features of Manopt are not used, this is irrelevant. For</span>
0094 <span class="comment">%       the CG algorithm, a store depth of 2 should always be sufficient.</span>
0095 <span class="comment">%</span>
0096 <span class="comment">%</span>
0097 <span class="comment">% In most of the examples bundled with the toolbox (see link below), the</span>
0098 <span class="comment">% solver can be replaced by the present one if need be.</span>
0099 <span class="comment">%</span>
0100 <span class="comment">% See also: steepestdescent trustregions manopt/solvers/linesearch manopt/examples</span>
0101 
0102 <span class="comment">% An explicit, general listing of this algorithm, with preconditioning,</span>
0103 <span class="comment">% can be found in the following paper:</span>
0104 <span class="comment">%     @Article{boumal2015lowrank,</span>
0105 <span class="comment">%       Title   = {Low-rank matrix completion via preconditioned optimization on the {G}rassmann manifold},</span>
0106 <span class="comment">%       Author  = {Boumal, N. and Absil, P.-A.},</span>
0107 <span class="comment">%       Journal = {Linear Algebra and its Applications},</span>
0108 <span class="comment">%       Year    = {2015},</span>
0109 <span class="comment">%       Pages   = {200--239},</span>
0110 <span class="comment">%       Volume  = {475},</span>
0111 <span class="comment">%       Doi     = {10.1016/j.laa.2015.02.027},</span>
0112 <span class="comment">%     }</span>
0113 
0114 <span class="comment">% This file is part of Manopt: www.manopt.org.</span>
0115 <span class="comment">% Original author: Bamdev Mishra, Dec. 30, 2012.</span>
0116 <span class="comment">% Contributors: Nicolas Boumal, Nick Vannieuwenhoven</span>
0117 <span class="comment">% Change log:</span>
0118 <span class="comment">%</span>
0119 <span class="comment">%   March 14, 2013, NB:</span>
0120 <span class="comment">%       Added preconditioner support : see Section 8 in</span>
0121 <span class="comment">%       https://www.math.lsu.edu/~hozhang/papers/cgsurvey.pdf</span>
0122 <span class="comment">%</span>
0123 <span class="comment">%   Sept. 13, 2013, NB:</span>
0124 <span class="comment">%       Now logging beta parameter too.</span>
0125 <span class="comment">%</span>
0126 <span class="comment">%   Nov. 7, 2013, NB:</span>
0127 <span class="comment">%       The search direction is no longer normalized before it is passed</span>
0128 <span class="comment">%       to the linesearch. This way, it is up to the designers of the</span>
0129 <span class="comment">%       linesearch to decide whether they want to use the norm of the</span>
0130 <span class="comment">%       search direction in their algorithm or not. There are reasons</span>
0131 <span class="comment">%       against it, but practical evidence that it may help too, so we</span>
0132 <span class="comment">%       allow it. The default linesearch_adaptive used does exploit the</span>
0133 <span class="comment">%       norm information. The base linesearch does not. You may select it</span>
0134 <span class="comment">%       by setting options.linesearch = @linesearch;</span>
0135 <span class="comment">%</span>
0136 <span class="comment">%   Nov. 29, 2013, NB:</span>
0137 <span class="comment">%       Documentation improved: options are now explicitly described.</span>
0138 <span class="comment">%       Removed the Daniel rule for beta: it was not appropriate for</span>
0139 <span class="comment">%       preconditioned CG and I could not find a proper reference for it.</span>
0140 <span class="comment">%</span>
0141 <span class="comment">%   April 3, 2015 (NB):</span>
0142 <span class="comment">%       Works with the new StoreDB class system.</span>
0143 <span class="comment">%</span>
0144 <span class="comment">%   Aug. 2, 2018 (NB):</span>
0145 <span class="comment">%       Now using storedb.remove() to keep the cache lean.</span>
0146 <span class="comment">%</span>
0147 <span class="comment">%   Feb. 7, 2022 (NV):</span>
0148 <span class="comment">%       Added support for Liu-Storey rule (L-S).</span>
0149 
0150 M = problem.M;
0151 
0152 <span class="comment">% Verify that the problem description is sufficient for the solver.</span>
0153 <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetCost.html" class="code" title="function candoit = canGetCost(problem)">canGetCost</a>(problem)
0154     warning(<span class="string">'manopt:getCost'</span>, <span class="keyword">...</span>
0155         <span class="string">'No cost provided. The algorithm will likely abort.'</span>);
0156 <span class="keyword">end</span>
0157 <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetGradient.html" class="code" title="function candoit = canGetGradient(problem)">canGetGradient</a>(problem) &amp;&amp; ~<a href="../../../manopt/core/canGetApproxGradient.html" class="code" title="function candoit = canGetApproxGradient(problem)">canGetApproxGradient</a>(problem)
0158     warning(<span class="string">'manopt:getGradient:approx'</span>, <span class="keyword">...</span>
0159            [<span class="string">'No gradient provided. Using an FD approximation instead (slow).\n'</span> <span class="keyword">...</span>
0160             <span class="string">'It may be necessary to increase options.tolgradnorm.\n'</span> <span class="keyword">...</span>
0161             <span class="string">'To disable this warning: warning(''off'', ''manopt:getGradient:approx'')'</span>]);
0162     problem.approxgrad = <a href="../../../manopt/solvers/gradientapproximations/approxgradientFD.html" class="code" title="function gradfun = approxgradientFD(problem, options)">approxgradientFD</a>(problem);
0163 <span class="keyword">end</span>
0164 
0165 <span class="comment">% Set local defaults here</span>
0166 localdefaults.minstepsize = 1e-10;
0167 localdefaults.maxiter = 1000;
0168 localdefaults.tolgradnorm = 1e-6;
0169 localdefaults.storedepth = 20;
0170 <span class="comment">% Changed by NB : H-S has the &quot;auto restart&quot; property.</span>
0171 <span class="comment">% See Hager-Zhang 2005/2006 survey about CG methods.</span>
0172 <span class="comment">% The auto restart comes from the 'max(0, ...)', not so much from the</span>
0173 <span class="comment">% reason stated in Hager-Zhang I think. P-R also has auto restart.</span>
0174 localdefaults.beta_type = <span class="string">'H-S'</span>;
0175 localdefaults.orth_value = Inf; <span class="comment">% by BM as suggested in Nocedal and Wright</span>
0176 
0177     
0178 <span class="comment">% Depending on whether the problem structure specifies a hint for</span>
0179 <span class="comment">% line-search algorithms, choose a default line-search that works on</span>
0180 <span class="comment">% its own (typical) or that uses the hint.</span>
0181 <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetLinesearch.html" class="code" title="function candoit = canGetLinesearch(problem)">canGetLinesearch</a>(problem)
0182     localdefaults.linesearch = @<a href="../../../manopt/solvers/linesearch/linesearch_adaptive.html" class="code" title="function [stepsize, newx, newkey, lsstats] =linesearch_adaptive(problem, x, d, f0, df0, options, storedb, key)">linesearch_adaptive</a>;
0183 <span class="keyword">else</span>
0184     localdefaults.linesearch = @<a href="../../../manopt/solvers/linesearch/linesearch_hint.html" class="code" title="function [stepsize, newx, newkey, lsstats] =linesearch_hint(problem, x, d, f0, df0, options, storedb, key)">linesearch_hint</a>;
0185 <span class="keyword">end</span>
0186 
0187 <span class="comment">% Merge global and local defaults, then merge w/ user options, if any.</span>
0188 localdefaults = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>(<a href="../../../manopt/core/getGlobalDefaults.html" class="code" title="function opts = getGlobalDefaults()">getGlobalDefaults</a>(), localdefaults);
0189 <span class="keyword">if</span> ~exist(<span class="string">'options'</span>, <span class="string">'var'</span>) || isempty(options)
0190     options = struct();
0191 <span class="keyword">end</span>
0192 options = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>(localdefaults, options);
0193 
0194 timetic = tic();
0195 
0196 <span class="comment">% If no initial point x is given by the user, generate one at random.</span>
0197 <span class="keyword">if</span> ~exist(<span class="string">'x'</span>, <span class="string">'var'</span>) || isempty(x)
0198     x = M.rand();
0199 <span class="keyword">end</span>
0200 
0201 <span class="comment">% Create a store database and generate a key for the current x</span>
0202 storedb = <a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>(options.storedepth);
0203 key = storedb.getNewKey();
0204 
0205 <span class="comment">% Compute cost-related quantities for x</span>
0206 [cost, grad] = <a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>(problem, x, storedb, key);
0207 gradnorm = M.norm(x, grad);
0208 Pgrad = <a href="../../../manopt/core/getPrecon.html" class="code" title="function Pd = getPrecon(problem, x, d, storedb, key)">getPrecon</a>(problem, x, grad, storedb, key);
0209 gradPgrad = M.inner(x, grad, Pgrad);
0210 
0211 <span class="comment">% Iteration counter (at any point, iter is the number of fully executed</span>
0212 <span class="comment">% iterations so far)</span>
0213 iter = 0;
0214 
0215 <span class="comment">% Save stats in a struct array info and preallocate.</span>
0216 stats = <a href="#_sub1" class="code" title="subfunction stats = savestats()">savestats</a>();
0217 info(1) = stats;
0218 info(min(10000, options.maxiter+1)).iter = [];
0219 
0220 
0221 <span class="keyword">if</span> options.verbosity &gt;= 2
0222     fprintf(<span class="string">' iter\t               cost val\t    grad. norm\n'</span>);
0223 <span class="keyword">end</span>
0224 
0225 <span class="comment">% Compute a first descent direction (not normalized)</span>
0226 desc_dir = M.lincomb(x, -1, Pgrad);
0227 
0228 
0229 <span class="comment">% Start iterating until stopping criterion triggers</span>
0230 <span class="keyword">while</span> true
0231     
0232     <span class="comment">% Display iteration information</span>
0233     <span class="keyword">if</span> options.verbosity &gt;= 2
0234         fprintf(<span class="string">'%5d\t%+.16e\t%.8e\n'</span>, iter, cost, gradnorm);
0235     <span class="keyword">end</span>
0236     
0237     <span class="comment">% Start timing this iteration</span>
0238     timetic = tic();
0239     
0240     <span class="comment">% Run standard stopping criterion checks</span>
0241     [stop, reason] = <a href="../../../manopt/core/stoppingcriterion.html" class="code" title="function [stop, reason] = stoppingcriterion(problem, x, options, info, last)">stoppingcriterion</a>(problem, x, options, info, iter+1);
0242     
0243     <span class="comment">% Run specific stopping criterion check</span>
0244     <span class="keyword">if</span> ~stop &amp;&amp; abs(stats.stepsize) &lt; options.minstepsize
0245         stop = true;
0246         reason = sprintf([<span class="string">'Last stepsize smaller than minimum '</span>  <span class="keyword">...</span>
0247                           <span class="string">'allowed; options.minstepsize = %g.'</span>], <span class="keyword">...</span>
0248                           options.minstepsize);
0249     <span class="keyword">end</span>
0250     
0251     <span class="keyword">if</span> stop
0252         <span class="keyword">if</span> options.verbosity &gt;= 1
0253             fprintf([reason <span class="string">'\n'</span>]);
0254         <span class="keyword">end</span>
0255         <span class="keyword">break</span>;
0256     <span class="keyword">end</span>
0257     
0258     
0259     <span class="comment">% The line search algorithms require the directional derivative of the</span>
0260     <span class="comment">% cost at the current point x along the search direction.</span>
0261     df0 = M.inner(x, grad, desc_dir);
0262         
0263     <span class="comment">% If we didn't get a descent direction: restart, i.e., switch to the</span>
0264     <span class="comment">% negative gradient. Equivalent to resetting the CG direction to a</span>
0265     <span class="comment">% steepest descent step, which discards the past information.</span>
0266     <span class="keyword">if</span> df0 &gt;= 0
0267         
0268         <span class="comment">% Or we switch to the negative gradient direction.</span>
0269         <span class="keyword">if</span> options.verbosity &gt;= 3
0270             fprintf([<span class="string">'Conjugate gradient info: got an ascent direction '</span><span class="keyword">...</span>
0271                      <span class="string">'(df0 = %2e), reset to the (preconditioned) '</span><span class="keyword">...</span>
0272                      <span class="string">'steepest descent direction.\n'</span>], df0);
0273         <span class="keyword">end</span>
0274         <span class="comment">% Reset to negative gradient: this discards the CG memory.</span>
0275         desc_dir = M.lincomb(x, -1, Pgrad);
0276         df0 = -gradPgrad;
0277         
0278     <span class="keyword">end</span>
0279     
0280     
0281     <span class="comment">% Execute line search</span>
0282     [stepsize, newx, newkey, lsstats] = options.linesearch( <span class="keyword">...</span>
0283                    problem, x, desc_dir, cost, df0, options, storedb, key);
0284                
0285     
0286     <span class="comment">% Compute the new cost-related quantities for newx</span>
0287     [newcost, newgrad] = <a href="../../../manopt/core/getCostGrad.html" class="code" title="function [cost, grad] = getCostGrad(problem, x, storedb, key)">getCostGrad</a>(problem, newx, storedb, newkey);
0288     newgradnorm = M.norm(newx, newgrad);
0289     Pnewgrad = <a href="../../../manopt/core/getPrecon.html" class="code" title="function Pd = getPrecon(problem, x, d, storedb, key)">getPrecon</a>(problem, newx, newgrad, storedb, newkey);
0290     newgradPnewgrad = M.inner(newx, newgrad, Pnewgrad);
0291     
0292     
0293     <span class="comment">% Apply the CG scheme to compute the next search direction.</span>
0294     <span class="comment">%</span>
0295     <span class="comment">% This paper https://www.math.lsu.edu/~hozhang/papers/cgsurvey.pdf</span>
0296     <span class="comment">% by Hager and Zhang lists many known beta rules. The rules defined</span>
0297     <span class="comment">% here can be found in that paper (or are provided with additional</span>
0298     <span class="comment">% references), adapted to the Riemannian setting.</span>
0299     <span class="comment">%</span>
0300     <span class="keyword">if</span> strcmpi(options.beta_type, <span class="string">'steep'</span>) || <span class="keyword">...</span>
0301        strcmpi(options.beta_type, <span class="string">'S-D'</span>)              <span class="comment">% Gradient Descent</span>
0302         
0303         beta = 0;
0304         desc_dir = M.lincomb(newx, -1, Pnewgrad);
0305         
0306     <span class="keyword">else</span>
0307         
0308         oldgrad = M.transp(x, newx, grad);
0309         orth_grads = M.inner(newx, oldgrad, Pnewgrad) / newgradPnewgrad;
0310         
0311         <span class="comment">% Powell's restart strategy (see page 12 of Hager and Zhang's</span>
0312         <span class="comment">% survey on conjugate gradient methods, for example)</span>
0313         <span class="keyword">if</span> abs(orth_grads) &gt;= options.orth_value
0314             beta = 0;
0315             desc_dir = M.lincomb(x, -1, Pnewgrad);
0316             
0317         <span class="keyword">else</span> <span class="comment">% Compute the CG modification</span>
0318             
0319             desc_dir = M.transp(x, newx, desc_dir);
0320             
0321             <span class="keyword">switch</span> upper(options.beta_type)
0322             
0323                 <span class="keyword">case</span> <span class="string">'F-R'</span>  <span class="comment">% Fletcher-Reeves</span>
0324                     beta = newgradPnewgrad / gradPgrad;
0325                 
0326                 <span class="keyword">case</span> <span class="string">'P-R'</span>  <span class="comment">% Polak-Ribiere+</span>
0327                     <span class="comment">% vector grad(new) - transported grad(current)</span>
0328                     diff = M.lincomb(newx, 1, newgrad, -1, oldgrad);
0329                     ip_diff = M.inner(newx, Pnewgrad, diff);
0330                     beta = ip_diff / gradPgrad;
0331                     beta = max(0, beta);
0332                 
0333                 <span class="keyword">case</span> <span class="string">'H-S'</span>  <span class="comment">% Hestenes-Stiefel+</span>
0334                     diff = M.lincomb(newx, 1, newgrad, -1, oldgrad);
0335                     ip_diff = M.inner(newx, Pnewgrad, diff);
0336                     beta = ip_diff / M.inner(newx, diff, desc_dir);
0337                     beta = max(0, beta);
0338 
0339                 <span class="keyword">case</span> <span class="string">'H-Z'</span> <span class="comment">% Hager-Zhang+</span>
0340                     diff = M.lincomb(newx, 1, newgrad, -1, oldgrad);
0341                     Poldgrad = M.transp(x, newx, Pgrad);
0342                     Pdiff = M.lincomb(newx, 1, Pnewgrad, -1, Poldgrad);
0343                     deno = M.inner(newx, diff, desc_dir);
0344                     numo = M.inner(newx, diff, Pnewgrad);
0345                     numo = numo - 2*M.inner(newx, diff, Pdiff)*<span class="keyword">...</span>
0346                                      M.inner(newx, desc_dir, newgrad) / deno;
0347                     beta = numo / deno;
0348 
0349                     <span class="comment">% Robustness (see Hager-Zhang paper mentioned above)</span>
0350                     desc_dir_norm = M.norm(newx, desc_dir);
0351                     eta_HZ = -1 / ( desc_dir_norm * min(0.01, gradnorm) );
0352                     beta = max(beta, eta_HZ);
0353                 
0354                 <span class="keyword">case</span> <span class="string">'L-S'</span> <span class="comment">% Liu-Storey+ from Sato</span>
0355                     diff = M.lincomb(newx, 1, newgrad, -1, oldgrad);
0356                     ip_diff = M.inner(newx, Pnewgrad, diff);
0357                     denom = -1*M.inner(x, grad, desc_dir);
0358                     betaLS = ip_diff / denom;
0359                     betaCD = newgradPnewgrad / denom;
0360                     beta = max(0, min(betaLS, betaCD));
0361 
0362                 <span class="keyword">otherwise</span>
0363                     error([<span class="string">'Unknown options.beta_type. '</span> <span class="keyword">...</span>
0364                            <span class="string">'Should be steep, S-D, F-R, P-R, H-S, H-Z, or L-S.'</span>]);
0365             <span class="keyword">end</span>
0366             
0367             desc_dir = M.lincomb(newx, -1, Pnewgrad, beta, desc_dir);
0368         
0369         <span class="keyword">end</span>
0370         
0371     <span class="keyword">end</span>
0372     
0373     <span class="comment">% Transfer iterate info.</span>
0374     storedb.removefirstifdifferent(key, newkey);
0375     x = newx;
0376     key = newkey;
0377     cost = newcost;
0378     grad = newgrad;
0379     Pgrad = Pnewgrad;
0380     gradnorm = newgradnorm;
0381     gradPgrad = newgradPnewgrad;
0382     
0383     <span class="comment">% iter is the number of iterations we have accomplished.</span>
0384     iter = iter + 1;
0385     
0386     <span class="comment">% Make sure we don't use too much memory for the store database.</span>
0387     storedb.purge();
0388     
0389     <span class="comment">% Log statistics for freshly executed iteration.</span>
0390     stats = <a href="#_sub1" class="code" title="subfunction stats = savestats()">savestats</a>();
0391     info(iter+1) = stats;
0392     
0393 <span class="keyword">end</span>
0394 
0395 
0396 info = info(1:iter+1);
0397 
0398 <span class="keyword">if</span> options.verbosity &gt;= 1
0399     fprintf(<span class="string">'Total time is %f [s] (excludes statsfun)\n'</span>, info(end).time);
0400 <span class="keyword">end</span>
0401 
0402 
0403 <span class="comment">% Routine in charge of collecting the current iteration stats</span>
0404 <a name="_sub1" href="#_subfunctions" class="code">function stats = savestats()</a>
0405     stats.iter = iter;
0406     stats.cost = cost;
0407     stats.gradnorm = gradnorm;
0408     <span class="keyword">if</span> iter == 0
0409         stats.stepsize = nan;
0410         stats.time = toc(timetic);
0411         stats.linesearch = [];
0412         stats.beta = 0;
0413     <span class="keyword">else</span>
0414         stats.stepsize = stepsize;
0415         stats.time = info(iter).time + toc(timetic);
0416         stats.linesearch = lsstats;
0417         stats.beta = beta;
0418     <span class="keyword">end</span>
0419     stats = <a href="../../../manopt/core/applyStatsfun.html" class="code" title="function stats = applyStatsfun(problem, x, storedb, key, options, stats)">applyStatsfun</a>(problem, x, storedb, key, options, stats);
0420 <span class="keyword">end</span>
0421 
0422 <span class="keyword">end</span>
0423 
0424</pre></div>
<hr><address>Generated on Fri 30-Sep-2022 13:18:25 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>