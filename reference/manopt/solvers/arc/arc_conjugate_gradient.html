<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of arc_conjugate_gradient</title>
  <meta name="keywords" content="arc_conjugate_gradient">
  <meta name="description" content="Subproblem solver for ARC based on a nonlinear conjugate gradient method.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../index.html">Home</a> &gt;  <a href="#">manopt</a> &gt; <a href="#">solvers</a> &gt; <a href="index.html">arc</a> &gt; arc_conjugate_gradient.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../index.html"><img alt="<" border="0" src="../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for manopt\solvers\arc&nbsp;<img alt=">" border="0" src="../../../right.png"></a></td></tr></table>-->

<h1>arc_conjugate_gradient
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>Subproblem solver for ARC based on a nonlinear conjugate gradient method.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>function [eta, Heta, hesscalls, stop_str, stats] = arc_conjugate_gradient(problem, x, grad, gradnorm, sigma, options, storedb, key) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Subproblem solver for ARC based on a nonlinear conjugate gradient method.

 [eta, Heta, hesscalls, stop_str, stats] = 
     arc_conjugate_gradient(problem, x, grad, gradnorm, sigma, options, storedb, key)

 This routine approximately solves the following problem:

   min_{eta in T_x M}  m(eta),  where

       m(eta) = &lt;eta, g&gt; + .5 &lt;eta, H[eta]&gt; + (sigma/3) ||eta||^3

 where eta is a tangent vector at x on the manifold given by problem.M,
 g = grad is a tangent vector at x, H[eta] is the result of applying the
 Hessian of the problem at x along eta, and the inner product and norm
 are those from the Riemannian structure on the tangent space T_x M.

 The solve is approximate in the sense that the returned eta only ought
 to satisfy the following conditions:

   ||gradient of m at eta|| &lt;= theta*||eta||^2   and   m(eta) &lt;= m(0),

 where theta is specified in options.theta (see below for default value.)
 Since the gradient of the model at 0 is g, if it is zero, then eta = 0
 is returned. This is the only scenario where eta = 0 is returned.

 Numerical errors can perturb the described expected behavior.

 Inputs:
     problem: Manopt optimization problem structure
     x: point on the manifold problem.M
     grad: gradient of the cost function of the problem at x
     gradnorm: norm of the gradient, often available to the caller
     sigma: cubic regularization parameter (positive scalar)
     options: structure containing options for the subproblem solver
     storedb, key: caching data for problem at x

 Options specific to this subproblem solver:
   theta (0.25)
     Stopping criterion parameter for subproblem solver: the gradient of
     the model at the returned step should have norm no more than theta
     times the squared norm of the step.
   maxinner (the manifold's dimension)
     Maximum number of iterations of the conjugate gradient algorithm.
   beta_type ('P-R')
     The update rule for calculating beta:
     'F-R' for Fletcher-Reeves, 'P-R' for Polak-Ribiere, and 'H-S' for
     Hestenes-Stiefel.

 Outputs:
     eta: approximate solution to the cubic regularized subproblem at x
     Heta: Hess f(x)[eta] -- this is necessary in the outer loop, and it
           is often naturally available to the subproblem solver at the
           end of execution, so that it may be cheaper to return it here.
     hesscalls: number of Hessian calls during execution
     stop_str: string describing why the subsolver stopped
     stats: a structure specifying some statistics about inner work - 
            we record the model cost value and model gradient norm at each
            inner iteration.</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../manopt/core/getHessian.html" class="code" title="function hess = getHessian(problem, x, d, storedb, key)">getHessian</a>	Computes the Hessian of the cost function at x along d.</li><li><a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>	Merges two options structures with one having precedence over the other.</li><li><a href="solve_along_line.html" class="code" title="function eta = solve_along_line(M, point, x, y, g, Hy, sigma)">solve_along_line</a>	Minimize the function h(eta) = f(x + eta*y) where</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="arc.html" class="code" title="function [x, cost, info, options] = arc(problem, x, options)">arc</a>	Adaptive regularization by cubics (ARC) minimization algorithm for Manopt</li></ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [eta, Heta, hesscalls, stop_str, stats] = arc_conjugate_gradient(problem, x, grad, gradnorm, sigma, options, storedb, key)</a>
0002 <span class="comment">% Subproblem solver for ARC based on a nonlinear conjugate gradient method.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% [eta, Heta, hesscalls, stop_str, stats] =</span>
0005 <span class="comment">%     arc_conjugate_gradient(problem, x, grad, gradnorm, sigma, options, storedb, key)</span>
0006 <span class="comment">%</span>
0007 <span class="comment">% This routine approximately solves the following problem:</span>
0008 <span class="comment">%</span>
0009 <span class="comment">%   min_{eta in T_x M}  m(eta),  where</span>
0010 <span class="comment">%</span>
0011 <span class="comment">%       m(eta) = &lt;eta, g&gt; + .5 &lt;eta, H[eta]&gt; + (sigma/3) ||eta||^3</span>
0012 <span class="comment">%</span>
0013 <span class="comment">% where eta is a tangent vector at x on the manifold given by problem.M,</span>
0014 <span class="comment">% g = grad is a tangent vector at x, H[eta] is the result of applying the</span>
0015 <span class="comment">% Hessian of the problem at x along eta, and the inner product and norm</span>
0016 <span class="comment">% are those from the Riemannian structure on the tangent space T_x M.</span>
0017 <span class="comment">%</span>
0018 <span class="comment">% The solve is approximate in the sense that the returned eta only ought</span>
0019 <span class="comment">% to satisfy the following conditions:</span>
0020 <span class="comment">%</span>
0021 <span class="comment">%   ||gradient of m at eta|| &lt;= theta*||eta||^2   and   m(eta) &lt;= m(0),</span>
0022 <span class="comment">%</span>
0023 <span class="comment">% where theta is specified in options.theta (see below for default value.)</span>
0024 <span class="comment">% Since the gradient of the model at 0 is g, if it is zero, then eta = 0</span>
0025 <span class="comment">% is returned. This is the only scenario where eta = 0 is returned.</span>
0026 <span class="comment">%</span>
0027 <span class="comment">% Numerical errors can perturb the described expected behavior.</span>
0028 <span class="comment">%</span>
0029 <span class="comment">% Inputs:</span>
0030 <span class="comment">%     problem: Manopt optimization problem structure</span>
0031 <span class="comment">%     x: point on the manifold problem.M</span>
0032 <span class="comment">%     grad: gradient of the cost function of the problem at x</span>
0033 <span class="comment">%     gradnorm: norm of the gradient, often available to the caller</span>
0034 <span class="comment">%     sigma: cubic regularization parameter (positive scalar)</span>
0035 <span class="comment">%     options: structure containing options for the subproblem solver</span>
0036 <span class="comment">%     storedb, key: caching data for problem at x</span>
0037 <span class="comment">%</span>
0038 <span class="comment">% Options specific to this subproblem solver:</span>
0039 <span class="comment">%   theta (0.25)</span>
0040 <span class="comment">%     Stopping criterion parameter for subproblem solver: the gradient of</span>
0041 <span class="comment">%     the model at the returned step should have norm no more than theta</span>
0042 <span class="comment">%     times the squared norm of the step.</span>
0043 <span class="comment">%   maxinner (the manifold's dimension)</span>
0044 <span class="comment">%     Maximum number of iterations of the conjugate gradient algorithm.</span>
0045 <span class="comment">%   beta_type ('P-R')</span>
0046 <span class="comment">%     The update rule for calculating beta:</span>
0047 <span class="comment">%     'F-R' for Fletcher-Reeves, 'P-R' for Polak-Ribiere, and 'H-S' for</span>
0048 <span class="comment">%     Hestenes-Stiefel.</span>
0049 <span class="comment">%</span>
0050 <span class="comment">% Outputs:</span>
0051 <span class="comment">%     eta: approximate solution to the cubic regularized subproblem at x</span>
0052 <span class="comment">%     Heta: Hess f(x)[eta] -- this is necessary in the outer loop, and it</span>
0053 <span class="comment">%           is often naturally available to the subproblem solver at the</span>
0054 <span class="comment">%           end of execution, so that it may be cheaper to return it here.</span>
0055 <span class="comment">%     hesscalls: number of Hessian calls during execution</span>
0056 <span class="comment">%     stop_str: string describing why the subsolver stopped</span>
0057 <span class="comment">%     stats: a structure specifying some statistics about inner work -</span>
0058 <span class="comment">%            we record the model cost value and model gradient norm at each</span>
0059 <span class="comment">%            inner iteration.</span>
0060 
0061 <span class="comment">% This file is part of Manopt: www.manopt.org.</span>
0062 <span class="comment">% Original authors: May 2, 2019,</span>
0063 <span class="comment">%    Bryan Zhu, Nicolas Boumal.</span>
0064 <span class="comment">% Contributors:</span>
0065 <span class="comment">% Change log:</span>
0066 <span class="comment">%</span>
0067 <span class="comment">%   Aug. 19, 2019 (NB):</span>
0068 <span class="comment">%       Option maxiter_cg renamed to maxinner to match trustregions.</span>
0069 
0070 <span class="comment">% TODO: Support preconditioning through getPrecon().</span>
0071 
0072     <span class="comment">% Some shortcuts</span>
0073     M = problem.M;
0074     n = M.dim();
0075     inner = @(u, v) M.inner(x, u, v);
0076     rnorm = @(u) M.norm(x, u);
0077     tangent = @(u) problem.M.tangent(x, u);
0078     Hess = @(u) <a href="../../../manopt/core/getHessian.html" class="code" title="function hess = getHessian(problem, x, d, storedb, key)">getHessian</a>(problem, x, u, storedb, key);
0079     
0080     <span class="comment">% Counter for Hessian calls issued</span>
0081     hesscalls = 0;
0082     
0083     <span class="comment">% If the gradient has norm zero, return a zero step</span>
0084     <span class="keyword">if</span> gradnorm == 0
0085         eta = M.zerovec(x);
0086         Heta = eta;
0087         stop_str = <span class="string">'Cost gradient is zero'</span>;
0088         stats = struct(<span class="string">'gradnorms'</span>, 0, <span class="string">'func_values'</span>, 0);
0089         <span class="keyword">return</span>;
0090     <span class="keyword">end</span>
0091     
0092     <span class="comment">% Set local defaults here</span>
0093     localdefaults.theta = 0.25;
0094     localdefaults.maxinner = n;
0095     localdefaults.beta_type = <span class="string">'P-R'</span>;
0096     localdefaults.subproblemstop = <span class="string">'sqrule'</span>;
0097     
0098     <span class="comment">% Merge local defaults with user options, if any</span>
0099     <span class="keyword">if</span> ~exist(<span class="string">'options'</span>, <span class="string">'var'</span>) || isempty(options)
0100         options = struct();
0101     <span class="keyword">end</span>
0102     options = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>(localdefaults, options);
0103     
0104     <span class="comment">% Calculate the Cauchy point as our initial step</span>
0105     hess_grad = Hess(grad);
0106     hesscalls = hesscalls + 1;
0107     temp = inner(grad, hess_grad) / (2 * sigma * gradnorm * gradnorm);
0108     R_c = -temp + sqrt(temp * temp + gradnorm / sigma);
0109     eta = M.lincomb(x, -R_c / gradnorm, grad);
0110     Heta = M.lincomb(x, -R_c / gradnorm, hess_grad);
0111     
0112     <span class="comment">% Initialize variables needed for calculation of conjugate direction</span>
0113     prev_grad = M.lincomb(x, -1, grad);
0114     prev_conj = prev_grad;
0115     Hp_conj = M.lincomb(x, -1, hess_grad);
0116     
0117     <span class="comment">% Main conjugate gradients iteration</span>
0118     maxiter = min(options.maxinner, 3*n);
0119     gradnorms = zeros(maxiter, 1);
0120     func_values = zeros(maxiter, 1);
0121     gradnorm_reached = false;
0122     j = 1;
0123     <span class="keyword">while</span> j &lt; maxiter
0124         <span class="comment">% Calculate the gradient of the model</span>
0125         eta_norm = rnorm(eta);
0126         new_grad = M.lincomb(x, 1, Heta, 1, grad);
0127         new_grad = M.lincomb(x, -1, new_grad, -sigma * eta_norm, eta);
0128         new_grad = tangent(new_grad);
0129         
0130         <span class="comment">% Compute some statistics</span>
0131         gradnorms(j) = rnorm(new_grad);
0132         func_values(j) = inner(grad, eta) + 0.5 * inner(eta, Heta) + (sigma/3) * eta_norm^3;
0133         
0134         <span class="keyword">if</span> options.verbosity &gt;= 4
0135             fprintf(<span class="string">'\nModel grad norm: %.16e, Iterate norm: %.16e'</span>, gradnorms(j), eta_norm);
0136         <span class="keyword">end</span>
0137 
0138         <span class="comment">% Check termination condition</span>
0139         <span class="comment">% TODO -- factor this out, as it is the same for all subsolvers.</span>
0140         <span class="comment">% TODO -- I haven't found a scenario where sqrule doens't win.</span>
0141         <span class="comment">% TODO -- 1e-4 might be too small (g, s, ss seem equivalent).</span>
0142         <span class="keyword">switch</span> lower(options.subproblemstop)
0143             <span class="keyword">case</span> <span class="string">'sqrule'</span>
0144                 stop = (gradnorms(j) &lt;= options.theta * eta_norm^2);
0145             <span class="keyword">case</span> <span class="string">'grule'</span>
0146                 stop = (gradnorms(j) &lt;= min(1e-4, sqrt(gradnorms(1)))*gradnorms(1));
0147             <span class="keyword">case</span> <span class="string">'srule'</span>
0148                 stop = (gradnorms(j) &lt;= min(1e-4, eta_norm)*gradnorms(1));
0149             <span class="keyword">case</span> <span class="string">'ssrule'</span>
0150                 stop = (gradnorms(j) &lt;= min(1e-4, eta_norm/max(1, sigma))*gradnorms(1));
0151             <span class="keyword">otherwise</span>
0152                 error([<span class="string">'Unknown value for options.subproblemstop.\n'</span> <span class="keyword">...</span>
0153                        <span class="string">'Possible values: ''sqrule'', ''grule'', '</span> <span class="keyword">...</span>
0154                        <span class="string">'''srule'', ''ssrule''.\n'</span>]); <span class="comment">% ...</span>
0155                        <span class="comment">% 'Current value: ', options.subproblemstop, '\n']);</span>
0156         <span class="keyword">end</span>
0157         <span class="keyword">if</span> stop
0158             stop_str = <span class="string">'Model grad norm condition satisfied'</span>;
0159             gradnorm_reached = true;
0160             <span class="keyword">break</span>;
0161         <span class="keyword">end</span>
0162         
0163         <span class="comment">% Calculate the conjugate direction using the selected beta rule</span>
0164         delta = M.lincomb(x, 1, new_grad, -1, prev_grad);
0165         <span class="keyword">switch</span> upper(options.beta_type)
0166             <span class="keyword">case</span> <span class="string">'F-R'</span>
0167                 beta = inner(new_grad, new_grad) / inner(prev_grad, prev_grad);
0168             <span class="keyword">case</span> <span class="string">'P-R'</span>
0169                 beta = max(0, inner(new_grad, delta) / inner(prev_grad, prev_grad));
0170             <span class="keyword">case</span> <span class="string">'H-S'</span>
0171                 beta = max(0, -inner(new_grad, delta) / inner(prev_conj, delta));
0172             <span class="keyword">otherwise</span>
0173                 error(<span class="string">'Unknown options.beta_type. Should be F-R, P-R, or H-S.'</span>);
0174         <span class="keyword">end</span>
0175         new_conj = M.lincomb(x, 1, new_grad, beta, prev_conj);
0176         Hn_grad = Hess(new_grad);
0177         hesscalls = hesscalls + 1;
0178         Hn_conj = M.lincomb(x, 1, Hn_grad, beta, Hp_conj);
0179         
0180         <span class="comment">% Find the optimal step in the conjugate direction</span>
0181         alpha = <a href="solve_along_line.html" class="code" title="function eta = solve_along_line(M, point, x, y, g, Hy, sigma)">solve_along_line</a>(M, x, eta, new_conj, grad, Hn_conj, sigma);
0182         <span class="keyword">if</span> alpha == 0
0183             stop_str = <span class="string">'Unable to make further progress in search direction'</span>;
0184             gradnorm_reached = true;
0185             <span class="keyword">break</span>;
0186         <span class="keyword">end</span>
0187         eta = M.lincomb(x, 1, eta, alpha, new_conj);
0188         Heta = M.lincomb(x, 1, Heta, alpha, Hn_conj);
0189         prev_grad = new_grad;
0190         prev_conj = new_conj;
0191         Hp_conj = Hn_conj;
0192         j = j + 1;
0193     <span class="keyword">end</span>
0194     
0195     <span class="comment">% Check why we stopped iterating</span>
0196     <span class="keyword">if</span> ~gradnorm_reached
0197         stop_str = sprintf([<span class="string">'Reached max number of conjugate gradient iterations '</span> <span class="keyword">...</span>
0198                <span class="string">'(options.maxinner = %d)'</span>], options.maxinner);
0199         j = j - 1;
0200     <span class="keyword">end</span>
0201     
0202     <span class="comment">% Return the point we ended on</span>
0203     eta = tangent(eta);
0204     stats = struct(<span class="string">'gradnorms'</span>, gradnorms(1:j), <span class="string">'func_values'</span>, func_values(1:j));
0205     <span class="keyword">if</span> options.verbosity &gt;= 4
0206         fprintf(<span class="string">'\n'</span>);
0207     <span class="keyword">end</span>
0208         
0209 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Tue 19-May-2020 18:46:12 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>