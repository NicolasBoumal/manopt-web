<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of arc_lanczos</title>
  <meta name="keywords" content="arc_lanczos">
  <meta name="description" content="Subproblem solver for ARC based on a Lanczos process.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../index.html">Home</a> &gt;  <a href="#">manopt</a> &gt; <a href="#">solvers</a> &gt; <a href="index.html">arc</a> &gt; arc_lanczos.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../index.html"><img alt="<" border="0" src="../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for manopt\solvers\arc&nbsp;<img alt=">" border="0" src="../../../right.png"></a></td></tr></table>-->

<h1>arc_lanczos
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>Subproblem solver for ARC based on a Lanczos process.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>function [eta, Heta, hesscalls, stop_str, stats] = arc_lanczos(problem, x, grad, gradnorm, sigma, options, storedb, key) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Subproblem solver for ARC based on a Lanczos process.

 [eta, Heta, hesscalls, stop_str, stats] = 
     arc_lanczos(problem, x, grad, gradnorm, sigma, options, storedb, key)

 This routine approximately solves the following problem:

   min_{eta in T_x M}  m(eta),  where

       m(eta) = &lt;eta, g&gt; + .5 &lt;eta, H[eta]&gt; + (sigma/3) ||eta||^3

 where eta is a tangent vector at x on the manifold given by problem.M,
 g = grad is a tangent vector at x, H[eta] is the result of applying the
 Hessian of the problem at x along eta and the inner product and norm
 are those from the Riemannian structure on the tangent space T_x M.

 The solve is approximate in the sense that the returned s only ought to
 satisfy the following conditions:

     ||gradient of m at s|| &lt;= theta*||s||^2   and   m(s) &lt;= m(0),

 where theta is specified in options.theta (see below for default value.)
 Since the gradient of the model at 0 is g, if it is zero, then s = 0 is
 returned. This is the only scenario where s = 0 is returned.

 Numerical errors can perturb the described expected behavior.

 Inputs:
     problem: Manopt optimization problem structure
     x: point on the manifold problem.M
     grad: gradient of the cost function of the problem at x
     gradnorm: norm of the gradient, often available to the caller
     sigma: cubic regularization parameter (positive scalar)
     options: structure containing options for the subproblem solver
     storedb, key: caching data for problem at x

 Options specific to this subproblem solver:
   theta (.5)
     Stopping criterion parameter for subproblem solver: the gradient of
     the model at the returned step should have norm no more than theta
     times the squared norm of the step.
   maxinner (M.dim())
     Maximum number of iterations of the Lanczos process, which is nearly
     the same as the maximum number of calls to the Hessian.
   maxiter_newton (100)
     Maximum number of iterations of the Newton root finder to solve each
     tridiagonal cubic problem.
   tol_newton (1e-16)
     Tolerance for the Newton root finder.

 Outputs:
     eta: approximate solution to the cubic regularized subproblem at x
     Heta: Hess f(x)[eta] -- this is necessary in the outer loop, and it
           is often naturally available to the subproblem solver at the
           end of execution, so that it may be cheaper to return it here.
     hesscalls: number of Hessian calls during execution
     stop_str: string describing why the subsolver stopped
     stats: a structure specifying some statistics about inner work

 See also: <a href="arc.html" class="code" title="function [x, cost, info, options] = arc(problem, x, options)">arc</a> <a href="minimize_cubic_newton.html" class="code" title="function [y, iter, lambda, status] = minimize_cubic_newton(H, g, sigma, options)">minimize_cubic_newton</a></pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../manopt/core/getHessian.html" class="code" title="function hess = getHessian(problem, x, d, storedb, key)">getHessian</a>	Computes the Hessian of the cost function at x along d.</li><li><a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>	Merges two options structures with one having precedence over the other.</li><li><a href="minimize_cubic_newton.html" class="code" title="function [y, iter, lambda, status] = minimize_cubic_newton(H, g, sigma, options)">minimize_cubic_newton</a>	Minimize a cubicly regularized quadratic via Newton root finding.</li><li><a href="../../../manopt/tools/lincomb.html" class="code" title="function vec = lincomb(M, x, vecs, coeffs)">lincomb</a>	Computes a linear combination of tangent vectors in the Manopt framework.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../matlabicon.gif)">
</ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [eta, Heta, hesscalls, stop_str, stats] = arc_lanczos(problem, x, grad, gradnorm, sigma, options, storedb, key)</a>
0002 <span class="comment">% Subproblem solver for ARC based on a Lanczos process.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% [eta, Heta, hesscalls, stop_str, stats] =</span>
0005 <span class="comment">%     arc_lanczos(problem, x, grad, gradnorm, sigma, options, storedb, key)</span>
0006 <span class="comment">%</span>
0007 <span class="comment">% This routine approximately solves the following problem:</span>
0008 <span class="comment">%</span>
0009 <span class="comment">%   min_{eta in T_x M}  m(eta),  where</span>
0010 <span class="comment">%</span>
0011 <span class="comment">%       m(eta) = &lt;eta, g&gt; + .5 &lt;eta, H[eta]&gt; + (sigma/3) ||eta||^3</span>
0012 <span class="comment">%</span>
0013 <span class="comment">% where eta is a tangent vector at x on the manifold given by problem.M,</span>
0014 <span class="comment">% g = grad is a tangent vector at x, H[eta] is the result of applying the</span>
0015 <span class="comment">% Hessian of the problem at x along eta and the inner product and norm</span>
0016 <span class="comment">% are those from the Riemannian structure on the tangent space T_x M.</span>
0017 <span class="comment">%</span>
0018 <span class="comment">% The solve is approximate in the sense that the returned s only ought to</span>
0019 <span class="comment">% satisfy the following conditions:</span>
0020 <span class="comment">%</span>
0021 <span class="comment">%     ||gradient of m at s|| &lt;= theta*||s||^2   and   m(s) &lt;= m(0),</span>
0022 <span class="comment">%</span>
0023 <span class="comment">% where theta is specified in options.theta (see below for default value.)</span>
0024 <span class="comment">% Since the gradient of the model at 0 is g, if it is zero, then s = 0 is</span>
0025 <span class="comment">% returned. This is the only scenario where s = 0 is returned.</span>
0026 <span class="comment">%</span>
0027 <span class="comment">% Numerical errors can perturb the described expected behavior.</span>
0028 <span class="comment">%</span>
0029 <span class="comment">% Inputs:</span>
0030 <span class="comment">%     problem: Manopt optimization problem structure</span>
0031 <span class="comment">%     x: point on the manifold problem.M</span>
0032 <span class="comment">%     grad: gradient of the cost function of the problem at x</span>
0033 <span class="comment">%     gradnorm: norm of the gradient, often available to the caller</span>
0034 <span class="comment">%     sigma: cubic regularization parameter (positive scalar)</span>
0035 <span class="comment">%     options: structure containing options for the subproblem solver</span>
0036 <span class="comment">%     storedb, key: caching data for problem at x</span>
0037 <span class="comment">%</span>
0038 <span class="comment">% Options specific to this subproblem solver:</span>
0039 <span class="comment">%   theta (.5)</span>
0040 <span class="comment">%     Stopping criterion parameter for subproblem solver: the gradient of</span>
0041 <span class="comment">%     the model at the returned step should have norm no more than theta</span>
0042 <span class="comment">%     times the squared norm of the step.</span>
0043 <span class="comment">%   maxinner (M.dim())</span>
0044 <span class="comment">%     Maximum number of iterations of the Lanczos process, which is nearly</span>
0045 <span class="comment">%     the same as the maximum number of calls to the Hessian.</span>
0046 <span class="comment">%   maxiter_newton (100)</span>
0047 <span class="comment">%     Maximum number of iterations of the Newton root finder to solve each</span>
0048 <span class="comment">%     tridiagonal cubic problem.</span>
0049 <span class="comment">%   tol_newton (1e-16)</span>
0050 <span class="comment">%     Tolerance for the Newton root finder.</span>
0051 <span class="comment">%</span>
0052 <span class="comment">% Outputs:</span>
0053 <span class="comment">%     eta: approximate solution to the cubic regularized subproblem at x</span>
0054 <span class="comment">%     Heta: Hess f(x)[eta] -- this is necessary in the outer loop, and it</span>
0055 <span class="comment">%           is often naturally available to the subproblem solver at the</span>
0056 <span class="comment">%           end of execution, so that it may be cheaper to return it here.</span>
0057 <span class="comment">%     hesscalls: number of Hessian calls during execution</span>
0058 <span class="comment">%     stop_str: string describing why the subsolver stopped</span>
0059 <span class="comment">%     stats: a structure specifying some statistics about inner work</span>
0060 <span class="comment">%</span>
0061 <span class="comment">% See also: arc minimize_cubic_newton</span>
0062 
0063 <span class="comment">% This file is part of Manopt: www.manopt.org.</span>
0064 <span class="comment">% Original authors: May 1, 2018,</span>
0065 <span class="comment">%    Naman Agarwal, Brian Bullins, Nicolas Boumal and Coralia Cartis.</span>
0066 <span class="comment">% Contributors:</span>
0067 <span class="comment">% Change log:</span>
0068 <span class="comment">%   Aug 16, 2019 (NB):</span>
0069 <span class="comment">%       Default value for theta changed from 50 to 0.5.</span>
0070 <span class="comment">%       Option maxiter_lanczos renamed to maxinner to match trustregions.</span>
0071 
0072 <span class="comment">% TODO: think whether we can save the Lanczos basis in the storedb at the</span>
0073 <span class="comment">% given key in case we get a rejection, and simply &quot;start where we left</span>
0074 <span class="comment">% off&quot; with the updated sigma.</span>
0075 
0076 <span class="comment">% TODO: Lanczos is notoriously numerically unstable, with loss of</span>
0077 <span class="comment">% orthogonality being a main hurdle. Look into the literature (Paige,</span>
0078 <span class="comment">% Parlett), for possible numerical fixes.</span>
0079 
0080     <span class="comment">% Some shortcuts</span>
0081     M = problem.M;
0082     n = M.dim();
0083     inner = @(u, v) M.inner(x, u, v);
0084     rnorm = @(u) M.norm(x, u);
0085     tangent = @(u) problem.M.tangent(x, u);
0086     Hess = @(u) <a href="../../../manopt/core/getHessian.html" class="code" title="function hess = getHessian(problem, x, d, storedb, key)">getHessian</a>(problem, x, u, storedb, key);
0087     
0088     <span class="comment">% Counter for Hessian calls issued</span>
0089     hesscalls = 0;
0090     
0091     <span class="comment">% If the gradient has norm zero, return a zero step</span>
0092     <span class="keyword">if</span> gradnorm == 0
0093         eta = M.zerovec(x);
0094         Heta = eta;
0095         stop_str = <span class="string">'Cost gradient is zero'</span>;
0096         stats = struct(<span class="string">'newton_iterations'</span>, 0, <span class="string">'gradnorms'</span>, 0, <span class="string">'func_values'</span>, 0);
0097         <span class="keyword">return</span>;
0098     <span class="keyword">end</span>
0099     
0100     <span class="comment">% Set local defaults here</span>
0101     localdefaults.theta = .5;
0102     localdefaults.maxinner = n;
0103     <span class="comment">% The following are here for the Newton solver called below</span>
0104     localdefaults.maxiter_newton = 100;
0105     localdefaults.tol_newton = 1e-16;
0106     
0107     <span class="comment">% Merge local defaults with user options, if any</span>
0108     <span class="keyword">if</span> ~exist(<span class="string">'options'</span>, <span class="string">'var'</span>) || isempty(options)
0109         options = struct();
0110     <span class="keyword">end</span>
0111     options = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>(localdefaults, options);
0112     
0113     <span class="comment">% Vectors where we keep track of the Newton root finder's work, the</span>
0114     <span class="comment">% gradient norm, and the function values at each inner iteration</span>
0115     newton_iterations = zeros(n, 1);
0116     gradnorms = zeros(n, 1);
0117     func_values = zeros(n, 1);
0118     
0119     <span class="comment">% Lanczos iteratively produces an orthonormal basis of tangent vectors</span>
0120     <span class="comment">% which tridiagonalize the Hessian. The corresponding tridiagonal</span>
0121     <span class="comment">% matrix is preallocated here as a sparse matrix.</span>
0122     T = spdiags(ones(n, 3), -1:1, n, n);
0123     
0124     <span class="comment">% The orthonormal basis (n tangent vectors at x) is stored in this cell</span>
0125     Q = cell(n, 1);
0126     
0127     <span class="comment">% Initialize Lanczos along the gradient direction (it is nonzero)</span>
0128     q = M.lincomb(x, 1/gradnorm, grad);
0129     Q{1} = q;
0130     Hq = Hess(q);
0131     hesscalls = hesscalls + 1;
0132     alpha = inner(Hq, q);
0133     T(1, 1) = alpha;
0134     Hq_perp = M.lincomb(x, 1, Hq, -alpha, q);
0135     
0136     <span class="comment">% Minimizing the cubic restricted to the one-dimensional space spanned</span>
0137     <span class="comment">% by Q{1} is easy: it amounts to minimizing a univariate cubic. Indeed,</span>
0138     <span class="comment">% with eta = y*q where y is a scalar, we minimize (since g = ||g||q):</span>
0139     <span class="comment">%  h(y) = &lt;y*q, g&gt; + .5 &lt;y*q, H[y*q]&gt; + (sigma/3) ||y*q||^3</span>
0140     <span class="comment">%       = ||g||*y + .5*alpha*y^2 + (sigma/3) |y|^3.</span>
0141     <span class="comment">% The sign of y affects only the linear term, hence it is clear we need</span>
0142     <span class="comment">% to pick y nonpositive. In that case, h becomes a cubic polynomial:</span>
0143     <span class="comment">%  h(y) = ||g||*y + .5*alpha*y^2 - (sigma/3) y^3</span>
0144     <span class="comment">% The derivative is a quadratic polynomial:</span>
0145     <span class="comment">%  h'(y) = ||g|| + alpha*y - sigma*y^2.</span>
0146     <span class="comment">% Since ||g|| and sigma are positive, h' has two real roots, one</span>
0147     <span class="comment">% positive and one negative (strictly). The negative root is the only</span>
0148     <span class="comment">% root of interest. It necessarily identifies a minimizer since</span>
0149     <span class="comment">% h(0) = 0, h(-inf) = inf and h'(0) &gt; 0.</span>
0150     <span class="comment">%</span>
0151     <span class="comment">% We take the real part only to be safe.</span>
0152     y = min(real(roots([-sigma, alpha, gradnorm])));
0153     
0154     <span class="comment">% Main Lanczos iteration</span>
0155     gradnorm_reached = false;
0156     <span class="keyword">for</span> j = 1 : min(options.maxinner, n) - 1
0157 
0158         <span class="comment">% Knowing that j Lanczos steps have been executed completely, now</span>
0159         <span class="comment">% execute the j+1st step to produce Q{j+1} and populate the</span>
0160         <span class="comment">% tridiagonal matrix T over the whole principal submatrix of size</span>
0161         <span class="comment">% j+1. This involves one Hessian call.</span>
0162         <span class="comment">%</span>
0163         <span class="comment">% In effect, we are computing this one step ahead. The reason is</span>
0164         <span class="comment">% that this makes it cheaper to compute the norm of the gradient of</span>
0165         <span class="comment">% the model, which is needed to check the stopping criterion (see</span>
0166         <span class="comment">% below).</span>
0167         beta = rnorm(Hq_perp);
0168         <span class="comment">% TODO: Figure out a sensible relative threshold</span>
0169         <span class="keyword">if</span> beta &gt; 1e-12
0170             q = M.lincomb(x, 1/beta, Hq_perp);
0171         <span class="keyword">else</span>
0172             <span class="comment">% It appears the Krylov space maxed out (Hq is very nearly in</span>
0173             <span class="comment">% the space spanned by the existing Lanczos vectors). In order</span>
0174             <span class="comment">% to continue, the standard procedure is to generate a random</span>
0175             <span class="comment">% vector, and to orthogonalize it against the current basis.</span>
0176             <span class="comment">% This event is supposed to be rare.</span>
0177             v = M.randvec(x);
0178             <span class="comment">% Orthogonalize in the style of a modified Gram-Schmidt.</span>
0179             <span class="keyword">for</span> k = 1 : j
0180                 v = M.lincomb(x, 1, v, -inner(v, Q{k}), Q{k});
0181             <span class="keyword">end</span>
0182             q = M.lincomb(x, 1/rnorm(v), v);
0183         <span class="keyword">end</span>
0184         
0185         q = tangent(q);
0186         
0187         Hq = Hess(q);
0188         hesscalls = hesscalls + 1;
0189         Hqm = M.lincomb(x, 1, Hq, -beta, Q{j});
0190         <span class="comment">% In exact arithmetic, alpha = &lt;Hq, q&gt;. Doing the computations in</span>
0191         <span class="comment">% this order amounts to a modified GS, which may help numerically.</span>
0192         alpha = inner(Hqm, q);
0193         Hq_perp = M.lincomb(x, 1, Hqm, -alpha, q);
0194         Q{j+1} = q;
0195         T(j, j+1) = beta;     <span class="comment">%#ok&lt;SPRIX&gt;</span>
0196         T(j+1, j) = beta;     <span class="comment">%#ok&lt;SPRIX&gt;</span>
0197         T(j+1, j+1) = alpha;  <span class="comment">%#ok&lt;SPRIX&gt;</span>
0198         <span class="comment">% End of the Lanczos procedure for step j.</span>
0199 
0200         <span class="comment">% Computing the norm of the gradient of the model at the computed</span>
0201         <span class="comment">% step 'Qy' (linear combination of the Q's with coefficients y.)</span>
0202         <span class="comment">% We actually compute the norm of a vector of coordinates for the</span>
0203         <span class="comment">% gradient of the model in the basis Q{1}, ..., Q{j+1}.</span>
0204         model_gradnorm = norm(gradnorm*eye(j+1, 1) + <span class="keyword">...</span>
0205                               T(1:j+1, 1:j)*y + <span class="keyword">...</span>
0206                               sigma*norm(y)*[y ; 0]);
0207         gradnorms(j) = model_gradnorm;
0208         func_values(j) = y(1)*gradnorm + 0.5 * dot(y, T(1:j, 1:j)*y) + (sigma/3) * norm(y)^3;
0209 
0210         <span class="keyword">if</span> options.verbosity &gt;= 4
0211             fprintf(<span class="string">'\nModel grad norm: %.16e, Iterate norm: %.16e'</span>, model_gradnorm, norm(y));
0212         <span class="keyword">end</span>
0213         
0214         <span class="comment">% Check termination condition</span>
0215         <span class="keyword">if</span> model_gradnorm &lt;= options.theta*norm(y)^2
0216             stop_str = <span class="string">'Model grad norm condition satisfied'</span>;
0217             gradnorm_reached = true;
0218             <span class="keyword">break</span>;
0219         <span class="keyword">end</span>
0220         
0221         <span class="comment">% Minimize the cubic model restricted to the subspace spanned by</span>
0222         <span class="comment">% the available Lanczos vectors. In its current form, this solver</span>
0223         <span class="comment">% cannot reuse prior work from earlier Lanczos iterations: this may</span>
0224         <span class="comment">% be a point to improve.</span>
0225         [y, newton_iter] = <a href="minimize_cubic_newton.html" class="code" title="function [y, iter, lambda, status] = minimize_cubic_newton(H, g, sigma, options)">minimize_cubic_newton</a>(T(1:j+1, 1:j+1), <span class="keyword">...</span>
0226                                      gradnorm*eye(j+1, 1), sigma, options);
0227         newton_iterations(j+1) = newton_iter;
0228     <span class="keyword">end</span>
0229     
0230     <span class="comment">% Check why we stopped iterating</span>
0231     points = numel(y);
0232     <span class="keyword">if</span> ~gradnorm_reached
0233         stop_str = sprintf([<span class="string">'Reached max number of Lanczos iterations '</span> <span class="keyword">...</span>
0234                <span class="string">'(options.maxinner = %d)'</span>], options.maxinner);
0235         points = points - 1;
0236     <span class="keyword">end</span>
0237     
0238     <span class="comment">% Construct the tangent vector eta as a linear combination of the basis</span>
0239     <span class="comment">% vectors and make sure the result is tangent up to numerical accuracy.</span>
0240     eta = <a href="../../../manopt/tools/lincomb.html" class="code" title="function vec = lincomb(M, x, vecs, coeffs)">lincomb</a>(M, x, Q(1:numel(y)), y);
0241     eta = tangent(eta);
0242     <span class="comment">% We could easily return the norm of eta as the norm of the coefficient</span>
0243     <span class="comment">% vector y here, but numerical errors might accumulate.</span>
0244     
0245     <span class="comment">% In principle we could avoid this call by computing an appropriate</span>
0246     <span class="comment">% linear combination of available vectors. For now at least, we favor</span>
0247     <span class="comment">% this numerically safer approach.</span>
0248     Heta = Hess(eta);
0249     hesscalls = hesscalls + 1;
0250     
0251     stats = struct(<span class="string">'newton_iterations'</span>, newton_iterations(1:points), <span class="keyword">...</span>
0252                    <span class="string">'gradnorms'</span>, gradnorms(1:points), <span class="keyword">...</span>
0253                    <span class="string">'func_values'</span>, func_values(1:points));
0254     
0255     <span class="keyword">if</span> options.verbosity &gt;= 4
0256         fprintf(<span class="string">'\n'</span>);
0257     <span class="keyword">end</span>
0258         
0259 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Tue 19-May-2020 18:46:12 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>