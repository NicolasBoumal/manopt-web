<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of stochasticgradient</title>
  <meta name="keywords" content="stochasticgradient">
  <meta name="description" content="Stochastic gradient (SG) minimization algorithm for Manopt.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../index.html">Home</a> &gt;  <a href="#">manopt</a> &gt; <a href="#">solvers</a> &gt; <a href="index.html">stochasticgradient</a> &gt; stochasticgradient.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../index.html"><img alt="<" border="0" src="../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for manopt\solvers\stochasticgradient&nbsp;<img alt=">" border="0" src="../../../right.png"></a></td></tr></table>-->

<h1>stochasticgradient
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>Stochastic gradient (SG) minimization algorithm for Manopt.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>function [x, info, options] = stochasticgradient(problem, x, options) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Stochastic gradient (SG) minimization algorithm for Manopt.

 function [x, info, options] = stochasticgradient(problem)
 function [x, info, options] = stochasticgradient(problem, x0)
 function [x, info, options] = stochasticgradient(problem, x0, options)
 function [x, info, options] = stochasticgradient(problem, [], options)

 Apply the Riemannian stochastic gradient algorithm to the problem defined
 in the problem structure, starting at x0 if it is provided (otherwise, at
 a random point on the manifold). To specify options whilst not specifying
 an initial guess, give x0 as [] (the empty matrix).

 The problem structure must contain the following fields:

  problem.M:
       Defines the manifold to optimize over, given by a factory.

  problem.partialgrad or problem.partialegrad (or equivalent)
       Describes the partial gradients of the cost function. If the cost
       function is of the form f(x) = sum_{k=1}^N f_k(x),
       then partialegrad(x, K) = sum_{k \in K} grad f_k(x).
       As usual, partialgrad must define the Riemannian gradient, whereas
       partialegrad defines a Euclidean (classical) gradient which will be
       converted automatically to a Riemannian gradient. Use the tool
       checkgradient(problem) to check it. K is a /row/ vector, which
       makes it natural to write for k = K, ..., end.

  problem.ncostterms
       An integer specifying how many terms are in the cost function (in
       the example above, that would be N.)

 Importantly, the cost function itself needs not be specified.

 Some of the options of the solver are specific to this file. Please have
 a look inside the code.

 To record the value of the cost function or the norm of the gradient for
 example (which are statistics the algorithm does not require and hence
 does not compute by default), one can set the following options:

   metrics.cost = @(problem, x) getCost(problem, x);
   metrics.gradnorm = @(problem, x) problem.M.norm(x, getGradient(problem, x));
   options.statsfun = statsfunhelper(metrics);

 Important caveat: stochastic algorithms usually return an average of the
 last few iterates. Computing averages on manifolds can be expensive.
 Currently, this solver does not compute averages and simply returns the
 last iterate. Using options.statsfun, it is possible for the user to
 compute averages manually. If you have ideas on how to do this
 generically, we welcome feedback. In particular, approximate means could
 be computed with M.pairmean which is available in many geometries.

 See also: steepestdescent</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>	</li><li><a href="../../../manopt/core/applyStatsfun.html" class="code" title="function stats = applyStatsfun(problem, x, storedb, key, options, stats)">applyStatsfun</a>	Apply the statsfun function to a stats structure (for solvers).</li><li><a href="../../../manopt/core/canGetPartialGradient.html" class="code" title="function candoit = canGetPartialGradient(problem)">canGetPartialGradient</a>	Checks whether the partial gradient can be computed for a given problem.</li><li><a href="../../../manopt/core/getGlobalDefaults.html" class="code" title="function opts = getGlobalDefaults()">getGlobalDefaults</a>	Returns a structure with default option values for Manopt.</li><li><a href="../../../manopt/core/getPartialGradient.html" class="code" title="function grad = getPartialGradient(problem, x, I, storedb, key)">getPartialGradient</a>	Computes the gradient of a subset of terms in the cost function at x.</li><li><a href="../../../manopt/core/getPrecon.html" class="code" title="function Pd = getPrecon(problem, x, d, storedb, key)">getPrecon</a>	Applies the preconditioner for the Hessian of the cost at x along d.</li><li><a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>	Merges two options structures with one having precedence over the other.</li><li><a href="../../../manopt/core/stoppingcriterion.html" class="code" title="function [stop, reason] = stoppingcriterion(problem, x, options, info, last)">stoppingcriterion</a>	Checks for standard stopping criteria, as a helper to solvers.</li><li><a href="stepsize_sg.html" class="code" title="function [stepsize, newx, newkey, ssstats] =stepsize_sg(problem, x, d, iter, options, storedb, key) %#ok<INUSD>">stepsize_sg</a>	Standard step size selection algorithm for the stochastic gradient method</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../examples/PCA_stochastic.html" class="code" title="function [X, A] = PCA_stochastic(A, k)">PCA_stochastic</a>	Example of stochastic gradient algorithm in Manopt on a PCA problem.</li></ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="#_sub1" class="code">function stats = savestats()</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [x, info, options] = stochasticgradient(problem, x, options)</a>
0002 <span class="comment">% Stochastic gradient (SG) minimization algorithm for Manopt.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% function [x, info, options] = stochasticgradient(problem)</span>
0005 <span class="comment">% function [x, info, options] = stochasticgradient(problem, x0)</span>
0006 <span class="comment">% function [x, info, options] = stochasticgradient(problem, x0, options)</span>
0007 <span class="comment">% function [x, info, options] = stochasticgradient(problem, [], options)</span>
0008 <span class="comment">%</span>
0009 <span class="comment">% Apply the Riemannian stochastic gradient algorithm to the problem defined</span>
0010 <span class="comment">% in the problem structure, starting at x0 if it is provided (otherwise, at</span>
0011 <span class="comment">% a random point on the manifold). To specify options whilst not specifying</span>
0012 <span class="comment">% an initial guess, give x0 as [] (the empty matrix).</span>
0013 <span class="comment">%</span>
0014 <span class="comment">% The problem structure must contain the following fields:</span>
0015 <span class="comment">%</span>
0016 <span class="comment">%  problem.M:</span>
0017 <span class="comment">%       Defines the manifold to optimize over, given by a factory.</span>
0018 <span class="comment">%</span>
0019 <span class="comment">%  problem.partialgrad or problem.partialegrad (or equivalent)</span>
0020 <span class="comment">%       Describes the partial gradients of the cost function. If the cost</span>
0021 <span class="comment">%       function is of the form f(x) = sum_{k=1}^N f_k(x),</span>
0022 <span class="comment">%       then partialegrad(x, K) = sum_{k \in K} grad f_k(x).</span>
0023 <span class="comment">%       As usual, partialgrad must define the Riemannian gradient, whereas</span>
0024 <span class="comment">%       partialegrad defines a Euclidean (classical) gradient which will be</span>
0025 <span class="comment">%       converted automatically to a Riemannian gradient. Use the tool</span>
0026 <span class="comment">%       checkgradient(problem) to check it. K is a /row/ vector, which</span>
0027 <span class="comment">%       makes it natural to write for k = K, ..., end.</span>
0028 <span class="comment">%</span>
0029 <span class="comment">%  problem.ncostterms</span>
0030 <span class="comment">%       An integer specifying how many terms are in the cost function (in</span>
0031 <span class="comment">%       the example above, that would be N.)</span>
0032 <span class="comment">%</span>
0033 <span class="comment">% Importantly, the cost function itself needs not be specified.</span>
0034 <span class="comment">%</span>
0035 <span class="comment">% Some of the options of the solver are specific to this file. Please have</span>
0036 <span class="comment">% a look inside the code.</span>
0037 <span class="comment">%</span>
0038 <span class="comment">% To record the value of the cost function or the norm of the gradient for</span>
0039 <span class="comment">% example (which are statistics the algorithm does not require and hence</span>
0040 <span class="comment">% does not compute by default), one can set the following options:</span>
0041 <span class="comment">%</span>
0042 <span class="comment">%   metrics.cost = @(problem, x) getCost(problem, x);</span>
0043 <span class="comment">%   metrics.gradnorm = @(problem, x) problem.M.norm(x, getGradient(problem, x));</span>
0044 <span class="comment">%   options.statsfun = statsfunhelper(metrics);</span>
0045 <span class="comment">%</span>
0046 <span class="comment">% Important caveat: stochastic algorithms usually return an average of the</span>
0047 <span class="comment">% last few iterates. Computing averages on manifolds can be expensive.</span>
0048 <span class="comment">% Currently, this solver does not compute averages and simply returns the</span>
0049 <span class="comment">% last iterate. Using options.statsfun, it is possible for the user to</span>
0050 <span class="comment">% compute averages manually. If you have ideas on how to do this</span>
0051 <span class="comment">% generically, we welcome feedback. In particular, approximate means could</span>
0052 <span class="comment">% be computed with M.pairmean which is available in many geometries.</span>
0053 <span class="comment">%</span>
0054 <span class="comment">% See also: steepestdescent</span>
0055 
0056 <span class="comment">% This file is part of Manopt: www.manopt.org.</span>
0057 <span class="comment">% Original authors: Bamdev Mishra &lt;bamdevm@gmail.com&gt;,</span>
0058 <span class="comment">%                   Hiroyuki Kasai &lt;kasai@is.uec.ac.jp&gt;, and</span>
0059 <span class="comment">%                   Hiroyuki Sato &lt;hsato@ms.kagu.tus.ac.jp&gt;, 22 April 2016.</span>
0060 <span class="comment">% Contributors: Nicolas Boumal</span>
0061 <span class="comment">% Change log:</span>
0062 <span class="comment">%</span>
0063 <span class="comment">%   06 July 2019 (BM):</span>
0064 <span class="comment">%      Added preconditioner support. This allows to use adaptive algorithms.</span>
0065     
0066 
0067     <span class="comment">% Verify that the problem description is sufficient for the solver.</span>
0068     <span class="keyword">if</span> ~<a href="../../../manopt/core/canGetPartialGradient.html" class="code" title="function candoit = canGetPartialGradient(problem)">canGetPartialGradient</a>(problem)
0069         warning(<span class="string">'manopt:getPartialGradient'</span>, <span class="keyword">...</span>
0070          <span class="string">'No partial gradient provided. The algorithm will likely abort.'</span>);
0071     <span class="keyword">end</span>
0072     
0073    
0074     <span class="comment">% Set local default</span>
0075     localdefaults.maxiter = 1000;       <span class="comment">% Maximum number of iterations</span>
0076     localdefaults.batchsize = 1;        <span class="comment">% Batchsize (# cost terms per iter)</span>
0077     localdefaults.verbosity = 2;        <span class="comment">% Output verbosity (0, 1 or 2)</span>
0078     localdefaults.storedepth = 20;      <span class="comment">% Limit amount of caching</span>
0079     
0080     <span class="comment">% Check stopping criteria and save stats every checkperiod iterations.</span>
0081     localdefaults.checkperiod = 100;
0082     
0083     <span class="comment">% stepsizefun is a function implementing a step size selection</span>
0084     <span class="comment">% algorithm. See that function for help with options, which can be</span>
0085     <span class="comment">% specified in the options structure passed to the solver directly.</span>
0086     localdefaults.stepsizefun = @<a href="stepsize_sg.html" class="code" title="function [stepsize, newx, newkey, ssstats] =stepsize_sg(problem, x, d, iter, options, storedb, key) %#ok<INUSD>">stepsize_sg</a>;
0087     
0088     <span class="comment">% Merge global and local defaults, then merge w/ user options, if any.</span>
0089     localdefaults = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>(<a href="../../../manopt/core/getGlobalDefaults.html" class="code" title="function opts = getGlobalDefaults()">getGlobalDefaults</a>(), localdefaults);
0090     <span class="keyword">if</span> ~exist(<span class="string">'options'</span>, <span class="string">'var'</span>) || isempty(options)
0091         options = struct();
0092     <span class="keyword">end</span>
0093     options = <a href="../../../manopt/core/mergeOptions.html" class="code" title="function opts = mergeOptions(opts_sub, opts_master)">mergeOptions</a>(localdefaults, options);
0094     
0095     
0096     assert(options.checkperiod &gt;= 1, <span class="keyword">...</span>
0097                  <span class="string">'options.checkperiod must be a positive integer (&gt;= 1).'</span>);
0098     
0099     
0100     <span class="comment">% If no initial point x is given by the user, generate one at random.</span>
0101     <span class="keyword">if</span> ~exist(<span class="string">'x'</span>, <span class="string">'var'</span>) || isempty(x)
0102         x = problem.M.rand();
0103     <span class="keyword">end</span>
0104     
0105     <span class="comment">% Create a store database and get a key for the current x</span>
0106     storedb = <a href="../../../manopt/core/StoreDB.html" class="code" title="">StoreDB</a>(options.storedepth);
0107     key = storedb.getNewKey();
0108     
0109     
0110     <span class="comment">% Elapsed time for the current set of iterations, where a set of</span>
0111     <span class="comment">% iterations comprises options.checkperiod iterations. We do not</span>
0112     <span class="comment">% count time spent for such things as logging statistics, as these are</span>
0113     <span class="comment">% not relevant to the actual optimization process.</span>
0114     elapsed_time = 0;
0115     
0116     <span class="comment">% Total number of completed steps</span>
0117     iter = 0;
0118     
0119     
0120     <span class="comment">% Total number of saved stats at this point.</span>
0121     savedstats = 0;
0122     
0123     <span class="comment">% Collect and save stats in a struct array info, and preallocate.</span>
0124     stats = <a href="#_sub1" class="code" title="subfunction stats = savestats()">savestats</a>();
0125     info(1) = stats;
0126     savedstats = savedstats + 1;
0127     <span class="keyword">if</span> isinf(options.maxiter)
0128         <span class="comment">% We trust that if the user set maxiter = inf, then they defined</span>
0129         <span class="comment">% another stopping criterion.</span>
0130         preallocate = 1e5;
0131     <span class="keyword">else</span>
0132         preallocate = ceil(options.maxiter / options.checkperiod) + 1;
0133     <span class="keyword">end</span>
0134     info(preallocate).iter = [];
0135     
0136     
0137     <span class="comment">% Display information header for the user.</span>
0138     <span class="keyword">if</span> options.verbosity &gt;= 2
0139         fprintf(<span class="string">'    iter       time [s]       step size\n'</span>);
0140     <span class="keyword">end</span>
0141     
0142     
0143     <span class="comment">% Main loop.</span>
0144     stop = false;
0145     <span class="keyword">while</span> iter &lt; options.maxiter
0146         
0147         <span class="comment">% Record start time.</span>
0148         start_time = tic();
0149         
0150         <span class="comment">% Draw the samples with replacement.</span>
0151         idx_batch = randi(problem.ncostterms, options.batchsize, 1);
0152         
0153         <span class="comment">% Compute partial gradient on this batch.</span>
0154         pgrad = <a href="../../../manopt/core/getPartialGradient.html" class="code" title="function grad = getPartialGradient(problem, x, I, storedb, key)">getPartialGradient</a>(problem, x, idx_batch, storedb, key);
0155 
0156         <span class="comment">% Apply preconditioner to the partial gradient.</span>
0157         Ppgrad = <a href="../../../manopt/core/getPrecon.html" class="code" title="function Pd = getPrecon(problem, x, d, storedb, key)">getPrecon</a>(problem, x, pgrad, storedb, key);
0158         
0159         <span class="comment">% Compute a step size and the corresponding new point x.</span>
0160         [stepsize, newx, newkey, ssstats] = <span class="keyword">...</span>
0161                            options.stepsizefun(problem, x, Ppgrad, iter, <span class="keyword">...</span>
0162                                                options, storedb, key);
0163         
0164         <span class="comment">% Make the step: transfer iterate, remove cache from previous x.</span>
0165         storedb.removefirstifdifferent(key, newkey);
0166         x = newx;
0167         key = newkey;
0168         
0169         <span class="comment">% Make sure we do not use too much memory for the store database.</span>
0170         storedb.purge();
0171         
0172         <span class="comment">% Total number of completed steps.</span>
0173         iter = iter + 1;
0174         
0175         <span class="comment">% Elapsed time doing actual optimization work so far in this</span>
0176         <span class="comment">% set of options.checkperiod iterations.</span>
0177         elapsed_time = elapsed_time + toc(start_time);
0178         
0179         
0180         <span class="comment">% Check stopping criteria and save stats every checkperiod iters.</span>
0181         <span class="keyword">if</span> mod(iter, options.checkperiod) == 0
0182             
0183             <span class="comment">% Log statistics for freshly executed iteration.</span>
0184             stats = <a href="#_sub1" class="code" title="subfunction stats = savestats()">savestats</a>();
0185             info(savedstats+1) = stats;
0186             savedstats = savedstats + 1;
0187             
0188             <span class="comment">% Reset timer.</span>
0189             elapsed_time = 0;
0190             
0191             <span class="comment">% Print output.</span>
0192             <span class="keyword">if</span> options.verbosity &gt;= 2
0193                 fprintf(<span class="string">'%8d     %10.2f       %.3e\n'</span>, <span class="keyword">...</span>
0194                                                iter, stats.time, stepsize);
0195             <span class="keyword">end</span>
0196             
0197             <span class="comment">% Run standard stopping criterion checks.</span>
0198             [stop, reason] = <a href="../../../manopt/core/stoppingcriterion.html" class="code" title="function [stop, reason] = stoppingcriterion(problem, x, options, info, last)">stoppingcriterion</a>(problem, x, <span class="keyword">...</span>
0199                                                options, info, savedstats);
0200             <span class="keyword">if</span> stop
0201                 <span class="keyword">if</span> options.verbosity &gt;= 1
0202                     fprintf([reason <span class="string">'\n'</span>]);
0203                 <span class="keyword">end</span>
0204                 <span class="keyword">break</span>;
0205             <span class="keyword">end</span>
0206         
0207         <span class="keyword">end</span>
0208 
0209     <span class="keyword">end</span>
0210     
0211     
0212     <span class="comment">% Keep only the relevant portion of the info struct-array.</span>
0213     info = info(1:savedstats);
0214     
0215     
0216     <span class="comment">% Display a final information message.</span>
0217     <span class="keyword">if</span> options.verbosity &gt;= 1
0218         <span class="keyword">if</span> ~stop
0219             <span class="comment">% We stopped not because of stoppingcriterion but because the</span>
0220             <span class="comment">% loop came to an end, which means maxiter triggered.</span>
0221             msg = <span class="string">'Max iteration count reached; options.maxiter = %g.\n'</span>;
0222             fprintf(msg, options.maxiter);
0223         <span class="keyword">end</span>
0224         fprintf(<span class="string">'Total time is %f [s] (excludes statsfun)\n'</span>, <span class="keyword">...</span>
0225                 info(end).time + elapsed_time);
0226     <span class="keyword">end</span>
0227     
0228     
0229     <span class="comment">% Helper function to collect statistics to be saved at</span>
0230     <span class="comment">% index checkperiodcount+1 in info.</span>
0231     <a name="_sub1" href="#_subfunctions" class="code">function stats = savestats()</a>
0232         stats.iter = iter;
0233         <span class="keyword">if</span> savedstats == 0
0234             stats.time = 0;
0235             stats.stepsize = NaN;
0236             stats.stepsize_stats = [];
0237         <span class="keyword">else</span>
0238             stats.time = info(savedstats).time + elapsed_time;
0239             stats.stepsize = stepsize;
0240             stats.stepsize_stats = ssstats;
0241         <span class="keyword">end</span>
0242         stats = <a href="../../../manopt/core/applyStatsfun.html" class="code" title="function stats = applyStatsfun(problem, x, storedb, key, options, stats)">applyStatsfun</a>(problem, x, storedb, key, options, stats);
0243     <span class="keyword">end</span>
0244     
0245 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Sun 05-Sep-2021 17:57:00 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>