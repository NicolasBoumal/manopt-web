<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of shapefit_smoothed</title>
  <meta name="keywords" content="shapefit_smoothed">
  <meta name="description" content="ShapeFit formulation for sensor network localization from pair directions">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../index.html">Home</a> &gt;  <a href="index.html">examples</a> &gt; shapefit_smoothed.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../index.html"><img alt="<" border="0" src="../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for examples&nbsp;<img alt=">" border="0" src="../right.png"></a></td></tr></table>-->

<h1>shapefit_smoothed
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>ShapeFit formulation for sensor network localization from pair directions</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>function [T_hub, T_lsq, T_cvx] = shapefit_smoothed(V, J) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre class="comment"> ShapeFit formulation for sensor network localization from pair directions

 function [T_hub, T_lsq, T_cvx] = shapefit_smoothed(V, J)

 This example in based on the paper http://arxiv.org/abs/1506.01437:
 ShapeFit: Exact location recovery from corrupted pairwise directions, 2015
 by Paul Hand, Choongbum Lee and Vladislav Voroninski.

 The problem is the following: there are n points t_1, ..., t_n in R^d
 which need to be estimated (localized). To this end, we are given
 measurements of some of the pairwise directions,
 v_ij = (t_i - t_j) / norm(t_i - t_j) + noise.
 Assume there are m such pairwise measurements, defining a graph with m
 edges over n nodes. J is the signed incidence matrix of this graph (see
 in code). To build J from lists I, J in R^m of nodes, use:
 J = sparse([I ; J], [(1:m)' ; (1:m)'], [ones(m, 1), -ones(m, 1)], n, m, 2*m);

 The measurements are arranged in the matrix V of size d x m. From V, we
 attempt to estimate t_1, ..., t_n, arranged in T, a matrix of size d x n.
 The estimation can only be done up to translation and scaling. The
 returned T's are centered: the columns sum to zero.

 ShapeFit is a formulation of this estimation problem which is robust to
 outliers. It is a nonsmooth, convex optimization problem over an affine
 space, i.e., a linear manifold. We smooth the cost using the pseudo-Huber
 loss cost and solve the problem using Manopt. This requires two
 ingredients: (1) a factory to describe the affine space, see
 shapefitfactory; (2) defining the cost and its derivative, and minimizing
 it while progressively tightening the smooth approximation (with
 warm-start).

 Simply run the example to see the results on random data. It compares the
 smoothed ShapeFit formulation against a least-squares formulation, when
 the measurements include outliers. See in code to vary the noise
 parameters, dimension d, number of nodes n, number of measurements m, ...

 Note: since the problem is convex, this returns the global optimum.
 This example also illustrates the use of Manopt for optimization under
 linear constraints: admittedly a simple subcase of optimization on
 manifolds.


 See also: shapefitfactory</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="../manopt/manifolds/euclidean/shapefitfactory.html" class="code" title="function M = shapefitfactory(VJt)">shapefitfactory</a>	Linear manifold structure for optimization over the ShapeFit search space</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/full.html" class="code" title="function y = full( x )">full</a>	FULL Convert TTeMPS tensor to full array</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/minus.html" class="code" title="function z = minus( x, y )">minus</a>	MINUS Substraction of two TT/MPS tensors.</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/norm.html" class="code" title="function res = norm( x, safe )">norm</a>	NORM Norm of a TT/MPS tensor.</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS_block/full.html" class="code" title="function y = full( x )">full</a>	FULL Convert TTeMPS tensor to full array</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS_block/minus.html" class="code" title="function z = minus( x, y )">minus</a>	MINUS Substraction of two TT/MPS block-mu tensors.</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS_block/norm.html" class="code" title="function res = norm( x )">norm</a>	NORM Norm of a TT/MPS block-mu tensor.</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS_op/full.html" class="code" title="function Afull = full( A )">full</a>	FULL Convert TTeMPS_op operator to full array</li><li><a href="../manopt/solvers/trustregions/trustregions.html" class="code" title="function [x, cost, info, options] = trustregions(problem, x, options)">trustregions</a>	Riemannian trust-regions solver for optimization on manifolds.</li></ul>
This function is called by:
<ul style="list-style-image:url(../matlabicon.gif)">
</ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="#_sub1" class="code">function AT = Afun(T, V, J)</a></li><li><a href="#_sub2" class="code">function T_cvx = shapefit_cvx(V, J)</a></li><li><a href="#_sub3" class="code">function [I, J, A] = erdosrenyi(n, p)</a></li><li><a href="#_sub4" class="code">function [I, J, A] = randomgraph(n, m)</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [T_hub, T_lsq, T_cvx] = shapefit_smoothed(V, J)</a>
0002 <span class="comment">% ShapeFit formulation for sensor network localization from pair directions</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% function [T_hub, T_lsq, T_cvx] = shapefit_smoothed(V, J)</span>
0005 <span class="comment">%</span>
0006 <span class="comment">% This example in based on the paper http://arxiv.org/abs/1506.01437:</span>
0007 <span class="comment">% ShapeFit: Exact location recovery from corrupted pairwise directions, 2015</span>
0008 <span class="comment">% by Paul Hand, Choongbum Lee and Vladislav Voroninski.</span>
0009 <span class="comment">%</span>
0010 <span class="comment">% The problem is the following: there are n points t_1, ..., t_n in R^d</span>
0011 <span class="comment">% which need to be estimated (localized). To this end, we are given</span>
0012 <span class="comment">% measurements of some of the pairwise directions,</span>
0013 <span class="comment">% v_ij = (t_i - t_j) / norm(t_i - t_j) + noise.</span>
0014 <span class="comment">% Assume there are m such pairwise measurements, defining a graph with m</span>
0015 <span class="comment">% edges over n nodes. J is the signed incidence matrix of this graph (see</span>
0016 <span class="comment">% in code). To build J from lists I, J in R^m of nodes, use:</span>
0017 <span class="comment">% J = sparse([I ; J], [(1:m)' ; (1:m)'], [ones(m, 1), -ones(m, 1)], n, m, 2*m);</span>
0018 <span class="comment">%</span>
0019 <span class="comment">% The measurements are arranged in the matrix V of size d x m. From V, we</span>
0020 <span class="comment">% attempt to estimate t_1, ..., t_n, arranged in T, a matrix of size d x n.</span>
0021 <span class="comment">% The estimation can only be done up to translation and scaling. The</span>
0022 <span class="comment">% returned T's are centered: the columns sum to zero.</span>
0023 <span class="comment">%</span>
0024 <span class="comment">% ShapeFit is a formulation of this estimation problem which is robust to</span>
0025 <span class="comment">% outliers. It is a nonsmooth, convex optimization problem over an affine</span>
0026 <span class="comment">% space, i.e., a linear manifold. We smooth the cost using the pseudo-Huber</span>
0027 <span class="comment">% loss cost and solve the problem using Manopt. This requires two</span>
0028 <span class="comment">% ingredients: (1) a factory to describe the affine space, see</span>
0029 <span class="comment">% shapefitfactory; (2) defining the cost and its derivative, and minimizing</span>
0030 <span class="comment">% it while progressively tightening the smooth approximation (with</span>
0031 <span class="comment">% warm-start).</span>
0032 <span class="comment">%</span>
0033 <span class="comment">% Simply run the example to see the results on random data. It compares the</span>
0034 <span class="comment">% smoothed ShapeFit formulation against a least-squares formulation, when</span>
0035 <span class="comment">% the measurements include outliers. See in code to vary the noise</span>
0036 <span class="comment">% parameters, dimension d, number of nodes n, number of measurements m, ...</span>
0037 <span class="comment">%</span>
0038 <span class="comment">% Note: since the problem is convex, this returns the global optimum.</span>
0039 <span class="comment">% This example also illustrates the use of Manopt for optimization under</span>
0040 <span class="comment">% linear constraints: admittedly a simple subcase of optimization on</span>
0041 <span class="comment">% manifolds.</span>
0042 <span class="comment">%</span>
0043 <span class="comment">%</span>
0044 <span class="comment">% See also: shapefitfactory</span>
0045 
0046 <span class="comment">% This file is part of Manopt: www.manopt.org.</span>
0047 <span class="comment">% Original author: Nicolas Boumal, June 18, 2015.</span>
0048 <span class="comment">% Contributors:</span>
0049 <span class="comment">% Change log:</span>
0050 <span class="comment">%</span>
0051 <span class="comment">%   Jan.  4, 2021 (NB):</span>
0052 <span class="comment">%       Changes for compatibility with Octave 6.1.0.</span>
0053 <span class="comment">%</span>
0054 <span class="comment">%   Aug. 23, 2021 (XJ):</span>
0055 <span class="comment">%       Added AD to compute the egrad and the ehess</span>
0056 
0057     <span class="comment">% Generic useful functions</span>
0058     center_cols = @(A) bsxfun(@<a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/minus.html" class="code" title="function z = minus( x, y )">minus</a>, A, mean(A, 2));
0059     normalize_cols = @(A) bsxfun(@times, A, 1./sqrt(sum(A.^2, 1)));
0060     sqnorm_cols = @(A) sum(A.^2, 1);
0061 
0062     
0063     <span class="comment">% DATA GENERATION</span>
0064     <span class="comment">%</span>
0065     <span class="comment">% If no inputs are specified, generate some random data for</span>
0066     <span class="comment">% illustration purposes.</span>
0067     <span class="keyword">if</span> nargin == 0
0068 
0069         <span class="comment">% We estimate n points in R^d</span>
0070         d =   2;
0071         n = 500;
0072 
0073         <span class="comment">% Those points are the columns of T : they are what we need to</span>
0074         <span class="comment">% estimate, up to scaling and translation. We center T for</span>
0075         <span class="comment">% convenience.</span>
0076         T_tru = center_cols(rand(d, n));
0077 
0078         <span class="comment">% We get a measurement of some pairs of relative directions.</span>
0079         <span class="comment">% Which pairs is encoded in this graph, with J being the (signed,</span>
0080         <span class="comment">% transposed) incidence matrix. J is n x m, sparse.</span>
0081         <span class="comment">% There are roughly edge_fraction * n * (n-1) / 2 measurements.</span>
0082         edge_fraction = 0.10;
0083         <span class="comment">% [ii, jj] = erdosrenyi(n, edge_fraction);</span>
0084         [ii, jj] = <a href="#_sub4" class="code" title="subfunction [I, J, A] = randomgraph(n, m)">randomgraph</a>(n, edge_fraction*nchoosek(n, 2));
0085         m = length(ii);
0086         J = sparse([ii ; jj], [(1:m)' ; (1:m)'], <span class="keyword">...</span>
0087                    [ones(m, 1), -ones(m, 1)], n, m, 2*m);
0088 
0089         <span class="comment">% The measurements give the directions from one point to another.</span>
0090         <span class="comment">% That is: we get the position difference, normalized. Here, with</span>
0091         <span class="comment">% Gaussian noise. Least-squares will be well-suited for this.</span>
0092         sigma = .0;
0093         V = normalize_cols(T_tru*J + sigma*randn(d, m)); <span class="comment">% d x m</span>
0094 
0095         <span class="comment">% Outliers: we replace some of the direction measurements by</span>
0096         <span class="comment">% uniformly random unit-norm vectors.</span>
0097         outlier_fraction = .3;
0098         outliers = rand(1, m) &lt; outlier_fraction;
0099         V(:, outliers) = normalize_cols(randn(d, sum(outliers)));
0100         
0101     <span class="keyword">end</span> <span class="comment">% done generating random data</span>
0102     
0103     
0104     
0105     
0106     
0107     [d, m] = size(V);
0108     n = size(J, 1);
0109     assert(size(J, 2) == m, <span class="string">'J must be n x m, with V of size d x m.'</span>);
0110 
0111     VJt = <a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/full.html" class="code" title="function y = full( x )">full</a>(V*J');
0112 
0113     <span class="comment">% This &quot;manifold&quot; describes the Euclidean space of matrices T of size</span>
0114     <span class="comment">% d x n such that &lt;VJt, T&gt; = 1 and T has centered columns: T1 = 0.</span>
0115     problem.M = <a href="../manopt/manifolds/euclidean/shapefitfactory.html" class="code" title="function M = shapefitfactory(VJt)">shapefitfactory</a>(VJt);
0116 
0117     <span class="comment">% This linear operator computes the orthogonal projection of each</span>
0118     <span class="comment">% difference ti - tj on the orthogonal space to v_ij.</span>
0119     <span class="comment">% If the alignment is compatible with the data, then this is zero.</span>
0120     <span class="comment">% A(T) is a d x m matrix.</span>
0121     A = @(T) <a href="#_sub1" class="code" title="subfunction AT = Afun(T, V, J)">Afun</a>(T, V, J);
0122     <a name="_sub1" href="#_subfunctions" class="code">function AT = Afun(T, V, J)</a>
0123         TJ = T*J;
0124         AT = TJ - bsxfun(@times, V, sum(V .* TJ, 1));
0125     <span class="keyword">end</span>
0126 
0127     <span class="comment">% Need the adjoint of A, too. Input is d x m, output is d x n.</span>
0128     Astar = @(W) (W - bsxfun(@times, V, sum(V.*W, 1)))*J';
0129 
0130     
0131     
0132     <span class="comment">% LEAST-SQUARES</span>
0133     <span class="comment">%</span>
0134     <span class="comment">% First, work with a least-squares formulation of the problem.</span>
0135     <span class="comment">% That is, we minimize a (very nice) convex cost over an affine space.</span>
0136     <span class="comment">% Since the smooth solvers in Manopt converge to critical points, this</span>
0137     <span class="comment">% means they converge to global optimizers.</span>
0138     problem.cost  = @(T) 0.5*<a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/norm.html" class="code" title="function res = norm( x, safe )">norm</a>(A(T), <span class="string">'fro'</span>)^2;
0139     problem.egrad = @(T) Astar(A(T));
0140     problem.ehess = @(T, Tdot) Astar(A(Tdot));
0141 
0142     <span class="comment">% An alternative way to compute the egrad and the ehess is to use</span>
0143     <span class="comment">% automatic differentiation provided in the deep learning toolbox (slower)</span>
0144     <span class="comment">% Notice that the function norm is not supported for AD so far.</span>
0145     <span class="comment">% Replace norm(...,'fro')^2 with cnormsqfro described in the file</span>
0146     <span class="comment">% manoptADhelp.m. Also operations between sparse matrices and dlarrys</span>
0147     <span class="comment">% is not supported so far. Transform V,J into full matrices. AD does</span>
0148     <span class="comment">% not support bsxfunc as well. Translate it into the expression of</span>
0149     <span class="comment">% repmat and .*. The whole thing would make the computation much slower</span>
0150     <span class="comment">% than specifying the egrad and the ehess.</span>
0151     <span class="comment">% V_full = full(V);</span>
0152     <span class="comment">% J_full = full(J);</span>
0153     <span class="comment">% problem.cost  = @(T) 0.5*cnormsqfro(A_AD(T));</span>
0154     <span class="comment">% function AT = A_AD(T)</span>
0155     <span class="comment">%    sum1 = sum(V_full .* (T*J_full), 1);</span>
0156     <span class="comment">%    repsum1 = repmat(sum1,size(sum1,1),1);</span>
0157     <span class="comment">%    AT = T*J_full - V_full.*repsum1;</span>
0158     <span class="comment">% end</span>
0159     <span class="comment">% call manoptAD to prepare AD for the problem structure</span>
0160     <span class="comment">% problem = manoptAD(problem);</span>
0161     
0162     T_lsq = <a href="../manopt/solvers/trustregions/trustregions.html" class="code" title="function [x, cost, info, options] = trustregions(problem, x, options)">trustregions</a>(problem);
0163     
0164 
0165     
0166     <span class="comment">% PSEUDO-HUBER SMOOTHED SHAPEFIT</span>
0167     <span class="comment">%</span>
0168     <span class="comment">% Now solve the same, but with a pseudo-Huber loss instead of</span>
0169     <span class="comment">% least-squares.</span>
0170     <span class="comment">% We iteratively sharpen the Huber function, i.e., reduce delta.</span>
0171     <span class="comment">% It is important to warm start in such a fashion: trying to optimize</span>
0172     <span class="comment">% with a random initial guess and a very small delta is typically slow.</span>
0173     <span class="comment">% How fast one should decrease delta, and how accurately one should</span>
0174     <span class="comment">% optimize at each intermediate stage, is open for research.</span>
0175     delta = 1;
0176     T_hub = []; <span class="comment">% We could use T_lsq as initial guess, too.</span>
0177     problem = rmfield(problem, <span class="string">'ehess'</span>);
0178     warning(<span class="string">'off'</span>, <span class="string">'manopt:getHessian:approx'</span>);
0179     <span class="keyword">for</span> iter = 1 : 12
0180         
0181         delta = delta / 2;
0182         
0183         h = @(x2) sqrt(x2 + delta^2) - delta; <span class="comment">% pseudo-Huber loss</span>
0184 
0185         problem.cost  = @(T) sum(h(sqnorm_cols(A(T))));
0186         problem.egrad = @(T) Astar(bsxfun(@times, A(T), <span class="keyword">...</span>
0187                                     1./sqrt(sqnorm_cols(A(T)) + delta^2)));
0188 
0189         <span class="comment">% AD version</span>
0190         <span class="comment">% problem = rmfield(problem, 'egrad');</span>
0191         <span class="comment">% problem = rmfield(problem, 'autogradfunc');</span>
0192         <span class="comment">% problem = manoptAD(problem,'egrad');</span>
0193                                 
0194         <span class="comment">% Solve, using the previous solution as initial guess.</span>
0195         T_hub = <a href="../manopt/solvers/trustregions/trustregions.html" class="code" title="function [x, cost, info, options] = trustregions(problem, x, options)">trustregions</a>(problem, T_hub);
0196         
0197     <span class="keyword">end</span>
0198     
0199     
0200     
0201     <span class="comment">% CVX SHAPEFIT</span>
0202     <span class="comment">%</span>
0203     <span class="comment">% Actual ShapeFit cost (nonsmooth), with CVX.</span>
0204     <span class="comment">% You can get CVX from http://cvxr.com/.</span>
0205     use_cvx_if_available = false;
0206     <span class="keyword">if</span> use_cvx_if_available &amp;&amp; exist(<span class="string">'cvx_version'</span>, <span class="string">'file'</span>)
0207         T_cvx = <a href="#_sub2" class="code" title="subfunction T_cvx = shapefit_cvx(V, J)">shapefit_cvx</a>(V, J);
0208     <span class="keyword">else</span>
0209         T_cvx = NaN(d, n);
0210     <span class="keyword">end</span>
0211     
0212     
0213     
0214     <span class="comment">% VISUALIZATION</span>
0215     <span class="comment">%</span>
0216     <span class="comment">% If T_true is available, for display, we scale the estimators to match</span>
0217     <span class="comment">% the norm of the target. The scaling factor is obtained by minimizing</span>
0218     <span class="comment">% the norm of the discrepancy : norm(T_tru - scale*T_xxx, 'fro').</span>
0219     <span class="comment">% A plot is produced if d is 2 or 3.</span>
0220     <span class="keyword">if</span> exist(<span class="string">'T_tru'</span>, <span class="string">'var'</span>) &amp;&amp; (d == 2 || d == 3)
0221         
0222         T_lsq = T_lsq * trace(T_tru'*T_lsq) / <a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/norm.html" class="code" title="function res = norm( x, safe )">norm</a>(T_lsq, <span class="string">'fro'</span>)^2;
0223         T_hub = T_hub * trace(T_tru'*T_hub) / <a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/norm.html" class="code" title="function res = norm( x, safe )">norm</a>(T_hub, <span class="string">'fro'</span>)^2;
0224         T_cvx = T_cvx * trace(T_tru'*T_cvx) / <a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/norm.html" class="code" title="function res = norm( x, safe )">norm</a>(T_cvx, <span class="string">'fro'</span>)^2;
0225 
0226     
0227         <span class="keyword">switch</span> d
0228             <span class="keyword">case</span> 2
0229                 plot(T_tru(1, :), T_tru(2, :), <span class="string">'bo'</span>, <span class="keyword">...</span>
0230                      T_lsq(1, :), T_lsq(2, :), <span class="string">'rx'</span>, <span class="keyword">...</span>
0231                      T_hub(1, :), T_hub(2, :), <span class="string">'k.'</span>, <span class="keyword">...</span>
0232                      T_cvx(1, :), T_cvx(2, :), <span class="string">'g.'</span>);
0233             <span class="keyword">case</span> 3
0234                 plot3(T_tru(1, :), T_tru(2, :), T_tru(3, :), <span class="string">'bo'</span>, <span class="keyword">...</span>
0235                       T_lsq(1, :), T_lsq(2, :), T_lsq(3, :), <span class="string">'rx'</span>, <span class="keyword">...</span>
0236                       T_hub(1, :), T_hub(2, :), T_hub(3, :), <span class="string">'k.'</span>, <span class="keyword">...</span>
0237                       T_cvx(1, :), T_cvx(2, :), T_cvx(3, :), <span class="string">'g.'</span>);
0238         <span class="keyword">end</span>
0239 
0240         legend(<span class="string">'ground truth'</span>, <span class="string">'least squares'</span>, <span class="keyword">...</span>
0241                sprintf(<span class="string">'pseudo-huber, \\delta = %.1e'</span>, delta), <span class="keyword">...</span>
0242                <span class="string">'CVX ShapeFit'</span>);
0243            
0244         title(sprintf([<span class="string">'ShapeFit problem : d = %d, n = %d, edge '</span> <span class="keyword">...</span>
0245                        <span class="string">'fraction = %.2g, sigma = %.2g, outlier '</span> <span class="keyword">...</span>
0246                        <span class="string">'fraction = %.2g'</span>], d, n, edge_fraction, sigma, <span class="keyword">...</span>
0247                        outlier_fraction));
0248         axis equal;
0249     
0250     <span class="keyword">end</span>
0251 
0252 <span class="keyword">end</span>
0253 
0254 
0255 <span class="comment">% If CVX is available, it can be used to solve the nonsmooth problem</span>
0256 <span class="comment">% directly, very elegantly.</span>
0257 <a name="_sub2" href="#_subfunctions" class="code">function T_cvx = shapefit_cvx(V, J)</a>
0258     d = size(V, 1);
0259     n = size(J, 1); <span class="comment">%#ok&lt;NASGU&gt;</span>
0260     VJt = <a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/full.html" class="code" title="function y = full( x )">full</a>(V*J');
0261     cvx_begin
0262         variable T_cvx(d, n)
0263         <span class="comment">% We want to minimize this:</span>
0264         <span class="comment">% minimize sum( norms( A(T_cvx), 2, 1 ) )</span>
0265         <span class="comment">% But unfortunately, CVX doesn't handle bsxfun. Instead, we use</span>
0266         <span class="comment">% repmat, which is slower, and hence hurts the comparison in</span>
0267         <span class="comment">% disfavor of CVX.</span>
0268         minimize sum( norms( T_cvx*J - V .* repmat(sum(V .* (T_cvx*J), 1), [d, 1])  , 2, 1 ) )
0269         sum(T_cvx, 2) == zeros(d, 1); <span class="comment">%#ok&lt;NODEF,EQEFF&gt;</span>
0270         VJt(:).' * T_cvx(:) == 1; <span class="comment">%#ok&lt;EQEFF&gt;</span>
0271     cvx_end
0272 <span class="keyword">end</span>
0273 
0274 
0275 <a name="_sub3" href="#_subfunctions" class="code">function [I, J, A] = erdosrenyi(n, p) </a><span class="comment">%#ok&lt;DEFNU&gt;</span>
0276 <span class="comment">% Generate a random Erdos-Renyi graph with n nodes and edge probability p.</span>
0277 <span class="comment">%</span>
0278 <span class="comment">% [I, J, A] = erdosrenyi(n, p)</span>
0279 <span class="comment">%</span>
0280 <span class="comment">% Returns a list of edges (I(k), J(k)) for a random, undirected Erdos-Renyi</span>
0281 <span class="comment">% graph with n nodes and edge probability p. A is the adjacency matrix.</span>
0282 <span class="comment">%</span>
0283 <span class="comment">% I(k) &lt; J(k) for all k, i.e., all(I&lt;J) is true.</span>
0284 <span class="comment">%</span>
0285 <span class="comment">% The memory requirements for this simple implementation scale as O(n^2).</span>
0286 
0287     X = rand(n);
0288     mask = X &lt;= p;
0289     X( mask) = 1;
0290     X(~mask) = 0;
0291     X = triu(X, 1);
0292 
0293     <span class="comment">% A is the adjacency matrix</span>
0294     A = X + X';
0295     
0296     [I, J] = find(X);
0297 
0298 <span class="keyword">end</span>
0299 
0300 
0301 <a name="_sub4" href="#_subfunctions" class="code">function [I, J, A] = randomgraph(n, m)</a>
0302 <span class="comment">% Generates a random graph over n nodes with at most m edges.</span>
0303 <span class="comment">%</span>
0304 <span class="comment">% function [I, J, A] = randomgraph(n, m)</span>
0305 <span class="comment">%</span>
0306 <span class="comment">% Selects m (undirected) edges from a graph over n nodes, uniformly at</span>
0307 <span class="comment">% random, with replacement. The self edges and repeated edges are then</span>
0308 <span class="comment">% discarded. The remaining number of edges is at most m, and should be</span>
0309 <span class="comment">% close to m if m is much smaller than nchoosek(n, 2).</span>
0310 <span class="comment">%</span>
0311 <span class="comment">% The output satisfies all(I &lt; J). A is the corresponding adjacency matrix.</span>
0312 <span class="comment">%</span>
0313 <span class="comment">% Uses O(m) memory (not O(n^2)), making it fit for large, sparse graphs.</span>
0314 
0315     <span class="comment">% Generate m edges at random, with replacement, and remove repetitions.</span>
0316     IJ = unique(sort(randi(n, m, 2), 2), <span class="string">'rows'</span>);
0317     
0318     <span class="comment">% Remove self-edges if any.</span>
0319     IJ = IJ(IJ(:, 1) ~= IJ(:, 2), :);
0320     
0321     <span class="comment">% Actual number of selected edges</span>
0322     k = size(IJ, 1);
0323     
0324     I = IJ(:, 1);
0325     J = IJ(:, 2);
0326     
0327     <span class="comment">% Build the adjacency matrix of the graph.</span>
0328     A = sparse([I ; J], [J ; I], ones(2*k, 1), n, n, 2*k);
0329 
0330 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Fri 30-Sep-2022 13:18:25 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>