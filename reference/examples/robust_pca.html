<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of robust_pca</title>
  <meta name="keywords" content="robust_pca">
  <meta name="description" content="Computes a robust version of PCA (principal component analysis) on data.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../index.html">Home</a> &gt;  <a href="index.html">examples</a> &gt; robust_pca.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../index.html"><img alt="<" border="0" src="../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for examples&nbsp;<img alt=">" border="0" src="../right.png"></a></td></tr></table>-->

<h1>robust_pca
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>Computes a robust version of PCA (principal component analysis) on data.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>function [U, cost] = robust_pca(X, d) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Computes a robust version of PCA (principal component analysis) on data.
 
 function [U, cost] = robustpca(X, d)

 Given a matrix X of size p by n, such that each column represents a
 point in R^p, this computes U: an orthonormal basis of size p by d such
 that the column space of U captures the points X as well as possible.
 More precisely, the function attempts to compute U as the minimizer
 over the Grassmann manifold (the set of linear subspaces) of:

  f(U) = (1/n) Sum_{i = 1:n} dist(X(:, i), the space spanned by U)
       = (1/n) Sum_{i = 1:n} || U*U'*X(:, i) - X(:, i) ||

 The output cost represents the average distance achieved with the
 returned U. Notice that norms are not squared, for robustness.

 In practice, because this function is nonsmooth, it is smoothed with a
 pseudo-Huber loss function of parameter epsilon (noted e for short), and
 the smoothing parameter is iteratively reduced (with warm starts):

   f_e(U) = (1/n) Sum_{i = 1:n} l_e(|| U*U'*X(:, i) - X(:, i) ||)

   with l_e(x) = sqrt(x^2 + e^2) - e (for e = 0, this is absolute value).

 The intermediate optimization of the smooth cost over the Grassmann
 manifold is performed using the Manopt toolbox.

 Ideally, the non-outlier data should be centered. If not, this
 pre-processing centers all the data, but bear in mind that outliers will
 shift the center of mass too.
 X = X - repmat(mean(X, 2), [1, size(X, 2)]);

 There are no guarantees that this code will return the optimal U.
 This code is distributed to illustrate one possible way of optimizing
 a nonsmooth cost function over a manifold, using Manopt with smoothing.
 For practical use, the constants in the code would need to be tuned.</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="../manopt/manifolds/grassmann/grassmannfactory.html" class="code" title="function M = grassmannfactory(n, p, k)">grassmannfactory</a>	Returns a manifold struct to optimize over the space of vector subspaces.</li><li><a href="../manopt/solvers/trustregions/trustregions.html" class="code" title="function [x, cost, info, options] = trustregions(problem, x, options)">trustregions</a>	Riemannian trust-regions solver for optimization on manifolds.</li><li><a href="../manopt/tools/multiprod.html" class="code" title="function c = multiprod(a, b, idA, idB)">multiprod</a>	Multiplying 1-D or 2-D subarrays contained in two N-D arrays.</li><li><a href="../manopt/tools/multiscale.html" class="code" title="function A = multiscale(scale, A)">multiscale</a>	Multiplies the 2D slices in a 3D matrix by individual scalars.</li><li><a href="../manopt/tools/multitransp.html" class="code" title="function b = multitransp(a, dim)">multitransp</a>	Transposing arrays of matrices.</li></ul>
This function is called by:
<ul style="list-style-image:url(../matlabicon.gif)">
</ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="#_sub1" class="code">function value = robustpca_cost(U)</a></li><li><a href="#_sub2" class="code">function G = robustpca_gradient(U)</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [U, cost] = robust_pca(X, d)</a>
0002 <span class="comment">% Computes a robust version of PCA (principal component analysis) on data.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% function [U, cost] = robustpca(X, d)</span>
0005 <span class="comment">%</span>
0006 <span class="comment">% Given a matrix X of size p by n, such that each column represents a</span>
0007 <span class="comment">% point in R^p, this computes U: an orthonormal basis of size p by d such</span>
0008 <span class="comment">% that the column space of U captures the points X as well as possible.</span>
0009 <span class="comment">% More precisely, the function attempts to compute U as the minimizer</span>
0010 <span class="comment">% over the Grassmann manifold (the set of linear subspaces) of:</span>
0011 <span class="comment">%</span>
0012 <span class="comment">%  f(U) = (1/n) Sum_{i = 1:n} dist(X(:, i), the space spanned by U)</span>
0013 <span class="comment">%       = (1/n) Sum_{i = 1:n} || U*U'*X(:, i) - X(:, i) ||</span>
0014 <span class="comment">%</span>
0015 <span class="comment">% The output cost represents the average distance achieved with the</span>
0016 <span class="comment">% returned U. Notice that norms are not squared, for robustness.</span>
0017 <span class="comment">%</span>
0018 <span class="comment">% In practice, because this function is nonsmooth, it is smoothed with a</span>
0019 <span class="comment">% pseudo-Huber loss function of parameter epsilon (noted e for short), and</span>
0020 <span class="comment">% the smoothing parameter is iteratively reduced (with warm starts):</span>
0021 <span class="comment">%</span>
0022 <span class="comment">%   f_e(U) = (1/n) Sum_{i = 1:n} l_e(|| U*U'*X(:, i) - X(:, i) ||)</span>
0023 <span class="comment">%</span>
0024 <span class="comment">%   with l_e(x) = sqrt(x^2 + e^2) - e (for e = 0, this is absolute value).</span>
0025 <span class="comment">%</span>
0026 <span class="comment">% The intermediate optimization of the smooth cost over the Grassmann</span>
0027 <span class="comment">% manifold is performed using the Manopt toolbox.</span>
0028 <span class="comment">%</span>
0029 <span class="comment">% Ideally, the non-outlier data should be centered. If not, this</span>
0030 <span class="comment">% pre-processing centers all the data, but bear in mind that outliers will</span>
0031 <span class="comment">% shift the center of mass too.</span>
0032 <span class="comment">% X = X - repmat(mean(X, 2), [1, size(X, 2)]);</span>
0033 <span class="comment">%</span>
0034 <span class="comment">% There are no guarantees that this code will return the optimal U.</span>
0035 <span class="comment">% This code is distributed to illustrate one possible way of optimizing</span>
0036 <span class="comment">% a nonsmooth cost function over a manifold, using Manopt with smoothing.</span>
0037 <span class="comment">% For practical use, the constants in the code would need to be tuned.</span>
0038 
0039 <span class="comment">% This file is part of Manopt and is copyrighted. See the license file.</span>
0040 <span class="comment">%</span>
0041 <span class="comment">% Main author: Nicolas Boumal and Teng Zhang, May 2, 2014</span>
0042 <span class="comment">% Contributors:</span>
0043 <span class="comment">%</span>
0044 <span class="comment">% Change log:</span>
0045 <span class="comment">%</span>
0046 <span class="comment">%   March 4, 2015 (NB):</span>
0047 <span class="comment">%       Uses a pseudo-Huber loss rather than a Huber loss: this has the</span>
0048 <span class="comment">%       nice advantage of being smooth and simpler to code (no if's).</span>
0049 <span class="comment">%</span>
0050 <span class="comment">%   April 8, 2015 (NB):</span>
0051 <span class="comment">%       Built-in test data for quick tests; added comment about centering.</span>
0052 
0053 
0054 
0055     <span class="comment">% If no inputs, generate random data for illustration purposes.</span>
0056     <span class="keyword">if</span> nargin == 0
0057         <span class="comment">% Generate some data points aligned on a subspace</span>
0058         X = rand(2, 1)*(1:30) + .05*randn(2, 30).*[(1:30);(1:30)];
0059         <span class="comment">% And add some random outliers to the mix</span>
0060         P = randperm(size(X, 2));
0061         outliers = 10;
0062         X(:, P(1:outliers)) = 30*randn(2, outliers);
0063         <span class="comment">% Center the data</span>
0064         <span class="comment">% X = X - repmat(mean(X, 2), [1, size(X, 2)]);</span>
0065         d = 1;
0066     <span class="keyword">end</span>
0067 
0068 
0069 
0070 
0071 
0072     <span class="comment">% Prepare a Manopt problem structure for optimization of the given</span>
0073     <span class="comment">% cost (defined below) over the Grassmann manifold.</span>
0074     [p, n] = size(X);
0075     manifold = <a href="../manopt/manifolds/grassmann/grassmannfactory.html" class="code" title="function M = grassmannfactory(n, p, k)">grassmannfactory</a>(p, d);
0076     problem.M = manifold;
0077     problem.cost = @<a href="#_sub1" class="code" title="subfunction value = robustpca_cost(U)">robustpca_cost</a>;
0078     problem.egrad = @<a href="#_sub2" class="code" title="subfunction G = robustpca_gradient(U)">robustpca_gradient</a>;
0079     
0080     <span class="comment">% Do classical PCA for the initial guess.</span>
0081     <span class="comment">% This is just one idea: it is not necessarily useful or ideal.</span>
0082     <span class="comment">% Using a random initial guess, and starting over for a few different</span>
0083     <span class="comment">% ones is probably much better. For this example, we keep it simple.</span>
0084     [U, ~, ~] = svds(X, d);
0085 
0086     
0087     <span class="comment">% Iteratively reduce the smoothing constant epsilon and optimize</span>
0088     <span class="comment">% the cost function over Grassmann.</span>
0089     epsilon = 1;
0090     n_iterations = 6;
0091     reduction = .5;
0092     options.verbosity = 2; <span class="comment">% Change this number for more or less output</span>
0093     warning(<span class="string">'off'</span>, <span class="string">'manopt:getHessian:approx'</span>);
0094     <span class="keyword">for</span> iter = 1 : n_iterations
0095         U = <a href="../manopt/solvers/trustregions/trustregions.html" class="code" title="function [x, cost, info, options] = trustregions(problem, x, options)">trustregions</a>(problem, U, options);
0096         epsilon = epsilon * reduction;
0097     <span class="keyword">end</span>
0098     warning(<span class="string">'on'</span>, <span class="string">'manopt:getHessian:approx'</span>);
0099     
0100     
0101     <span class="comment">% Return the cost as the actual sum of distances, not smoothed.</span>
0102     epsilon = 0;
0103     cost = <a href="#_sub1" class="code" title="subfunction value = robustpca_cost(U)">robustpca_cost</a>(U);
0104     
0105     
0106     
0107     <span class="comment">% If working with the auto-generated input, plot the results.</span>
0108     <span class="keyword">if</span> nargin == 0
0109         scatter(X(1,:), X(2,:));
0110         hold on;
0111         plot(U(1)*[-1, 1]*100, U(2)*[-1 1]*100, <span class="string">'r'</span>);
0112         hold off;
0113         <span class="comment">% Compare to a standard PCA</span>
0114         [Upca, ~, ~] = svds(X,1);
0115         hold on;
0116         plot(Upca(1)*[-1, 1]*100, Upca(2)*[-1 1]*100, <span class="string">'k'</span>);
0117         hold off;
0118         xlim(1.1*[min(X(1,:)), max(X(1,:))]);
0119         ylim(1.1*[min(X(2,:)), max(X(2,:))]);
0120         legend(<span class="string">'data points'</span>, <span class="string">'Robust PCA fit'</span>, <span class="string">'Standard PCA fit'</span>);
0121     <span class="keyword">end</span>
0122 
0123     
0124     
0125     <span class="comment">% Smoothed cost</span>
0126     <a name="_sub1" href="#_subfunctions" class="code">function value = robustpca_cost(U)</a>
0127 
0128         vecs = U*(U'*X) - X;
0129         sqnrms = sum(vecs.^2, 1);
0130         vals = sqrt(sqnrms + epsilon^2) - epsilon;
0131         value = mean(vals);
0132 
0133     <span class="keyword">end</span>
0134 
0135     <span class="comment">% Euclidean gradient of the smoothed cost (it will be transformed into</span>
0136     <span class="comment">% the Riemannian gradient automatically by Manopt).</span>
0137     <a name="_sub2" href="#_subfunctions" class="code">function G = robustpca_gradient(U)</a>
0138 
0139         <span class="comment">% Note that the computation of vecs and sqnrms is redundant</span>
0140         <span class="comment">% with their computation in the cost function. To speed</span>
0141         <span class="comment">% up the code, it would be wise to use the caching capabilities</span>
0142         <span class="comment">% of Manopt (the store structure). See online documentation.</span>
0143         <span class="comment">% It is not done here to keep the code a bit simpler.</span>
0144         UtX = U'*X;
0145         vecs = U*UtX-X;
0146         sqnrms = sum(vecs.^2, 1);
0147         <span class="comment">% This explicit loop is a bit slow: the code below is equivalent</span>
0148         <span class="comment">% and faster to compute the gradient.</span>
0149         <span class="comment">% G = zeros(p, d);</span>
0150         <span class="comment">% for i=1:n</span>
0151         <span class="comment">%     G = G + (1/sqrt(sqnrms(i) + epsilon^2)) * vecs(:,i) * UtX(:,i)';</span>
0152         <span class="comment">% end</span>
0153         <span class="comment">% G = G/n;</span>
0154         G = mean(<a href="../manopt/tools/multiscale.html" class="code" title="function A = multiscale(scale, A)">multiscale</a>(1./sqrt(sqnrms + epsilon^2), <span class="keyword">...</span>
0155                            <a href="../manopt/tools/multiprod.html" class="code" title="function c = multiprod(a, b, idA, idB)">multiprod</a>(reshape(vecs, [p, 1, n]), <span class="keyword">...</span>
0156                               <a href="../manopt/tools/multitransp.html" class="code" title="function b = multitransp(a, dim)">multitransp</a>(reshape(UtX, [d, 1, n])))), 3);
0157     <span class="keyword">end</span>
0158 
0159 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Tue 07-Jul-2015 09:52:38 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>