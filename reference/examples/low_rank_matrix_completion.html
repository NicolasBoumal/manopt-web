<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of low_rank_matrix_completion</title>
  <meta name="keywords" content="low_rank_matrix_completion">
  <meta name="description" content="Given partial observation of a low rank matrix, attempts to complete it.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../index.html">Home</a> &gt;  <a href="index.html">examples</a> &gt; low_rank_matrix_completion.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../index.html"><img alt="<" border="0" src="../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for examples&nbsp;<img alt=">" border="0" src="../right.png"></a></td></tr></table>-->

<h1>low_rank_matrix_completion
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>Given partial observation of a low rank matrix, attempts to complete it.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>function low_rank_matrix_completion() </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Given partial observation of a low rank matrix, attempts to complete it.

 function low_rank_matrix_completion()

 This example demonstrates how to use the geometry factory for the
 embedded submanifold of fixed-rank matrices, fixedrankembeddedfactory.
 This geometry is described in the paper
 &quot;Low-rank matrix completion by Riemannian optimization&quot;
 Bart Vandereycken - SIAM Journal on Optimization, 2013.

 This can be a starting point for many optimization problems of the form:

 minimize f(X) such that rank(X) = k, size(X) = [m, n].

 Note that the code is long because it showcases quite a few features of
 Manopt: most of the code is optional.

 Input:  None. This example file generates random data.
 
 Output: None.</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="../manopt/manifolds/fixedrank/fixedrankembeddedfactory.html" class="code" title="function M = fixedrankembeddedfactory(m, n, k)">fixedrankembeddedfactory</a>	Manifold struct to optimize fixed-rank matrices w/ an embedded geometry.</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/disp.html" class="code" title="function disp( x, name )">disp</a>	DISP Display TT/MPS tensor.</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/norm.html" class="code" title="function res = norm( x, safe )">norm</a>	NORM Norm of a TT/MPS tensor.</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS_block/disp.html" class="code" title="function disp( x, name )">disp</a>	DISP Display TT/MPS block-mu tensor.</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS_block/norm.html" class="code" title="function res = norm( x )">norm</a>	NORM Norm of a TT/MPS block-mu tensor.</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS_op/disp.html" class="code" title="function disp( x, name )">disp</a>	DISP Display TT/MPS operator.</li><li><a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS_op_laplace/disp.html" class="code" title="function disp( x, name )">disp</a>	DISP Display TT/MPS operator.</li><li><a href="../manopt/solvers/conjugategradient/conjugategradient.html" class="code" title="function [x, cost, info, options] = conjugategradient(problem, x, options)">conjugategradient</a>	Conjugate gradient minimization algorithm for Manopt.</li><li><a href="../manopt/solvers/trustregions/trustregions.html" class="code" title="function [x, cost, info, options] = trustregions(problem, x, options)">trustregions</a>	Riemannian trust-regions solver for optimization on manifolds.</li><li><a href="../manopt/tools/hessianspectrum.html" class="code" title="function lambdas = hessianspectrum(problem, x, usepreconstr, storedb, key)">hessianspectrum</a>	Returns the eigenvalues of the (preconditioned) Hessian at x.</li></ul>
This function is called by:
<ul style="list-style-image:url(../matlabicon.gif)">
</ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="#_sub1" class="code">function f = cost(X)</a></li><li><a href="#_sub2" class="code">function G = egrad(X)</a></li><li><a href="#_sub3" class="code">function ehess = euclidean_hessian(X, H)</a></li><li><a href="#_sub4" class="code">function t = linesearch_helper(X, H)</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function low_rank_matrix_completion()</a>
0002 <span class="comment">% Given partial observation of a low rank matrix, attempts to complete it.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% function low_rank_matrix_completion()</span>
0005 <span class="comment">%</span>
0006 <span class="comment">% This example demonstrates how to use the geometry factory for the</span>
0007 <span class="comment">% embedded submanifold of fixed-rank matrices, fixedrankembeddedfactory.</span>
0008 <span class="comment">% This geometry is described in the paper</span>
0009 <span class="comment">% &quot;Low-rank matrix completion by Riemannian optimization&quot;</span>
0010 <span class="comment">% Bart Vandereycken - SIAM Journal on Optimization, 2013.</span>
0011 <span class="comment">%</span>
0012 <span class="comment">% This can be a starting point for many optimization problems of the form:</span>
0013 <span class="comment">%</span>
0014 <span class="comment">% minimize f(X) such that rank(X) = k, size(X) = [m, n].</span>
0015 <span class="comment">%</span>
0016 <span class="comment">% Note that the code is long because it showcases quite a few features of</span>
0017 <span class="comment">% Manopt: most of the code is optional.</span>
0018 <span class="comment">%</span>
0019 <span class="comment">% Input:  None. This example file generates random data.</span>
0020 <span class="comment">%</span>
0021 <span class="comment">% Output: None.</span>
0022 
0023 <span class="comment">% This file is part of Manopt and is copyrighted. See the license file.</span>
0024 <span class="comment">%</span>
0025 <span class="comment">% Main author: Nicolas Boumal, July 15, 2014</span>
0026 <span class="comment">% Contributors: Bart Vandereycken</span>
0027 <span class="comment">%</span>
0028 <span class="comment">% Change log:</span>
0029 <span class="comment">%</span>
0030 <span class="comment">%    Xiaowen Jiang Aug. 20, 2021</span>
0031 <span class="comment">%       Added AD to compute the egrad and the ehess</span>
0032     
0033     <span class="comment">% Random data generation. First, choose the size of the problem.</span>
0034     <span class="comment">% We will complete a matrix of size mxn of rank k.</span>
0035     m = 200;
0036     n = 200;
0037     k = 5;
0038     <span class="comment">% Generate a random mxn matrix A of rank k</span>
0039     L = randn(m, k);
0040     R = randn(n, k);
0041     A = L*R';
0042     <span class="comment">% Generate a random mask for observed entries: P(i, j) = 1 if the entry</span>
0043     <span class="comment">% (i, j) of A is observed, and 0 otherwise.</span>
0044     fraction = 4 * k*(m+n-k)/(m*n);
0045     P = sparse(rand(m, n) &lt;= fraction);
0046     <span class="comment">% Hence, we know the nonzero entries in PA:</span>
0047     PA = P.*A;
0048     
0049     
0050     
0051     
0052     
0053     
0054     
0055     <span class="comment">% Pick the manifold of matrices of size mxn of fixed rank k.</span>
0056     problem.M = <a href="../manopt/manifolds/fixedrank/fixedrankembeddedfactory.html" class="code" title="function M = fixedrankembeddedfactory(m, n, k)">fixedrankembeddedfactory</a>(m, n, k);
0057 
0058     <span class="comment">% Define the problem cost function. The input X is a structure with</span>
0059     <span class="comment">% fields U, S, V representing a rank k matrix as U*S*V'.</span>
0060     <span class="comment">% f(X) = 1/2 * || P.*(X-A) ||^2</span>
0061     problem.cost = @<a href="#_sub1" class="code" title="subfunction f = cost(X)">cost</a>;
0062     <a name="_sub1" href="#_subfunctions" class="code">function f = cost(X)</a>
0063         <span class="comment">% Note that it is very much inefficient to explicitly construct the</span>
0064         <span class="comment">% matrix X in this way. Seen as we only need to know the entries</span>
0065         <span class="comment">% of Xmat corresponding to the mask P, it would be far more</span>
0066         <span class="comment">% efficient to compute those only.</span>
0067         Xmat = X.U*X.S*X.V';
0068         f = .5*<a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/norm.html" class="code" title="function res = norm( x, safe )">norm</a>( P.*Xmat - PA , <span class="string">'fro'</span>)^2;
0069     <span class="keyword">end</span>
0070 
0071     <span class="comment">% Define the Euclidean gradient of the cost function, that is, the</span>
0072     <span class="comment">% gradient of f(X) seen as a standard function of X.</span>
0073     <span class="comment">% nabla f(X) = P.*(X-A)</span>
0074     problem.egrad = @<a href="#_sub2" class="code" title="subfunction G = egrad(X)">egrad</a>;
0075     <a name="_sub2" href="#_subfunctions" class="code">function G = egrad(X)</a>
0076         <span class="comment">% Same comment here about Xmat.</span>
0077         Xmat = X.U*X.S*X.V';
0078         G = P.*Xmat - PA;
0079     <span class="keyword">end</span>
0080 
0081     <span class="comment">% This is optional, but it's nice if you have it.</span>
0082     <span class="comment">% Define the Euclidean Hessian of the cost at X, along H, where H is</span>
0083     <span class="comment">% represented as a tangent vector: a structure with fields Up, Vp, M.</span>
0084     <span class="comment">% This is the directional derivative of nabla f(X) at X along Xdot:</span>
0085     <span class="comment">% nabla^2 f(X)[Xdot] = P.*Xdot</span>
0086     problem.ehess = @<a href="#_sub3" class="code" title="subfunction ehess = euclidean_hessian(X, H)">euclidean_hessian</a>;
0087     <a name="_sub3" href="#_subfunctions" class="code">function ehess = euclidean_hessian(X, H)</a>
0088         <span class="comment">% The function tangent2ambient transforms H (a tangent vector) into</span>
0089         <span class="comment">% its equivalent ambient vector representation. The output is a</span>
0090         <span class="comment">% structure with fields U, S, V such that U*S*V' is an mxn matrix</span>
0091         <span class="comment">% corresponding to the tangent vector H. Note that there are no</span>
0092         <span class="comment">% additional guarantees about U, S and V. In particular, U and V</span>
0093         <span class="comment">% are not orthonormal.</span>
0094         ambient_H = problem.M.tangent2ambient(X, H);
0095         Xdot = ambient_H.U*ambient_H.S*ambient_H.V';
0096         <span class="comment">% Same comment here about explicitly constructing the ambient</span>
0097         <span class="comment">% vector as an mxn matrix Xdot: we only need its entries</span>
0098         <span class="comment">% corresponding to the mask P, and this could be computed</span>
0099         <span class="comment">% efficiently.</span>
0100         ehess = P.*Xdot;
0101     <span class="keyword">end</span>
0102     
0103     <span class="comment">% An alternative way to compute the egrad and the ehess is to use</span>
0104     <span class="comment">% automatic differentiation provided in the deep learning toolbox (slower)</span>
0105     <span class="comment">% Notice that the function norm is not supported for AD so far.</span>
0106     <span class="comment">% Replace norm(...,'fro')^2 with cnormsqfro described in the file</span>
0107     <span class="comment">% manoptADhelp.m. Also, operations between sparse matrices and dlarrays</span>
0108     <span class="comment">% are not supported for now. convert PA and P into full matrices.</span>
0109     <span class="comment">% PA_full = full(PA);</span>
0110     <span class="comment">% P_full = full(P);</span>
0111     <span class="comment">% problem.cost = @cost_AD;</span>
0112     <span class="comment">%    function f = cost_AD(X)</span>
0113     <span class="comment">%        Xmat = X.U*X.S*X.V';</span>
0114     <span class="comment">%        f = .5*cnormsqfro(P_full.*Xmat - PA_full);</span>
0115     <span class="comment">%    end</span>
0116     
0117     <span class="comment">% Computating the ehess for the set of fixed-rank matrices with</span>
0118     <span class="comment">% an embedded geometry is currently not supported.</span>
0119     <span class="comment">% Call manoptAD to automatically obtain the grad</span>
0120     <span class="comment">% problem = manoptAD(problem);</span>
0121 
0122 
0123     <span class="comment">% Check consistency of the gradient and the Hessian. Useful if you</span>
0124     <span class="comment">% adapt this example for a new cost function and you would like to make</span>
0125     <span class="comment">% sure there is no mistake.</span>
0126     <span class="comment">% warning('off', 'manopt:fixedrankembeddedfactory:exp');</span>
0127     <span class="comment">% checkgradient(problem); pause;</span>
0128     <span class="comment">% checkhessian(problem); pause;</span>
0129     
0130     
0131     
0132     
0133     
0134     <span class="comment">% Compute an initial guess. Points on the manifold are represented as</span>
0135     <span class="comment">% structures with three fields: U, S and V. U and V need to be</span>
0136     <span class="comment">% orthonormal, S needs to be diagonal.</span>
0137     [U, S, V] = svds(PA, k);
0138     X0.U = U;
0139     X0.S = S;
0140     X0.V = V;
0141     
0142     <span class="comment">% Minimize the cost function using Riemannian trust-regions, starting</span>
0143     <span class="comment">% from the initial guess X0.</span>
0144     X = <a href="../manopt/solvers/trustregions/trustregions.html" class="code" title="function [x, cost, info, options] = trustregions(problem, x, options)">trustregions</a>(problem, X0);
0145     
0146     <span class="comment">% The reconstructed matrix is X, represented as a structure with fields</span>
0147     <span class="comment">% U, S and V.</span>
0148     Xmat = X.U*X.S*X.V';
0149     fprintf(<span class="string">'||X-A||_F = %g\n'</span>, <a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/norm.html" class="code" title="function res = norm( x, safe )">norm</a>(Xmat - A, <span class="string">'fro'</span>));
0150     
0151     
0152     
0153     
0154     
0155     
0156     <span class="comment">% Alternatively, we could decide to use a solver such as</span>
0157     <span class="comment">% steepestdescent or conjugategradient. These solvers need to solve a</span>
0158     <span class="comment">% line-search problem at each iteration. Standard line searches in</span>
0159     <span class="comment">% Manopt have generic purpose systems to do this. But for the problem</span>
0160     <span class="comment">% at hand, it so happens that we can rather accurately guess how far</span>
0161     <span class="comment">% the line-search should look, and it would be a waste to not use that.</span>
0162     <span class="comment">% Look up the paper referenced above for the mathematical explanation</span>
0163     <span class="comment">% of the code below.</span>
0164     <span class="comment">%</span>
0165     <span class="comment">% To tell Manopt about this special information, we specify the</span>
0166     <span class="comment">% linesearch hint function in the problem structure. Notice that this</span>
0167     <span class="comment">% is not the same thing as specifying a linesearch function in the</span>
0168     <span class="comment">% options structure.</span>
0169     <span class="comment">%</span>
0170     <span class="comment">% Both the SD and the CG solvers will detect that we</span>
0171     <span class="comment">% specify the hint function below, and they will use an appropriate</span>
0172     <span class="comment">% linesearch algorithm by default, as a result. Typically, they will</span>
0173     <span class="comment">% try the step t*H first, then if it does not satisfy an Armijo</span>
0174     <span class="comment">% criterion, they will decrease t geometrically until satisfaction or</span>
0175     <span class="comment">% failure.</span>
0176     <span class="comment">%</span>
0177     <span class="comment">% Just like the cost, egrad and ehess functions, the linesearch</span>
0178     <span class="comment">% function could use a store structure if you like. The present code</span>
0179     <span class="comment">% does not use the store structure, which means quite a bit of the</span>
0180     <span class="comment">% computations are made redundantly, and as a result a better method</span>
0181     <span class="comment">% could appear slower. See the Manopt tutorial about caching when you</span>
0182     <span class="comment">% are ready to switch from a proof-of-concept code to an efficient</span>
0183     <span class="comment">% code.</span>
0184     <span class="comment">%</span>
0185     <span class="comment">% The inputs are X (a point on the manifold) and H, a tangent vector at</span>
0186     <span class="comment">% X that is assumed to be a descent direction. That is, there exists a</span>
0187     <span class="comment">% positive t such that f(Retraction_X(tH)) &lt; f(X). The function below</span>
0188     <span class="comment">% is supposed to output a &quot;t&quot; that it is a good &quot;guess&quot; at such a t.</span>
0189     problem.linesearch = @<a href="#_sub4" class="code" title="subfunction t = linesearch_helper(X, H)">linesearch_helper</a>;
0190     <a name="_sub4" href="#_subfunctions" class="code">function t = linesearch_helper(X, H)</a>
0191         <span class="comment">% Note that you would not usually need the Hessian for this.</span>
0192         residual_omega = nonzeros(problem.egrad(X));
0193         dir_omega      = nonzeros(problem.ehess(X, H));
0194         t = - dir_omega \ residual_omega ;
0195     <span class="keyword">end</span>
0196 
0197     <span class="comment">% Notice that for this solver, the Hessian is not needed.</span>
0198     [Xcg, xcost, info, options] = <a href="../manopt/solvers/conjugategradient/conjugategradient.html" class="code" title="function [x, cost, info, options] = conjugategradient(problem, x, options)">conjugategradient</a>(problem, X0); <span class="comment">%#ok&lt;ASGLU&gt;</span>
0199     
0200     fprintf(<span class="string">'Take a look at the options that CG used:\n'</span>);
0201     <a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/disp.html" class="code" title="function disp( x, name )">disp</a>(options);
0202     fprintf(<span class="string">'And see how many trials were made at each line search call:\n'</span>);
0203     info_ls = [info.linesearch];
0204     <a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/disp.html" class="code" title="function disp( x, name )">disp</a>([info_ls.costevals]);
0205 
0206     
0207     
0208     
0209     
0210     
0211     
0212     
0213     fprintf(<span class="string">'Try it again without the linesearch helper.\n'</span>);
0214     
0215     <span class="comment">% Remove the linesearch helper from the problem structure.</span>
0216     problem = rmfield(problem, <span class="string">'linesearch'</span>);
0217     
0218     [Xcg, xcost, info, options] = <a href="../manopt/solvers/conjugategradient/conjugategradient.html" class="code" title="function [x, cost, info, options] = conjugategradient(problem, x, options)">conjugategradient</a>(problem, X0); <span class="comment">%#ok&lt;ASGLU&gt;</span>
0219     
0220     fprintf(<span class="string">'Take a look at the options that CG used:\n'</span>);
0221     <a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/disp.html" class="code" title="function disp( x, name )">disp</a>(options);
0222     fprintf(<span class="string">'And see how many trials were made at each line search call:\n'</span>);
0223     info_ls = [info.linesearch];
0224     <a href="../manopt/manifolds/ttfixedrank/TTeMPS_1.1/@TTeMPS/disp.html" class="code" title="function disp( x, name )">disp</a>([info_ls.costevals]);
0225     
0226     
0227     
0228     
0229 
0230     <span class="comment">% If the problem has a small enough dimension, we may (for analysis</span>
0231     <span class="comment">% purposes) compute the spectrum of the Hessian at a point X. This may</span>
0232     <span class="comment">% help in studying the conditioning of a problem. If you don't provide</span>
0233     <span class="comment">% the Hessian, Manopt will approximate the Hessian with finite</span>
0234     <span class="comment">% differences of the gradient and try to estimate its &quot;spectrum&quot; (it's</span>
0235     <span class="comment">% not a proper linear operator). This can give some intuition, but</span>
0236     <span class="comment">% should not be relied upon.</span>
0237     <span class="keyword">if</span> exist(<span class="string">'OCTAVE_VERSION'</span>, <span class="string">'builtin'</span>) == 0
0238         hstgrm = @histogram;
0239     <span class="keyword">else</span>
0240         hstgrm = @hist;
0241     <span class="keyword">end</span>
0242     <span class="keyword">if</span> problem.M.dim() &lt; 2000
0243         fprintf(<span class="string">'Computing the spectrum of the Hessian...\n'</span>);
0244         s = <a href="../manopt/tools/hessianspectrum.html" class="code" title="function lambdas = hessianspectrum(problem, x, usepreconstr, storedb, key)">hessianspectrum</a>(problem, X);
0245         subplot(1, 2, 1);
0246         hstgrm(s);
0247         title(<span class="string">'Histogram of eigenvalues of the Hessian at the solution'</span>);
0248         subplot(1, 2, 2);
0249         hstgrm(log10(abs(s)));
0250         title(<span class="string">'Histogram of log_{10}(|eigenvalues|) of the Hessian at the solution'</span>);
0251     <span class="keyword">end</span>
0252     
0253     
0254     
0255     
0256 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Fri 30-Sep-2022 13:18:25 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>