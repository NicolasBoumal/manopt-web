{"title":"Solvers","markdown":{"yaml":{"title":"Solvers","subtitle":"Available optimization algorithms and how to use them","toc":true,"toc-depth":3,"toc-expand":2,"sidebar":"tutorial","aliases":["tutorial.html#solvers"]},"headingText":"General description ![](images/icon_salute.gif)","containsRefs":false,"markdown":"\n\n\nSolvers (that is, optimization algorithms) are functions in Manopt.\nBuilt-in solvers are located in `/manopt/solvers`.\nIn principle, all solvers accept the following basic call format:\n\n```matlab\nx = mysolver(problem)\n```\n\nThe returned value `x` is a point on the manifold `problem.M`. Depending on the properties of your problem and on the guarantees of the solver, `x` is more or less close to a good minimizer of the cost function described in the `problem` structure.\n\nBear in mind that we are dealing with usually nonconvex, and possibly nonsmooth or derivative-free optimization, so that it is in general not guaranteed that `x` is a global minimizer of the cost. For smooth problems with gradient information though, most decent algorithms guarantee that `x` is (approximately) a critical point. Typically, we even expect an approximate local minimizer, but even that is usually not guaranteed in all cases: this is a fundamental limitation of continuous optimization.\n\n::: {.callout-note}\n## Min or max?\nAll provided solvers are _minimization_ algorithms.\nIf you want to _maximize_ your objective function, multiply it by -1 (and accordingly for the derivatives of the objective function if needed), as we did in the [first example](firstexample.qmd).\n:::\n\nIn principle, all solvers also admit a more complete call format:\n\n```matlab\n[x, xcost, info, options] = mysolver(problem, x0, options)\n```\n\n**Inputs:**\n\n* The `problem` structure specifies the manifold and describes the cost function.\n* Optionally, `x0` is an initial guess, or initial iterate, for the solver. It is typically a point on the manifold `problem.M`, but may be something else depending on the solver. It can be omitted by passing the empty matrix `[]` instead.\n* Optionally, the `options` structure is used to fine tune the behavior of the optimization algorithm. On top of hosting the algorithmic parameters, it manages the stopping criteria as well as what information needs to be displayed and / or logged during execution.\n\n**Outputs:**\n\n* `x` is the returned point on the manifold.\n* `xcost` is the value of the cost function at `x`.\n* The `info` struct-array is described below, and contains information collected at each iteration of the solver's progress.\n* The `options` structure is returned too, so you can see what default values the solver used on top of the options you (possibly) specified.\n\n\n## Available solvers\n\nThe toolbox comes with a handful of solvers.\n\nThe most trust-worthy is the trust-regions algorithm.\nOriginally, it is a modification of the code of [GenRTR](https://www.math.fsu.edu/%7Ecbaker/GenRTR/ \"GenRTR, by Chris Baker, P.-A. Absil and Kyle Gallivan\").\n\nThe toolbox was designed to accommodate many more solvers though: we have since then added BFGS-style solvers, stochastic gradient descent and more.\nIn particular, we look forward to proposing algorithms for nonsmooth cost functions (which notably arise when L1 penalties are at play).\nYou can also propose your own solvers.\n\n| Name | Requires (benefits of) | Comment | Call |\n|-|-|-|-|\n| [Trust-regions (RTR)](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/trustregions/trustregions.m) | Cost, gradient (Hessian, approximate Hessian, preconditioner) | #1 choice for smooth optimization; uses finite differences (FD) of the gradient in the absence of Hessian. | `trustregions` |\n| [Adaptive regularization by cubics (ARC)](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/arc/arc.m)  | Cost, gradient (Hessian, approximate Hessian) | Alternative to RTR; uses FD of the gradient in the absence of Hessian. | `arc` |\n| [Steepest descent](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/steepestdescent/steepestdescent.m) | Cost, gradient | Gradient descent (GD) with line-search (default is backtracking-based). | `steepestdescent` |\n| [Conjugate gradient](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/conjugategradient/conjugategradient.m) | Cost, gradient (preconditioner) | Often performs better than steepest descent. | `conjugategradient` |\n| [Barzilai-Borwein](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/barzilaiborwein/barzilaiborwein.m) | Cost, gradient | Gradient descent with BB step size heuristic. | `barzilaiborwein` |\n| [BFGS](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/bfgs/rlbfgs.m) | Cost, gradient | Limited-memory version of BFGS. | `rlbfgs` |\n| [SGD](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/stochasticgradient/stochasticgradient.m) | Partial gradient (preconditioner) [Cost not needed] | Stochastic gradient algorithm for optimization of large sums. | `stochasticgradient` |\n| [Particle swarm (PSO)](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/pso/pso.m) | Cost | Derivative-free optimizer (DFO) based on a population of points. | `pso` |\n| [Nelder-Mead](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/neldermead/neldermead.m) | Cost | DFO based on a simplex; requires `M.pairmean`; limited to (very) low-dimensional problems. | `neldermead` |\n\n: Table of minimization algorithms available in Manopt. {.striped}\n\n\n## The options structure\n\nIn Manopt, _all_ options are optional.\nStandard options are assigned a default value at the toolbox level in [`/manopt/core/getGlobalDefaults.m`](https://github.com/NicolasBoumal/manopt/blob/master/manopt/core/getGlobalDefaults.m) (it's a core tool, best not to edit it).\nSolvers then overwrite and complement these options with solver-specific fields.\nThese options are in turn overwritten by the user-specified options, if any.\n\nThe subsections below list commonly used options.\nSee each solver's documentation for specific information (e.g., type `help trustregions`).\n\n<!--\nKeep in mind that a specific solver may not use all of these options.\nIt is also likely to use additional options, which would then be described in the help section of the solver's code.\n-->\n\n### Generic options\n\nThe following table lists options to control text output during solver execution, whether additional debugging checks are run, and a (deprecated) option to control memory usage from caching.\n\n| Field name (`options.\"...\"`)  | Value type  | Description  |\n|-|-|----|\n| `verbosity` | integer  | Controls how much information a solver outputs during execution; 0: no output; 1 : output at init and at exit; 2: light output at each iteration; 3+: all you can read. |\n| `debug` | integer  | If larger than 0, the solver may perform additional computations for debugging purposes.  |\n| `storedepth` | integer  | Maximum number of `store` structures that may be kept in memory (see the [cost description](costdescription.qmd) page). As of Manopt 5.0, this is mostly irrelevant because the main solvers do a much better job of discarding stale information on the go. |\n\n: Table of generic options used by most solvers.\n\n### Options for stopping criterion\n\nSolvers stop running when one of several stopping criteria triggers.\nThe following table lists options for some standard criteria.\n\n| Field name (`options.\"...\"`)  | Value type  | Description  |\n|-|-|----|\n| `maxiter` | integer  | Limits the number of iterations of the solver.  |\n| `maxtime` | double  | Limits the execution time of the solver, in seconds.  |\n| `tolcost` | double  | Stop as soon as the cost drops below this tolerance.  |\n| `tolgradnorm` | double  | Stop as soon as the norm of the gradient drops below this tolerance.  |\n| `stopfun` | fun. handle  | Custom stopping criterion: see below.  |\n\n: Table of generic stopping criterion options used by most solvers.\n\nSome solvers offer additional stopping criteria.\n\nUsers can also define **custom stopping criteria** with `options.stopfun`.\nThe general usage pattern is as follows:\n\n```matlab\n% define your problem structure, then:\noptions.stopfun = @mystopfun;\nx = mysolver(problem, [], options);\n\nfunction [stopnow, reason] = mystopfun(problem, x, info, last)\n   if xyz % decide if should stop based on inputs\n    stopnow = true;\n    reason = 'This optional message explains why we stopped.';\n  else\n    stopnow = false;\n    reason = '';\n  end\nend\n```\n\nThe function handle `options.stopfun` is called after each iteration completes.\nAs input, it receives:\n\n* the `problem` structure,\n* the current point `x`,\n* the whole `info` struct-array built so far, and\n* an index `last` such that `info(last)` is the structure pertaining to the current iteration (this is because `info` is pre-allocated, so that `info(end)` typically does _not_ refer to the current iteration).\n\nThe outputs are:\n\n* a Boolean `stopnow`: the solver terminates if this is true;\n* optionally, a string `reason` which explains why the criterion triggered. This is displayed by the solver if `options.verbosity > 0`.\n\nConsider the following example:\n\n```matlab\noptions.stopfun = @mystopfun;\nfunction stopnow = mystopfun(problem, x, info, last)\n    stopnow = (last >= 3 && info(last-2).cost - info(last).cost < 1e-3);\nend\n```\n\nThis tells the solver to exit as soon as two successive iterations combined have decreased the cost by less than $10^{-3}$.\n\n\nSee also the [tools](tools.qmd) page for a list of built-in ways to use `stopfun` to good effect.\nFor example:\n\n* Stop a solver on demand [by closing a figure or by deleting a file](tools.qmd#interactive-stopping-criteria).\nThese allow to gracefully interrupt a solver interactively without breaking program execution.\nThe second one is especially useful when running code remotely without GUI.\n* Stop a solver [using Manopt counters](tools.qmd#counters-to-track-computations). This makes it easy to stop after a certain budget of function calls, matrix-vector products etc. has been exceeded.\n\n\n### Statsfun option for recording info at each iteration\n\nThe function handle `options.statsfun` is called after each iteration completes.\nIt provides an opportunity to process information about each iteration.\nThe main purpose is to store information, but this option can also be used to print, save, plot, etc.\n\nHere is an example:\n\n```matlab\n% define your problem structure, then:\noptions.statsfun = @mystatsfun;\n[x, fx, info] = mysolver(problem, [], options);\n\nfunction stats = mystatsfun(problem, x, stats, store)\n    stats.current_point = x;\nend\n```\n\nThis logs all the points visited during the optimization process in the `info` struct-array returned by the solver.\nOne could also write `x` to disk during this call, or update a plot (if that is useful).\n\nAs input, `statsfun` receives:\n\n* the `problem` structure,\n* the current point `x`,\n* the `stats` structure for the current iteration as recorded in the `info` struct-array, and\n* (optionally) the `store` structure with the cache for `x` (and the common cache in `store.shared`).\n\nThe output is the `stats` structure.\nThis function gives you a chance to modify the `stats` structure, hence to add fields if you want to.\n\nBear in mind that structures in a struct-array must _all_ have the same fields: if `statsfun` adds a certain field to a `stats` structure, it must do so for _all_ iterations.\n(In some instances, it is even important that the fields be created in the same order.)\n\nTime spent in `statsfun` is discounted from execution time recorded in `[info.time]`, as this is typically only used for prototyping, debugging, plotting etc.\n\nIf you include `store` as an input for the `statsfun`, this lets you access the data you [cached](costdescription.qmd), both for that particular iterate.\nYou also get access to the cache in `store.shared`, common to _all_ points `x` visited by the solver so far.\nNote that `statsfun`can read but it cannot write to `store` (modifications are lost).\n\nAn alternative to creating a `statsfun` by hand is to use the [`statsfunhelper`](https://github.com/NicolasBoumal/manopt/blob/master/manopt/tools/statsfunhelper.m) tool.\nThis is sometimes simpler (and allows to pass inline functions).\nThe example above simplifies to:\n\n```matlab\noptions.statsfun = statsfunhelper('current_point', @(x) x);\n```\n\nThe helper can also be used to log more than one metric, by passing it a structure.\nIn the example below, `x_reference` is a certain point on the manifold `problem.M`.\nThe stats structures will include fields `current_point` and `dist_to_ref`.\nNotice how the function handles can take different inputs.\nSee the help of that tool for more info.\n\n```matlab\nmetrics.current_point = @(x) x;\nmetrics.dist_to_ref = @(problem, x) problem.M.dist(x, x_reference);\noptions.statsfun = statsfunhelper(metrics);\n```\n\nSee also [how to use Manopt counters](tools.qmd#counters-to-track-computations) to keep track of things such as the number of calls to cost, gradient and Hessian, or other special operations such as matrix-vector products.\nThese counters are registered at every iteration and available in the returned stats structure.\nThey can also be used in a stopping criterion.\n\n\n\n### Option to specify a line-search\n\nSome solvers, such as `steepestdescent` and `conjugategradient`, need to solve a line-search problem at each iteration. That is, they need to (approximately) solve the one-dimensional optimization problem:  \n$$\n  \\min_{t\\geq 0} \\phi(t) = f(\\Retr_x(td)),\n$$  \nwhere $x$ is the current point on the manifold, $d$ is a tangent vector at $x$ (the search direction), $\\Retr$ is the retraction on the manifold and $f$ is the cost function. Assuming $d$ is a descent direction, there exists $t > 0$ such that $\\phi(t) < \\phi(0) = f(x)$. The purpose of a line-search algorithm is to find such a real number $t$.\n\nManopt includes certain [generic purpose line-search algorithms](https://github.com/NicolasBoumal/manopt/tree/master/manopt/solvers/linesearch). To force the use of one of them or of your own, specify this in the options structure (not in the problem structure), for example as follows:\n\n```matlab\noptions.linesearch = @linesearch_adaptive;\n```\n\nEach line-search algorithm accepts its own options which can be added in this same `options` structure passed to the main solver.\nSee each line-search's help for details.\n\nFor certain problems, you may want to implement your own line-search, typically in order to exploit structure specific to the problem at hand.\nTo this end, it is best to start from an existing line-search function and to adapt it.\nAlternatively (and perhaps more easily), you may specify a `linesearch` function in the `problem` structure (see the [cost description](costdescription.qmd) page) and use a line-search that uses it, to incorporate the additional information you supply there.\nDo not hesitate to ask for help on the forum if you have questions on this feature.\n\n\n<!--\n::: {.callout-tip}\n## Need a problem-specific stopping criterion?\nInclude a `stopfun` in your `options` structure.\nSee above for details.\n:::\n-->\n\n\n## The info struct-array\n\nThe various solvers log information at each iteration about their progress.\nThis information is returned in the output `info`.\n\nThis is a struct-array, that is, an array of structures.\nRead this [MathWorks blog post](https://blogs.mathworks.com/loren/2007/04/19/vectorizing-access-to-an-array-of-structures/) for more on this data container in Matlab.\n\nFor example, to extract a vector containing the cost value at each iteration, call `[info.cost]` _with the brackets_.\nTo plot the cost function value against computation time, call\n\n```matlab\nplot([info.time], [info.cost]);\n```\n\nThe following table lists some indicators that might be present in the `info` output.\n\n| Field name (`[info.\"...\"]`) | Value type | Description |\n|-|-|---|\n| `iter` | integer | Iteration number (0 corresponds to the initial guess). |\n| `time` | double | Elapsed execution time until completion of the iterate, in seconds. |\n| `cost` | double | Attained value of the cost function. |\n| `gradnorm` | double | Attained value for the norm of the Riemannian gradient. |\n\n: Table of typical information tracked at each iteration and returned in the `info` struct-array. Specific solvers typically include additional fields: check out their documentation. {.striped}\n\nA specific solver may not populate all of these fields and may provide additional fields, which would then be described in the solver's documentation.\n\n::: {.callout-tip}\n## Need to log problem-specific information at each iteration?\nInclude a `statsfun` in your `options` structure: [read more here](solvers.qmd#statsfun-option-for-recording-info-at-each-iteration).\nAlso read about [Manopt counters](tools.qmd#counters-to-track-computations) to keep track of things such as function calls, Hessian calls, matrix products, etc.\n:::\n\n::: {.callout-important}\n## About execution time recorded in `info.time`\nThe execution time is logged _without_ incorporating time spent in `statsfun`, as it usually performs computations that are not needed to solve the optimization problem.\nIf, however, you use information logged by `statsfun` for your `stopfun` criterion, and if this is important for your method (i.e., it is not just for convenience during prototyping), you should time the execution time of `statsfun` and add it to the `stats.time` field.\n:::\n","srcMarkdownNoYaml":"\n\n## General description ![](images/icon_salute.gif)\n\nSolvers (that is, optimization algorithms) are functions in Manopt.\nBuilt-in solvers are located in `/manopt/solvers`.\nIn principle, all solvers accept the following basic call format:\n\n```matlab\nx = mysolver(problem)\n```\n\nThe returned value `x` is a point on the manifold `problem.M`. Depending on the properties of your problem and on the guarantees of the solver, `x` is more or less close to a good minimizer of the cost function described in the `problem` structure.\n\nBear in mind that we are dealing with usually nonconvex, and possibly nonsmooth or derivative-free optimization, so that it is in general not guaranteed that `x` is a global minimizer of the cost. For smooth problems with gradient information though, most decent algorithms guarantee that `x` is (approximately) a critical point. Typically, we even expect an approximate local minimizer, but even that is usually not guaranteed in all cases: this is a fundamental limitation of continuous optimization.\n\n::: {.callout-note}\n## Min or max?\nAll provided solvers are _minimization_ algorithms.\nIf you want to _maximize_ your objective function, multiply it by -1 (and accordingly for the derivatives of the objective function if needed), as we did in the [first example](firstexample.qmd).\n:::\n\nIn principle, all solvers also admit a more complete call format:\n\n```matlab\n[x, xcost, info, options] = mysolver(problem, x0, options)\n```\n\n**Inputs:**\n\n* The `problem` structure specifies the manifold and describes the cost function.\n* Optionally, `x0` is an initial guess, or initial iterate, for the solver. It is typically a point on the manifold `problem.M`, but may be something else depending on the solver. It can be omitted by passing the empty matrix `[]` instead.\n* Optionally, the `options` structure is used to fine tune the behavior of the optimization algorithm. On top of hosting the algorithmic parameters, it manages the stopping criteria as well as what information needs to be displayed and / or logged during execution.\n\n**Outputs:**\n\n* `x` is the returned point on the manifold.\n* `xcost` is the value of the cost function at `x`.\n* The `info` struct-array is described below, and contains information collected at each iteration of the solver's progress.\n* The `options` structure is returned too, so you can see what default values the solver used on top of the options you (possibly) specified.\n\n\n## Available solvers\n\nThe toolbox comes with a handful of solvers.\n\nThe most trust-worthy is the trust-regions algorithm.\nOriginally, it is a modification of the code of [GenRTR](https://www.math.fsu.edu/%7Ecbaker/GenRTR/ \"GenRTR, by Chris Baker, P.-A. Absil and Kyle Gallivan\").\n\nThe toolbox was designed to accommodate many more solvers though: we have since then added BFGS-style solvers, stochastic gradient descent and more.\nIn particular, we look forward to proposing algorithms for nonsmooth cost functions (which notably arise when L1 penalties are at play).\nYou can also propose your own solvers.\n\n| Name | Requires (benefits of) | Comment | Call |\n|-|-|-|-|\n| [Trust-regions (RTR)](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/trustregions/trustregions.m) | Cost, gradient (Hessian, approximate Hessian, preconditioner) | #1 choice for smooth optimization; uses finite differences (FD) of the gradient in the absence of Hessian. | `trustregions` |\n| [Adaptive regularization by cubics (ARC)](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/arc/arc.m)  | Cost, gradient (Hessian, approximate Hessian) | Alternative to RTR; uses FD of the gradient in the absence of Hessian. | `arc` |\n| [Steepest descent](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/steepestdescent/steepestdescent.m) | Cost, gradient | Gradient descent (GD) with line-search (default is backtracking-based). | `steepestdescent` |\n| [Conjugate gradient](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/conjugategradient/conjugategradient.m) | Cost, gradient (preconditioner) | Often performs better than steepest descent. | `conjugategradient` |\n| [Barzilai-Borwein](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/barzilaiborwein/barzilaiborwein.m) | Cost, gradient | Gradient descent with BB step size heuristic. | `barzilaiborwein` |\n| [BFGS](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/bfgs/rlbfgs.m) | Cost, gradient | Limited-memory version of BFGS. | `rlbfgs` |\n| [SGD](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/stochasticgradient/stochasticgradient.m) | Partial gradient (preconditioner) [Cost not needed] | Stochastic gradient algorithm for optimization of large sums. | `stochasticgradient` |\n| [Particle swarm (PSO)](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/pso/pso.m) | Cost | Derivative-free optimizer (DFO) based on a population of points. | `pso` |\n| [Nelder-Mead](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/neldermead/neldermead.m) | Cost | DFO based on a simplex; requires `M.pairmean`; limited to (very) low-dimensional problems. | `neldermead` |\n\n: Table of minimization algorithms available in Manopt. {.striped}\n\n\n## The options structure\n\nIn Manopt, _all_ options are optional.\nStandard options are assigned a default value at the toolbox level in [`/manopt/core/getGlobalDefaults.m`](https://github.com/NicolasBoumal/manopt/blob/master/manopt/core/getGlobalDefaults.m) (it's a core tool, best not to edit it).\nSolvers then overwrite and complement these options with solver-specific fields.\nThese options are in turn overwritten by the user-specified options, if any.\n\nThe subsections below list commonly used options.\nSee each solver's documentation for specific information (e.g., type `help trustregions`).\n\n<!--\nKeep in mind that a specific solver may not use all of these options.\nIt is also likely to use additional options, which would then be described in the help section of the solver's code.\n-->\n\n### Generic options\n\nThe following table lists options to control text output during solver execution, whether additional debugging checks are run, and a (deprecated) option to control memory usage from caching.\n\n| Field name (`options.\"...\"`)  | Value type  | Description  |\n|-|-|----|\n| `verbosity` | integer  | Controls how much information a solver outputs during execution; 0: no output; 1 : output at init and at exit; 2: light output at each iteration; 3+: all you can read. |\n| `debug` | integer  | If larger than 0, the solver may perform additional computations for debugging purposes.  |\n| `storedepth` | integer  | Maximum number of `store` structures that may be kept in memory (see the [cost description](costdescription.qmd) page). As of Manopt 5.0, this is mostly irrelevant because the main solvers do a much better job of discarding stale information on the go. |\n\n: Table of generic options used by most solvers.\n\n### Options for stopping criterion\n\nSolvers stop running when one of several stopping criteria triggers.\nThe following table lists options for some standard criteria.\n\n| Field name (`options.\"...\"`)  | Value type  | Description  |\n|-|-|----|\n| `maxiter` | integer  | Limits the number of iterations of the solver.  |\n| `maxtime` | double  | Limits the execution time of the solver, in seconds.  |\n| `tolcost` | double  | Stop as soon as the cost drops below this tolerance.  |\n| `tolgradnorm` | double  | Stop as soon as the norm of the gradient drops below this tolerance.  |\n| `stopfun` | fun. handle  | Custom stopping criterion: see below.  |\n\n: Table of generic stopping criterion options used by most solvers.\n\nSome solvers offer additional stopping criteria.\n\nUsers can also define **custom stopping criteria** with `options.stopfun`.\nThe general usage pattern is as follows:\n\n```matlab\n% define your problem structure, then:\noptions.stopfun = @mystopfun;\nx = mysolver(problem, [], options);\n\nfunction [stopnow, reason] = mystopfun(problem, x, info, last)\n   if xyz % decide if should stop based on inputs\n    stopnow = true;\n    reason = 'This optional message explains why we stopped.';\n  else\n    stopnow = false;\n    reason = '';\n  end\nend\n```\n\nThe function handle `options.stopfun` is called after each iteration completes.\nAs input, it receives:\n\n* the `problem` structure,\n* the current point `x`,\n* the whole `info` struct-array built so far, and\n* an index `last` such that `info(last)` is the structure pertaining to the current iteration (this is because `info` is pre-allocated, so that `info(end)` typically does _not_ refer to the current iteration).\n\nThe outputs are:\n\n* a Boolean `stopnow`: the solver terminates if this is true;\n* optionally, a string `reason` which explains why the criterion triggered. This is displayed by the solver if `options.verbosity > 0`.\n\nConsider the following example:\n\n```matlab\noptions.stopfun = @mystopfun;\nfunction stopnow = mystopfun(problem, x, info, last)\n    stopnow = (last >= 3 && info(last-2).cost - info(last).cost < 1e-3);\nend\n```\n\nThis tells the solver to exit as soon as two successive iterations combined have decreased the cost by less than $10^{-3}$.\n\n\nSee also the [tools](tools.qmd) page for a list of built-in ways to use `stopfun` to good effect.\nFor example:\n\n* Stop a solver on demand [by closing a figure or by deleting a file](tools.qmd#interactive-stopping-criteria).\nThese allow to gracefully interrupt a solver interactively without breaking program execution.\nThe second one is especially useful when running code remotely without GUI.\n* Stop a solver [using Manopt counters](tools.qmd#counters-to-track-computations). This makes it easy to stop after a certain budget of function calls, matrix-vector products etc. has been exceeded.\n\n\n### Statsfun option for recording info at each iteration\n\nThe function handle `options.statsfun` is called after each iteration completes.\nIt provides an opportunity to process information about each iteration.\nThe main purpose is to store information, but this option can also be used to print, save, plot, etc.\n\nHere is an example:\n\n```matlab\n% define your problem structure, then:\noptions.statsfun = @mystatsfun;\n[x, fx, info] = mysolver(problem, [], options);\n\nfunction stats = mystatsfun(problem, x, stats, store)\n    stats.current_point = x;\nend\n```\n\nThis logs all the points visited during the optimization process in the `info` struct-array returned by the solver.\nOne could also write `x` to disk during this call, or update a plot (if that is useful).\n\nAs input, `statsfun` receives:\n\n* the `problem` structure,\n* the current point `x`,\n* the `stats` structure for the current iteration as recorded in the `info` struct-array, and\n* (optionally) the `store` structure with the cache for `x` (and the common cache in `store.shared`).\n\nThe output is the `stats` structure.\nThis function gives you a chance to modify the `stats` structure, hence to add fields if you want to.\n\nBear in mind that structures in a struct-array must _all_ have the same fields: if `statsfun` adds a certain field to a `stats` structure, it must do so for _all_ iterations.\n(In some instances, it is even important that the fields be created in the same order.)\n\nTime spent in `statsfun` is discounted from execution time recorded in `[info.time]`, as this is typically only used for prototyping, debugging, plotting etc.\n\nIf you include `store` as an input for the `statsfun`, this lets you access the data you [cached](costdescription.qmd), both for that particular iterate.\nYou also get access to the cache in `store.shared`, common to _all_ points `x` visited by the solver so far.\nNote that `statsfun`can read but it cannot write to `store` (modifications are lost).\n\nAn alternative to creating a `statsfun` by hand is to use the [`statsfunhelper`](https://github.com/NicolasBoumal/manopt/blob/master/manopt/tools/statsfunhelper.m) tool.\nThis is sometimes simpler (and allows to pass inline functions).\nThe example above simplifies to:\n\n```matlab\noptions.statsfun = statsfunhelper('current_point', @(x) x);\n```\n\nThe helper can also be used to log more than one metric, by passing it a structure.\nIn the example below, `x_reference` is a certain point on the manifold `problem.M`.\nThe stats structures will include fields `current_point` and `dist_to_ref`.\nNotice how the function handles can take different inputs.\nSee the help of that tool for more info.\n\n```matlab\nmetrics.current_point = @(x) x;\nmetrics.dist_to_ref = @(problem, x) problem.M.dist(x, x_reference);\noptions.statsfun = statsfunhelper(metrics);\n```\n\nSee also [how to use Manopt counters](tools.qmd#counters-to-track-computations) to keep track of things such as the number of calls to cost, gradient and Hessian, or other special operations such as matrix-vector products.\nThese counters are registered at every iteration and available in the returned stats structure.\nThey can also be used in a stopping criterion.\n\n\n\n### Option to specify a line-search\n\nSome solvers, such as `steepestdescent` and `conjugategradient`, need to solve a line-search problem at each iteration. That is, they need to (approximately) solve the one-dimensional optimization problem:  \n$$\n  \\min_{t\\geq 0} \\phi(t) = f(\\Retr_x(td)),\n$$  \nwhere $x$ is the current point on the manifold, $d$ is a tangent vector at $x$ (the search direction), $\\Retr$ is the retraction on the manifold and $f$ is the cost function. Assuming $d$ is a descent direction, there exists $t > 0$ such that $\\phi(t) < \\phi(0) = f(x)$. The purpose of a line-search algorithm is to find such a real number $t$.\n\nManopt includes certain [generic purpose line-search algorithms](https://github.com/NicolasBoumal/manopt/tree/master/manopt/solvers/linesearch). To force the use of one of them or of your own, specify this in the options structure (not in the problem structure), for example as follows:\n\n```matlab\noptions.linesearch = @linesearch_adaptive;\n```\n\nEach line-search algorithm accepts its own options which can be added in this same `options` structure passed to the main solver.\nSee each line-search's help for details.\n\nFor certain problems, you may want to implement your own line-search, typically in order to exploit structure specific to the problem at hand.\nTo this end, it is best to start from an existing line-search function and to adapt it.\nAlternatively (and perhaps more easily), you may specify a `linesearch` function in the `problem` structure (see the [cost description](costdescription.qmd) page) and use a line-search that uses it, to incorporate the additional information you supply there.\nDo not hesitate to ask for help on the forum if you have questions on this feature.\n\n\n<!--\n::: {.callout-tip}\n## Need a problem-specific stopping criterion?\nInclude a `stopfun` in your `options` structure.\nSee above for details.\n:::\n-->\n\n\n## The info struct-array\n\nThe various solvers log information at each iteration about their progress.\nThis information is returned in the output `info`.\n\nThis is a struct-array, that is, an array of structures.\nRead this [MathWorks blog post](https://blogs.mathworks.com/loren/2007/04/19/vectorizing-access-to-an-array-of-structures/) for more on this data container in Matlab.\n\nFor example, to extract a vector containing the cost value at each iteration, call `[info.cost]` _with the brackets_.\nTo plot the cost function value against computation time, call\n\n```matlab\nplot([info.time], [info.cost]);\n```\n\nThe following table lists some indicators that might be present in the `info` output.\n\n| Field name (`[info.\"...\"]`) | Value type | Description |\n|-|-|---|\n| `iter` | integer | Iteration number (0 corresponds to the initial guess). |\n| `time` | double | Elapsed execution time until completion of the iterate, in seconds. |\n| `cost` | double | Attained value of the cost function. |\n| `gradnorm` | double | Attained value for the norm of the Riemannian gradient. |\n\n: Table of typical information tracked at each iteration and returned in the `info` struct-array. Specific solvers typically include additional fields: check out their documentation. {.striped}\n\nA specific solver may not populate all of these fields and may provide additional fields, which would then be described in the solver's documentation.\n\n::: {.callout-tip}\n## Need to log problem-specific information at each iteration?\nInclude a `statsfun` in your `options` structure: [read more here](solvers.qmd#statsfun-option-for-recording-info-at-each-iteration).\nAlso read about [Manopt counters](tools.qmd#counters-to-track-computations) to keep track of things such as function calls, Hessian calls, matrix products, etc.\n:::\n\n::: {.callout-important}\n## About execution time recorded in `info.time`\nThe execution time is logged _without_ incorporating time spent in `statsfun`, as it usually performs computations that are not needed to solve the optimization problem.\nIf, however, you use information logged by `statsfun` for your `stopfun` criterion, and if this is important for your method (i.e., it is not just for convenience during prototyping), you should time the execution time of `statsfun` and add it to the `stats.time` field.\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":{"file":"_header.html"},"highlight-style":"pygments","css":["styles.css"],"toc":true,"html-math-method":"mathjax","strip-comments":true,"toc-depth":3,"output-file":"solvers.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","crossref":{"eq-prefix":"Eq."},"theme":{"dark":["darkly","custom_darkly.scss"],"light":"sandstone"},"grid":{"sidebar-width":"230px","body-width":"800px","margin-width":"270px","gutter-width":"1.5rem"},"anchor-sections":true,"smooth-scroll":true,"title":"Solvers","subtitle":"Available optimization algorithms and how to use them","toc-expand":2,"sidebar":"tutorial","aliases":["tutorial.html#solvers"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}