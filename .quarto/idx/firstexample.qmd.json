{"title":"A first example","markdown":{"yaml":{"title":"A first example","toc":false,"sidebar":"tutorial","aliases":["tutorial.html#firstexample"]},"headingText":"The math","containsRefs":false,"markdown":"\n\nLet us first illustrate the most commonly useful features of Manopt using a standard optimization problem on a sphere.\n\n\nLet $A \\in \\mathbb{R}^{n\\times n}$ be a symmetric matrix.\nThe eigenvectors of $A$ associated to its largest eigenvalue [are known to be](https://en.wikipedia.org/wiki/Rayleigh_quotient) the optimizers for the following optimization problem:\n\n$$\\max\\limits_{x\\in\\mathbb{R}^n, x \\neq 0} \\frac{x^\\top A x}{x^\\top x}.$$\n\nThe cost function is insensitive to the norm of $x$, hence we might as well fix it to $x^\\top x = \\|x\\|^2 = 1$.\nAlso, we would like to have a minimization problem rather than a maximization problem (we'll see why in a moment).\nAccordingly, we flip the sign of the cost function.\nIn the end, we want to solve:\n\n$$\\min\\limits_{x\\in\\mathbb{R}^n, \\|x\\| = 1} -x^\\top A x.$$\n\nThe cost function and its gradient in $\\mathbb{R}^n$ are:\n\n$$  \n\\begin{align*}\n    f(x) = -x^\\top A x, &&\n    \\nabla f(x) = -2Ax.\n\\end{align*}\n$$\n\n<!-- Make this a link to the learning page and have a section there? -->\n(See the [matrix calculus](https://www.matrixcalculus.org/) website for help with figuring out gradients.\nYou can also learn the math to do these computations in Section 4.7 of [this book](https://www.nicolasboumal.net/book).)\n\nThe constraint on the vector $x$ tells us that $x$ is a point on the sphere (one of the nicest manifolds):\n\n$$\\mathbb{S}^{n-1} = \\{x \\in \\mathbb{R}^n : x^\\top x = 1\\}.$$\n\nThis is all the information we need to apply Manopt to our problem.\nFor additional theory, see the [cost function](costdescription.qmd) page and the [learning](learning.qmd) page.\n\n<!--\nUsers interested in how optimization on manifolds works will be interested in the following too: the cost function is smooth on $\\mathbb{S}^{n-1}$. Its Riemannian gradient on $\\mathbb{S}^{n-1}$ at $x$ is a tangent vector to the sphere at $x$. It can be computed as the projection from the usual gradient $\\nabla f(x)$ to that tangent space using the orthogonal projector $\\mathrm{Proj}_x u = (I-xx^\\top)u$:\n\n$$\\mathrm{grad}\\,f(x) = \\mathrm{Proj}_x \\nabla f(x) = -2(I-xx^\\top)Ax.$$\n\nThis is an example of a mathematical relationship between the Euclidean gradient $\\nabla f$, which we often already know how to compute from calculus courses, and the Riemannian gradient $\\mathrm{grad}\\,f$, which is needed for the optimization. Fortunately, in Manopt the conversion happens behind the scenes via a function called `egrad2rgrad` and we only need to compute $\\nabla f$.\n\nWe solve this simple optimization problem using Manopt to illustrate the most basic usage of the toolbox. \n-->\n\n## The code\n\nSolving this optimization problem using Manopt requires a few lines of Matlab code.\nHere they are, and explanations follow.\n\n```matlab\n% Generate random problem data.\nn = 1000;\nA = randn(n);\nA = .5*(A+A');\n\n% Create the problem structure.\nmanifold = spherefactory(n);\nproblem.M = manifold;\n\n% Define the problem cost function and its Euclidean gradient.\nproblem.cost  = @(x) -x'*(A*x);\nproblem.egrad = @(x) -2*A*x;      % notice the 'e' in 'egrad' for Euclidean\n\n% Numerically check gradient consistency (just once, optional).\ncheckgradient(problem); pause;\n\n% Solve.\n[x, xcost, info, options] = trustregions(problem);\n\n% Display some statistics.\nfigure;\nsemilogy([info.iter], [info.gradnorm], '.-');\nxlabel('Iteration number');\nylabel('Norm of the Riemannian gradient of f');\n```\n\nLet us look at the code bit by bit.\nFirst, we generate some data for our problem.\nThen, we execute these two lines:\n\n```matlab\nmanifold = spherefactory(n);\nproblem.M = manifold;\n```\n\nThe call to [spherefactory](https://github.com/NicolasBoumal/manopt/tree/master/manopt/manifolds/sphere) returns a structure describing the manifold $\\mathbb{S}^{n-1}$, i.e., the sphere.\nThis manifold corresponds to the constraint appearing in our optimization problem.\nFor other constraints, take a look at the [various supported manifolds](manifolds.qmd).\n\nThe second instruction creates a structure named `problem` and sets the field `problem.M` to contain the manifold structure.\n\nThe `problem` structure is further populated with everything a solver might need to know about the problem in order to solve it, such as the cost function and its gradient:\n\n```matlab\nproblem.cost = @(x) -x'*(A*x);\nproblem.egrad = @(x) -2*A*x;\n```\n\nThe cost function and its derivatives are specified as [function handles](https://ch.mathworks.com/help/matlab/ref/function_handle.html).\nNotice how the gradient was specified as the _Euclidean_ gradient of $f$, i.e., $\\nabla f(x) = -2Ax$ in the function `egrad` (mind the `e`).\nThe conversion to the Riemannian gradient happens automatically behind the scene.\nThis is especially useful with more complicated manifolds.\n\n<!--\nAn alternative to the definition of the gradient is to specify the Riemannian gradient directly, possibly calling the conversion tool `problem.M.egrad2rgrad` explicitly.\nThis would replace the line `problem.egrad = ...` with:\n\n```matlab\nproblem.grad = @(x) manifold.egrad2rgrad(x, -2*A*x);\n```\n\nThis is useful if an expression for the Riemannian gradient is known for example, and it is natural to use that explicitly.\nMind the names: `problem.grad` is to specify the _Riemannian_ gradient.\nIf you want to specify the _Euclidean_ gradient, the correct name is `problem.egrad`, with an \"e\".\nFor day to day use, `egrad` is often the preferred way to go.\n-->\n\nWe could also define the cost and its derivatives in several other ways, e.g., to avoid the redundant computation of the product `A*x`: see the [cost description](costdescription.qmd) page.\nIn particular, with Manopt 7.0 and Matlab R2021a or later, if you have the Deep Learning Toolbox, then you can also use **automatic differentiation** (AD) instead of defining the gradient (and even the Hessian) yourself:\n\n```matlab\nproblem = manoptAD(problem);\n```\n\nSee the [cost description](costdescription.qmd) page for more information about how this works, and about what to do when it does not work.\nKeep in mind that, while AD is convenient and reasonably efficient, it is usually slower than (good) hand-written code.\n<!--Indeed, it is typical for AD to slow-down optimization by about a factor of five.\nStill, for prototyping, this is often a comfortable option.-->\n\n<!--\n::: {.callout-tip}\n## Avoiding redundant computations\nNotice that the functions `cost` and `egrad` both compute the product $Ax$, which is likely to be the most expensive operation for large scale problems.\nThis is perfectly fine for prototyping, but less so for a final version of the implementation.\nSee the many ways of [describing the cost function](costdescription.qmd) for alternatives that reduce redundant computations.\n:::\n-->\n\nThe next instruction is not needed to solve the problem but often helps at the prototyping stage:\n\n```matlab\ncheckgradient(problem);\n```\n\nThe [checkgradient](https://github.com/NicolasBoumal/manopt/blob/master/manopt/tools/checkgradient.m) tool verifies numerically that the cost function and its gradient agree up to the appropriate order.\nSee the [tools page](tools.qmd) for further details and more helpful tools offered by Manopt.\nThis tool generates the following figure:\n\n![The `checkgradient` tool outputs a figure and text in the command line. In the figure here, we see that part of the blue curve has a slope that matches that of the dashed line: that's what we like to see. If not, then it is likely that the gradient is incorrectly implemented.](images/tutorial-gradientcheck.png){width=80%}\n\nThe blue curve seems to have the same slope as the dashed line over a decent segment (highlighted in orange): that's what we want to see.\nAlso check the text output in the command prompt.\n\nWe now run one of [the solvers](solvers.qmd) on our problem:\n\n```matlab\n[x, xcost, info, options] = trustregions(problem);\n```\n\nThis instruction calls [`trustregions`](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/trustregions/trustregions.m), without initial guess and without options structure.\nAs a result, the solver generates a random initial guess automatically and resorts to the default values for all options.\nAs a general feature in Manopt, all options are, well, optional.\n\n:::{.callout-note}\n## Manopt always minimizes\nAll solvers in Manopt aim to _minimize_ the cost function in the problem structure.\nIf you want to _maximize_ a function, do as we did earlier on this page: flip the sign of $f$ (and accordingly for its derivatives).\n:::\n\nThe returned values are:\n\n* `x`: usually an approximate local minimizer of the cost function,\n* `xcost`: the value of $f$ at `x`,\n* `info`: a struct-array containing information about the successive iterations performed by the solver, and\n* `options`: a structure containing the options used and their values: peek inside to find out what you can parameterize.\n\nFor more details and more solvers, see the [solvers](solvers.qmd) page.\n\nThis call issues a warning:\n\n> `Warning: No Hessian provided. Using FD approximation.`\n\nThat is because the trust-regions algorithm normally requires the Hessian of the cost function to be provided in the problem structure.\nWhen the Hessian is not provided, Manopt approximates it using finite differences  on the gradient and it warns you about it.\nYou may disable this warning by calling `warning('off', 'manopt:getHessian:approx');`.\n\nFinally, we access the contents of the struct-array `info` to display a convergence plot:\n\n```matlab\nsemilogy([info.iter], [info.gradnorm], '.-');  \nxlabel('Iteration number');  \nylabel('Norm of the gradient of f');\n```\n\nThis generates the following figure.\nFor more information on what data is stored in `info`, look inside and see the [solvers](solvers.qmd) page.\n\n![The Riemannian gradient norm is converging to zero fast: that's what we like to see.](images/tutorial-gradientnorm.png){width=80%}\n\n\n::: {.callout-note}\n## `info` is a struct-array\nNotice that we write `[info.xyz]` with brackets, and not simply `info.xyz`.\nThis is because `info` is a struct-array.\nRead this [MathWorks blog post](https://blogs.mathworks.com/loren/2007/04/19/vectorizing-access-to-an-array-of-structures/) for further information.\n:::\n\n\n::: {.callout-warning}\n## Fallback to finite differences is automatic\nIf you do not specify the gradient, then Manopt approximates it with finite differences.\nThis is convenient for prototyping on low-dimensional manifolds, but it is slow and can make it difficult to reach accurate solutions.\nAs in that case the solver may have a hard time reaching points with a small gradient, you can pass an options structure to the solver with `options.tolgradnorm` set to a larger value to allow it to stop earlier.\n:::\n\n","srcMarkdownNoYaml":"\n\nLet us first illustrate the most commonly useful features of Manopt using a standard optimization problem on a sphere.\n\n## The math\n\nLet $A \\in \\mathbb{R}^{n\\times n}$ be a symmetric matrix.\nThe eigenvectors of $A$ associated to its largest eigenvalue [are known to be](https://en.wikipedia.org/wiki/Rayleigh_quotient) the optimizers for the following optimization problem:\n\n$$\\max\\limits_{x\\in\\mathbb{R}^n, x \\neq 0} \\frac{x^\\top A x}{x^\\top x}.$$\n\nThe cost function is insensitive to the norm of $x$, hence we might as well fix it to $x^\\top x = \\|x\\|^2 = 1$.\nAlso, we would like to have a minimization problem rather than a maximization problem (we'll see why in a moment).\nAccordingly, we flip the sign of the cost function.\nIn the end, we want to solve:\n\n$$\\min\\limits_{x\\in\\mathbb{R}^n, \\|x\\| = 1} -x^\\top A x.$$\n\nThe cost function and its gradient in $\\mathbb{R}^n$ are:\n\n$$  \n\\begin{align*}\n    f(x) = -x^\\top A x, &&\n    \\nabla f(x) = -2Ax.\n\\end{align*}\n$$\n\n<!-- Make this a link to the learning page and have a section there? -->\n(See the [matrix calculus](https://www.matrixcalculus.org/) website for help with figuring out gradients.\nYou can also learn the math to do these computations in Section 4.7 of [this book](https://www.nicolasboumal.net/book).)\n\nThe constraint on the vector $x$ tells us that $x$ is a point on the sphere (one of the nicest manifolds):\n\n$$\\mathbb{S}^{n-1} = \\{x \\in \\mathbb{R}^n : x^\\top x = 1\\}.$$\n\nThis is all the information we need to apply Manopt to our problem.\nFor additional theory, see the [cost function](costdescription.qmd) page and the [learning](learning.qmd) page.\n\n<!--\nUsers interested in how optimization on manifolds works will be interested in the following too: the cost function is smooth on $\\mathbb{S}^{n-1}$. Its Riemannian gradient on $\\mathbb{S}^{n-1}$ at $x$ is a tangent vector to the sphere at $x$. It can be computed as the projection from the usual gradient $\\nabla f(x)$ to that tangent space using the orthogonal projector $\\mathrm{Proj}_x u = (I-xx^\\top)u$:\n\n$$\\mathrm{grad}\\,f(x) = \\mathrm{Proj}_x \\nabla f(x) = -2(I-xx^\\top)Ax.$$\n\nThis is an example of a mathematical relationship between the Euclidean gradient $\\nabla f$, which we often already know how to compute from calculus courses, and the Riemannian gradient $\\mathrm{grad}\\,f$, which is needed for the optimization. Fortunately, in Manopt the conversion happens behind the scenes via a function called `egrad2rgrad` and we only need to compute $\\nabla f$.\n\nWe solve this simple optimization problem using Manopt to illustrate the most basic usage of the toolbox. \n-->\n\n## The code\n\nSolving this optimization problem using Manopt requires a few lines of Matlab code.\nHere they are, and explanations follow.\n\n```matlab\n% Generate random problem data.\nn = 1000;\nA = randn(n);\nA = .5*(A+A');\n\n% Create the problem structure.\nmanifold = spherefactory(n);\nproblem.M = manifold;\n\n% Define the problem cost function and its Euclidean gradient.\nproblem.cost  = @(x) -x'*(A*x);\nproblem.egrad = @(x) -2*A*x;      % notice the 'e' in 'egrad' for Euclidean\n\n% Numerically check gradient consistency (just once, optional).\ncheckgradient(problem); pause;\n\n% Solve.\n[x, xcost, info, options] = trustregions(problem);\n\n% Display some statistics.\nfigure;\nsemilogy([info.iter], [info.gradnorm], '.-');\nxlabel('Iteration number');\nylabel('Norm of the Riemannian gradient of f');\n```\n\nLet us look at the code bit by bit.\nFirst, we generate some data for our problem.\nThen, we execute these two lines:\n\n```matlab\nmanifold = spherefactory(n);\nproblem.M = manifold;\n```\n\nThe call to [spherefactory](https://github.com/NicolasBoumal/manopt/tree/master/manopt/manifolds/sphere) returns a structure describing the manifold $\\mathbb{S}^{n-1}$, i.e., the sphere.\nThis manifold corresponds to the constraint appearing in our optimization problem.\nFor other constraints, take a look at the [various supported manifolds](manifolds.qmd).\n\nThe second instruction creates a structure named `problem` and sets the field `problem.M` to contain the manifold structure.\n\nThe `problem` structure is further populated with everything a solver might need to know about the problem in order to solve it, such as the cost function and its gradient:\n\n```matlab\nproblem.cost = @(x) -x'*(A*x);\nproblem.egrad = @(x) -2*A*x;\n```\n\nThe cost function and its derivatives are specified as [function handles](https://ch.mathworks.com/help/matlab/ref/function_handle.html).\nNotice how the gradient was specified as the _Euclidean_ gradient of $f$, i.e., $\\nabla f(x) = -2Ax$ in the function `egrad` (mind the `e`).\nThe conversion to the Riemannian gradient happens automatically behind the scene.\nThis is especially useful with more complicated manifolds.\n\n<!--\nAn alternative to the definition of the gradient is to specify the Riemannian gradient directly, possibly calling the conversion tool `problem.M.egrad2rgrad` explicitly.\nThis would replace the line `problem.egrad = ...` with:\n\n```matlab\nproblem.grad = @(x) manifold.egrad2rgrad(x, -2*A*x);\n```\n\nThis is useful if an expression for the Riemannian gradient is known for example, and it is natural to use that explicitly.\nMind the names: `problem.grad` is to specify the _Riemannian_ gradient.\nIf you want to specify the _Euclidean_ gradient, the correct name is `problem.egrad`, with an \"e\".\nFor day to day use, `egrad` is often the preferred way to go.\n-->\n\nWe could also define the cost and its derivatives in several other ways, e.g., to avoid the redundant computation of the product `A*x`: see the [cost description](costdescription.qmd) page.\nIn particular, with Manopt 7.0 and Matlab R2021a or later, if you have the Deep Learning Toolbox, then you can also use **automatic differentiation** (AD) instead of defining the gradient (and even the Hessian) yourself:\n\n```matlab\nproblem = manoptAD(problem);\n```\n\nSee the [cost description](costdescription.qmd) page for more information about how this works, and about what to do when it does not work.\nKeep in mind that, while AD is convenient and reasonably efficient, it is usually slower than (good) hand-written code.\n<!--Indeed, it is typical for AD to slow-down optimization by about a factor of five.\nStill, for prototyping, this is often a comfortable option.-->\n\n<!--\n::: {.callout-tip}\n## Avoiding redundant computations\nNotice that the functions `cost` and `egrad` both compute the product $Ax$, which is likely to be the most expensive operation for large scale problems.\nThis is perfectly fine for prototyping, but less so for a final version of the implementation.\nSee the many ways of [describing the cost function](costdescription.qmd) for alternatives that reduce redundant computations.\n:::\n-->\n\nThe next instruction is not needed to solve the problem but often helps at the prototyping stage:\n\n```matlab\ncheckgradient(problem);\n```\n\nThe [checkgradient](https://github.com/NicolasBoumal/manopt/blob/master/manopt/tools/checkgradient.m) tool verifies numerically that the cost function and its gradient agree up to the appropriate order.\nSee the [tools page](tools.qmd) for further details and more helpful tools offered by Manopt.\nThis tool generates the following figure:\n\n![The `checkgradient` tool outputs a figure and text in the command line. In the figure here, we see that part of the blue curve has a slope that matches that of the dashed line: that's what we like to see. If not, then it is likely that the gradient is incorrectly implemented.](images/tutorial-gradientcheck.png){width=80%}\n\nThe blue curve seems to have the same slope as the dashed line over a decent segment (highlighted in orange): that's what we want to see.\nAlso check the text output in the command prompt.\n\nWe now run one of [the solvers](solvers.qmd) on our problem:\n\n```matlab\n[x, xcost, info, options] = trustregions(problem);\n```\n\nThis instruction calls [`trustregions`](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/trustregions/trustregions.m), without initial guess and without options structure.\nAs a result, the solver generates a random initial guess automatically and resorts to the default values for all options.\nAs a general feature in Manopt, all options are, well, optional.\n\n:::{.callout-note}\n## Manopt always minimizes\nAll solvers in Manopt aim to _minimize_ the cost function in the problem structure.\nIf you want to _maximize_ a function, do as we did earlier on this page: flip the sign of $f$ (and accordingly for its derivatives).\n:::\n\nThe returned values are:\n\n* `x`: usually an approximate local minimizer of the cost function,\n* `xcost`: the value of $f$ at `x`,\n* `info`: a struct-array containing information about the successive iterations performed by the solver, and\n* `options`: a structure containing the options used and their values: peek inside to find out what you can parameterize.\n\nFor more details and more solvers, see the [solvers](solvers.qmd) page.\n\nThis call issues a warning:\n\n> `Warning: No Hessian provided. Using FD approximation.`\n\nThat is because the trust-regions algorithm normally requires the Hessian of the cost function to be provided in the problem structure.\nWhen the Hessian is not provided, Manopt approximates it using finite differences  on the gradient and it warns you about it.\nYou may disable this warning by calling `warning('off', 'manopt:getHessian:approx');`.\n\nFinally, we access the contents of the struct-array `info` to display a convergence plot:\n\n```matlab\nsemilogy([info.iter], [info.gradnorm], '.-');  \nxlabel('Iteration number');  \nylabel('Norm of the gradient of f');\n```\n\nThis generates the following figure.\nFor more information on what data is stored in `info`, look inside and see the [solvers](solvers.qmd) page.\n\n![The Riemannian gradient norm is converging to zero fast: that's what we like to see.](images/tutorial-gradientnorm.png){width=80%}\n\n\n::: {.callout-note}\n## `info` is a struct-array\nNotice that we write `[info.xyz]` with brackets, and not simply `info.xyz`.\nThis is because `info` is a struct-array.\nRead this [MathWorks blog post](https://blogs.mathworks.com/loren/2007/04/19/vectorizing-access-to-an-array-of-structures/) for further information.\n:::\n\n\n::: {.callout-warning}\n## Fallback to finite differences is automatic\nIf you do not specify the gradient, then Manopt approximates it with finite differences.\nThis is convenient for prototyping on low-dimensional manifolds, but it is slow and can make it difficult to reach accurate solutions.\nAs in that case the solver may have a hard time reaching points with a small gradient, you can pass an options structure to the solver with `options.tolgradnorm` set to a larger value to allow it to stop earlier.\n:::\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":{"file":"_header.html"},"highlight-style":"pygments","css":["styles.css"],"toc":false,"html-math-method":"mathjax","strip-comments":true,"output-file":"firstexample.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","crossref":{"eq-prefix":"Eq."},"theme":{"dark":["darkly","custom_darkly.scss"],"light":"sandstone"},"grid":{"sidebar-width":"230px","body-width":"800px","margin-width":"270px","gutter-width":"1.5rem"},"anchor-sections":true,"smooth-scroll":true,"title":"A first example","sidebar":"tutorial","aliases":["tutorial.html#firstexample"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}