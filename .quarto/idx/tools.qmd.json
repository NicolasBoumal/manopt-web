{"title":"Helpful tools","markdown":{"yaml":{"title":"Helpful tools","toc":true,"toc-depth":3,"toc-expand":2,"sidebar":"tutorial","aliases":["tutorial.html#tools"]},"headingText":"Checks for the cost function","containsRefs":false,"markdown":"\n\nManopt offers a suite of tools to help with studying a problem's landscape, detecting bugs, speeding up computations, handling geometric objects, keeping track of computational efforts, etc.\nThese are located in the folder [/manopt/tools](https://github.com/NicolasBoumal/manopt/tree/master/manopt/tools), and documented below.\n\n\n\nThe cost function $f$ and its derivatives satisfy certain relationships.\nBy checking these numerically, we can detect possible coding errors.\nManopt provides tools to this effect:\n\n* `checkdiff(problem)` checks directional derivatives (`problem.diff` etc.)\n* `checkgradient(problem)` checks the Riemannian gradient (`problem.egrad`, `problem.grad`, `problem.costgrad` etc.)\n* `checkhessian(problem)` checks the Riemannian Hessian (`problem.ehess`, `problem.hess` etc.)\n\nThe theory underlying these checks is explained in Sections 4.8 and 6.8 of [this book](https://www.nicolasboumal.net/book).\n\n### Gradient check\n\nPick a point $x$ on the manifold and a tangent vector $u$ at $x$.\nFrom a truncated Taylor expansion, we know that the following holds with any retraction $\\Retr$:\n$$\n  E(t) = \\Big| f(\\Retr_x(tu)) - \\big[ f(x) + t\\cdot\\D f(x)[u] \\big] \\Big| = \\mathcal{O}(t^2).\n$$\nHence, in a log-log plot with $\\log(t)$ on the abscissa, the error should behave as $\\log(t^2) = 2\\log(t)$.\nThat is, we should observe a slope of 2.\nCalling `checkdiff(problem, x, u)` produces such a plot and reports the slope of it in a text output.\nNumerical errors prevent the curve to have a slope of 2 everywhere even if directional derivatives are correct, so you should really inspect the plot visually.\nIf `x` and `u` are omitted, they are picked at random.\n\nThe Riemannian gradient is the (only) tangent vector field that satisfies\n$$\n  \\inner{\\grad f(x)}{u}_x = \\D f(x)[u]\n$$\nfor all $x, u$.\nCalling `checkgradient(problem, x, u)` computes $\\grad f(x)$ then does two things:\n\n1. It runs `checkdiff(problem, x, u)` with $\\D f(x)[u]$ replaced by $\\inner{\\grad f(x)}{u}_x$ in the expression for $E(t)$: this outputs the slope plot and text.\n2. It checks that the gradient is indeed a tangent vector, by reporting (as text) the norm of the difference between the gradient `gradfx` and the output of `problem.M.tangent(x, gradfx)`. This should be zero.\n\nIf `x` and `u` are omitted, they are picked at random.\n\n### Hessian check\n\nGoing back to $E(t)$ and including the next term in the Taylor expansion, we know that\n$$\n  E(t) = \\left| f(\\Retr_x(tu)) - \\left[ f(x) + t\\cdot\\D f(x)[u] + \\frac{t^2}{2} \\cdot \\inner{u}{\\Hess f(x)[u]}_x \\right] \\right| = \\mathcal{O}(t^3)\n$$\nas long as one (or both) of the two conditions holds:\n\n1. The retraction $\\Retr$ is second order (see remarks below), or\n2. The point $x$ is a critical point: $\\grad f(x) = 0$.\n\nHence, in a log-log plot with $\\log(t)$ on the abscissa, the error should behave as $\\log(t^3) = 3\\log(t)$, i.e., we should observe a slope of 3.\nThis tool produces such a plot and tries to compute the slope of it.\nAgain, numerical errors prevent the curve to have a slope of 3 everywhere even if the derivatives are correct, so you should inspect the plot visually.\n\nThe tool also verifies that the Hessian indeed returns a tangent vector, in the same way that we checked above that the gradient is a tangent vector.\nThis produces a text output.\n\nThe Hessian is a linear, symmetric operator from the tangent space at $x$ to itself.\nThe tool generates two random scalars $a_1, a_2$ and two random tangent vectors $u_1$ and $u_2$ at $x$ to test linearity:\n$$\n  \\Big\\| a_1 \\cdot \\Hess f(x)[u_1] + a_2 \\cdot \\Hess f(x)[u_2] - \\Hess f(x)[a_1 u_1 + a_2 u_2] \\Big\\|_x = 0.\n$$\nThe quantity on the left-hand side is reported in text output, and should be zero up to machine precision.\n\nTo verify symmetry, the tool further computes the difference\n$$\n  \\inner{\\Hess f(x)[u_1]}{u_2}_x - \\inner{u_1}{\\Hess f(x)[u_2]}_x,\n$$\nwhich should also be zero.\n\nIf `x` and `u` are omitted, they are picked at random.\n\nA couple of remarks:\n\n* It is important to check _both_ the slope test and the symmetry test. That is because $\\inner{u}{\\Hess f(x)[u]}_x$ only \"sees\" the symmetric part of $\\Hess f(x)$. If the code for the Hessian has a skew-symmetric part, then the Hessian is wrong, yet the slope test could succeed.\n* The tool `checkhessian` tries to use the exponential map `M.exp` by default (since this is a second-order retraction). If that is not available, the default retraction is used. It may be second order: read the `help` section of your manifold factory to confirm this. If it is not, then the slope test is inconclusive.\n\n\n\n## Checks for manifolds\n\n* `checkretraction(M, x, v)` <br>\n  For manifolds `M` which have a correct exponential map `M.exp` implemented, this tool produces a slopt-test plot to check the order of agreement of the retraction `M.retr` with the exponential.\n  A slope of 2 indicates the retraction is a first-order approximation of the exponential (which is necessary for most (all?) convergence theorems to hold.)\n  A slope of 3 indicates the retraction is second-order, which is convenient in some cases.\n  The check is conducted at point `x` along direction `v`; they are generated at random if omitted.\n\n* `checkmanifold(M)` <br>\n  Runs a collection of tests on a manifold structure produced by a factory.\n  The purpose of this tool is ease the process of creating factories for new manifolds.\n  Contributions are welcome to extend it.\n\n\n## Eigenvalues and eigenvectors of the Hessian\n\nGiven a function $f \\colon \\calM \\to \\reals$ and a point $x \\in \\calM$, we may want to investigate the spectrum of $\\Hess f(x)$, that is, the Riemannian Hessian at $x$.\nWith a `problem` structure to describe $f$, and `x` to identify the point $x$, we can do so in several ways using the tools\n\n* `hessianmatrix` (then `eig`)\n* `hessianspectrum` (internally via `eigs`)\n* `hessianextreme` (via optimization)\n\n### Via matrix representation\n\nThe first way is to obtain a representation of the Hessian as a matrix, then to compute the eigenvalues of that matrix.\nThe one-line vesion goes as follows:\n```matlab\neig(hessianmatrix(problem, x)) % eigenvalues of Hess f(x)\n```\nMore explicitly: $\\Hess f(x)$ is a symmetric linear map on the tangent space $\\T_x\\calM$.\nIf $b_1, \\ldots, b_k \\in \\T_x\\calM$ form an orthonormal basis for the tangent space, then the symmetric matrix $H \\in \\reals^{k \\times k}$ with\n$$H_{ij} = \\inner{b_i}{\\Hess f(x)[b_j]}_x$$\nrepresents the Hessian in those coordinates, in the sense that if $v = a_1 b_1 + \\cdots + a_k b_k$, then $\\Hess f(x)[v] = c_1 b_1 + \\cdots + c_k b_k$ where $c = Ha$.\nIn particular, the eigenvalues of $H$ are the same as the eigenvalues of $\\Hess f(x)$ because $b_1, \\ldots, b_k$ are orthonormal.\n\nIf `hessianmatrix` is called without providing an orthonormal basis, then a basis is generated at random via `tangentorthobasis`.\nYou can recover that basis too, as follows:\n```matlab\n[H, basis] = hessianmatrix(problem, x);\n```\nThen, `basis` is a cell such that `basis{1}, basis{2}, ...` form an orthonormal basis $b_1, b_2, \\ldots$ for $\\T_x\\calM$.\nThis makes it possible to access the eigenvectors of $\\Hess f(x)$ too, like so:\n```matlab\n[H, basis] = hessianmatrix(problem, x);\n[V, D] = eig(H);\nv = lincomb(problem.M, x, basis, V(:, 1)); % eigenvector for eigenvalue D(1, 1)\n```\nThat code:\n\n1. generates an orthonormal basis for $\\T_x\\calM$ and computes the matrix $H$ which represents $\\Hess f(x)$ in that basis with `hessianmatrix`,\n2. determines the eigenvalues and eigenvectors of $H$ with `eig`, then\n3. expands the first such eigenvector as a linear combination of the basis vectors with `lincomb`.\n\nThe result is a tangent vector $v$ at $x$ which is an eigenvector of $\\Hess f(x)$.\nYou can check this by comparing `v` with `getHessian(problem, x, v)`.\n\nIf you already have an orthonormal basis, you can use that one by calling\n```matlab\nH = hessianmatrix(problem, x, basis);\n```\nIf `basis` is an orthonormal basis for a _subspace_ of the tangent space, then `H` is a matrix that represents the restriction of the Hessian to that subspace.\n\n:::{.callout-note}\n## The matrix-way does not scale well\nGenerating the orthonormal basis takes time.\nSo does applying the Hessian to each basis vector.\nThis is a convenient tool for prototyping and exploration, but expect performance to degrade as dimension increases.\n:::\n\n\n### In a matrix-free way\n\nIn contrast to the `hessianmatrix` tool, the `hessianspectrum` tool provides access to the eigenvalues of the Hessian without building a basis for the tangent vector (and therefore also without constructing a matrix representation of the Hessian).\nIt relies on Matlab's `eigs`.\nAn additional advantage is that it also provides access to the spectrum of the preconditioned Hessian (if a preconditioner is included in the `problem` structure).\n\nTo compute the eigenvalues of the Hessian $\\Hess f(x)$ at $x$ with this tool, call\n```matlab\nhessianspectrum(problem, x) % eigenvalues of Hess f(x)\n```\nIf a preconditioner $\\mathrm{Prec}$ is specified in the problem structure and you call\n```matlab\nhessianspectrum(problem, x, 'precon') % eigenvalues of preconditioned Hess f(x)\n```\nthen the eigenvalues of the preconditioned Hessian $\\Hess f(x) \\circ \\mathrm{Prec}(x)$ are computed.\n\nThis function relies on `problem.M.vec` and `problem.M.mat` to pass the computations down to Matlab's built-in `eigs` function.\nFor the eigenvalue problem to remain symmetric in the column-vector representation domain, we need `M.vec` and `M.mat` to be orthonormal, i.e., isometries (see `matvecareisometries` in the [manifold section](manifolds.qmd)).\nIf they are not isometries, computations may take longer.\n\nIndeed, let $G$ denote the `M.vec` operator and let $G^{-1}$ represent the `M.mat` operator (on the appropriate domains).\nLet $H$ and $P$ denotes the Hessian and preconditioner at $x$ (with $P$ being identity if there is none).\nThen, `eigs` computes the spectrum of $GHG^{-1}$ or $GHPG^{-1}$, which are identical to, respectively, the spectra of $H$ and $HP$.\nThis is only symmetric if there is no preconditioner and $G^\\top = G^{-1}$.\n\nIf a preconditioner is used, the symmetry of the eigenvalue problem is lost: $H$ and $P$ are symmetric, but $HP$ is not.\nIf `M.vec` and `M.mat` are isometries and the dimension of the manifold is large, it may be useful to restore symmetry by giving this tool a function handle for the _square root_ of the preconditioner, $P^{1/2}$ (optional).\nThen, `eigs` is given the problem of computing the spectrum of $GP^{1/2}HP^{1/2}G^\\top$ (symmetric), which is equal to the spectrum of $HP$.\nTypically, the square root of the preconditioner is given via `problem.sqrtprecon` (see [cost description](costdescription.qmd)).\n\nThis tool can be faster than `hessianmatrix`, but it still aims to compute all eigenvalues.\nIf you only need to compute an eigenvector for the largest or smallest eigenvalue, try `hessianextreme` as follows:\n```matlab\n[u_min, lambda_min] = hessianextreme(problem, x, 'min');\n[u_max, lambda_max] = hessianextreme(problem, x, 'max');\n```\nThese run a Manopt solver on the Rayleigh quotient over the unit sphere in the tangent space $\\T_x\\calM$, aiming to compute (respectively) a minimizer and a maximizer.\nAs such, this tool is not guaranteed to succeed, but it always provides an upperbound on the smallest eigenvalue and a lowerbound on the largest eigenvalue of $\\Hess f(x)$.\nCall `help hessianextreme` for more options.\n\nComments:\n\n* At this time, `hessianspectrum` outputs the eigenvalues only.\n  It does not provide access to the eigenvectors, though it could be modified to that effect.\n  It could also be modified to call `eigs` in a way that targets only extreme eigenvalues.\n* Both `hessianspectrum` and `hessianextreme` accept `(storedb, key)` as optional inputs, to use the caching system.\n\n\n\n## Finding critical points\n\nWhen studying the landscape of an optimization problem, we may want to find critical points of $f$, that is, points where the Riemannian gradient is zero.\nIf `problem` is the structure that describes your manifold $\\calM$ and cost function $f$ (with derivatives), call\n```matlab\ncp_problem = criticalpointfinder(problem);\n```\nto create a new problem structure.\nThis one is on the same manifold $\\calM$, but with the cost function\n$$\n  g(x) = \\frac{1}{2} \\| \\grad f(x) \\|^2_x.\n$$\nThe gradient of $g$ is computed via $\\grad g(x) = \\Hess f(x)[\\grad f(x)]$.\nAn approximate Hessian can also be generated.\n\nEvidently, the minimizers of $g$ are the critical points of $f$.\nThus, running a solver such as `x = trustregions(cp_problem)` could find a critical point of $f$.\nThis is not guaranteed to work because $g$ may have non-global local minima.\nAccordingly, it is best to run the solver many times from various random initial guesses, and to check the gradient norm.\nFor example:\n\n```matlab\n% first define the problem structure, then:\ncp_problem = criticalpointfinder(problem);\nnrepeats = 100;\npoints = cell(nrepeats, 1);\ngradfnorms = inf(nrepeats, 1);\ncp_options.tolgradnorm = 1e-10;\nfor rep = 1 : nrepeats\n    x = trustregions(cp_problem, [], cp_options); % random init\n    points{rep} = x;\n    gradfnorms(rep) = problem.M.norm(x, getGradient(problem, x));\nend\n% Now check which points have a satisfactorily small gradient norm.\n```\n\n\n## Plotting the cost function\n\n* `plotprofile(problem, x, d, t)` <br>\n  Plots the cost function along a geodesic or a retraction path starting at $x$, along direction $d$.\n  All inputs are optional except `problem`.\n  See `help plotprofile` for more information.\n\n* `surfprofile(problem, x, d1, d2, t1, t2)` <br>\n  Plots the cost function, lifted and restricted to a 2-dimensional subspace of the tangent space at $x$.\n  All inputs are optional except `problem`.\n  See `help surfprofile` for more information.\n\n\n## Matrix computations\n\nManopt includes tools to facilitate certain matrix computations as listed in the first table below.\nThey provide help to:\n\n* differentiate matrix functions,\n* solve matrix equations, and\n* compute factorizations.\n\n| Call | Description |\n|-|---|\n| `dfunm`, `dlogm`, `dexpm`, `dsqrtm` | Fréchet derivatives of the (built-in) matrix functions, and their particularization to `logm`, `expm` and `sqrtm`. For example, the call `[A, B] = dexpm(X, Xdot)` outputs both $A = \\D\\mathrm{exp}(X)[\\dot X]$ and $B = \\mathrm{exp}(X)$. |\n| `lyapunov_symmetric` | Tool to solve the Lyapunov matrix equation $AX + XA = C$ when $A = A^*$ (real symmetric or Hermitian). Can solve for more than one right-hand side at a time. |\n| `lyapunov_symmetric_eig` | Same as `lyapunov_symmetric` but the user supplies the eigenvalue decomposition of $A$ instead of $A$. This is more efficient if several systems with the same $A$ need to be solved, but the various right-hand sides are not all known at the same time. |\n| `sylvester_nochecks` | Solves the Sylvester equation $AX + XB = C$, where $A$ is an m-by-m matrix, $B$ is an n-by-n matrix, and $X$ and $C$ are two m-by-n matrices. This is a stripped-down version of Matlab's own `sylvester` function that bypasses any input checks. This is significantly faster for small m and n, which is often useful in Manopt. |\n| `qr_unique` | Given $A$ with full columns rank, `Q = qr_unique(A)` computes $Q$ of the same size as $A$ such that $A = QR$, $Q$ has orthonormal columns and $R$ is upper triangular with positive diagonal entries. This fully specifies $Q$. (Matlab's `[Q, ~] = qr(A, 0)` does not enforce positive diagonal entries of $R$ by default, losing the uniqueness property). This Q-factor is exactly what one would compute through Gram--Schmidt orthonormalization of the columns of $A$, but it is computed differently. Works with 3D arrays (on each slice separately) and with both real and complex matrices. |\n\n: Tools for matrix computations that sometimes come up when using Manopt. {.striped}\n\nMoreover, it is often useful to apply the same operations to many matrices.\nFor best performance, it is important to vectorize such computations (in order to exploit [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) features of processors).\nThe table below list tools Manopt provides to do just that:\n\n| Call | Description |\n|-|---|\n| `B = multiscale(scale, A)` | For a 3D matrix `A` of size nxmxN and a vector `scale` of length N, outputs `B`: a 3D matrix of the same size as `A` such that `B(:, :, k) = scale(k) * A(:, :, k)` for each `k`. |\n| `tr = multitrace(A)` | For a 3D matrix `A` of size nxnxN, outputs a column vector `tr` of length N such that `tr(k) = trace(A(:, :, k))` for each `k`. |\n| `sq = multisqnorm(A)` | For a 3D matrix `A` of size nxmxN, outputs a column vector `sq` of length N such that `sq(k) = norm(A(:, :, k), 'fro')^2` for each `k`. |\n| `B = multitransp(A)` | For a 3D matrix `A` of size nxmxN, outputs `B`, a 3D matrix of size mxnxN such that `B(:, :, k) = A(:, :, k).'` for each `k` (transpose). |\n| `B = multihconj(A)` | For a 3D matrix `A` of size nxmxN, outputs `B`, a 3D matrix of size mxnxN such that `B(:, :, k) = A(:, :, k)'` for each `k` (conjugate transpose). |\n| `C = multiprod(A, B)` | For 3D matrices `A` of size nxpxN and `B` of size pxmxN, outputs `C`, a 3D matrix of size nxmxN such that `C(:, :, k) = A(:, :, k) * B(:, :, k)` for each `k`. |\n| `B = multiskew(A)` | For a 3D matrix `A` of size nxnxN, outputs a 3D matrix `B` the same size as `A` such that each slice `B(:, :, i)` is the skew-symmetric part of the slice `A(:, :, i)`, that is, `(A(:, :, i)-A(:, :, i).')/2`. |\n| `B = multiskewh(A)` | For a 3D matrix `A` of size nxnxN, outputs a 3D matrix `B` the same size as `A` such that each slice `B(:, :, i)` is the Hermitian skew-symmetric part of the slice `A(:, :, i)`, that is, `(A(:, :, i)-A(:, :, i)')/2`. |\n| `B = multisym(A)` | For a 3D matrix `A` of size nxnxN, outputs a 3D matrix `B` the same size as `A` such that each slice `B(:, :, i)` is the symmetric part of the slice `A(:, :, i)`, that is, `(A(:, :, i)+A(:, :, i).')/2`. |\n| `B = multiherm(A)` | For a 3D matrix `A` of size nxnxN, outputs a 3D matrix `B` the same size as `A` such that each slice `B(:, :, i)` is the Hermitian part of the slice `A(:, :, i)`, that is, `(A(:, :, i)+A(:, :, i)')/2`. |\n\n: Tools to apply the same computations to many matrices without for-loop. This improves performance significantly. {.striped}\n\n\n## Counters (to track computations)\n\nManopt counters provide a way to track all sorts of metrics, including function calls, time spent in specific parts of them, particular operations, etc.\nThey are accessed via two tools:\n\n* `S = statscounters(names)` is used to register Manopt counters in `options.statsfun` via `statsfunhelper`.\n* `incrementcounter(store, countername, increment)` increments a counter in a `store` or `storedb`.\n\nA basic usage would go as follows.\nSee the [cost description](costdescription.qmd) page, especially the section about caching, for more information about how `store` and `prepare` are used here.\n\n```matlab\nfunction foo()\n\n    n = 100;\n    A = randsym(n);\n\n    problem.M = spherefactory(size(A, 1));\n\n    problem.cost = @cost;\n    problem.egrad = @egrad;\n    problem.ehess = @ehess;\n\n    % List the names of counters we want the optimization algorithm to log.\n    % The fields in the structure stats are function handles: one for each\n    % counter. Before passing stats to statsfunhelper, we could add more\n    % fields to stats to log other things as well.\n    %\n    % Names of the counters (here, Aproducts and some_other_counter) are\n    % for us to choose: they only need to be valid structure field names.\n    % They need not have been defined in advance.\n    stats = statscounters({'Aproducts', 'some_other_counter'});\n    options.statsfun = statsfunhelper(stats);\n\n    [x, fx, info] = trustregions(problem, [], options);\n\n\n    semilogy([info.Aproducts], [info.gradnorm], '.-');\n    xlabel('Number of matrix-vector products with A');\n    ylabel('Riemannian gradient norm');\n\n    % Below, we have the code for the cost function and its derivatives.\n    % Everytime we use a matrix-vector product with A, we increment the\n    % counter.\n\n    function store = prepare(x, store)\n        if ~isfield(store, 'Ax')\n            store.Ax = A*x;\n            store = incrementcounter(store, 'Aproducts');\n        end\n    end\n    function [f, store] = cost(x, store)\n        store = prepare(x, store);\n        Ax = store.Ax;\n        f = .5*x'*Ax;\n    end\n    function [g, store] = egrad(x, store)\n        store = prepare(x, store);\n        g = store.Ax;\n    end\n    function [h, store] = ehess(x, u, store)\n        h = A*u;\n        store = incrementcounter(store, 'Aproducts');\n    end\n\nend\n``` \n\nBy default, `incrementcounter` increments by 1.\nYou may also specify the increment as the last input (it can be any `double` value, not necessarily integer or positive).\n\nSee the [full working example](https://github.com/NicolasBoumal/manopt/blob/master/examples/using_counters.m) in the [/examples](https://github.com/NicolasBoumal/manopt/blob/master/examples) folder to see how to:\n\n* register more than one counter,\n* use counters in a stopping criterion,\n* run several solvers on the same problem and compare the metrics tracked by counters.\n\n:::{.callout-note}\nCounter names (such as `'Aproducts'` in the example) must be valid names for structure fields.\nEssentially, this means they should be valid variable names (no spaces, do not start with a digit, etc.)\n:::\n\n\n## Working with tangent vectors\n\nThe following tools ease certain tasks involving tangent spaces and tangent vectors.\n\n * `vec = lincomb(M, x, vectors, coeffs)` <br>\n   Given a cell `vectors` of $n$ tangent vectors to the manifold `M` at `x` and a vector `coeffs` of $n$ real coefficients, outputs the linear combination of the given vectors with the given coefficients.\n   The empty linear combination is the zero vector at `x`.\n   \n * `coeffs = tangent2vec(M, x, basis, u)` <br>\n   Given a tangent vector `u` at `x` and an _orthonormal_ basis `basis` for the corresponding tangent space, outputs the coordinates `coeffs` of `u` in that basis.\n   The inverse operation is `u = lincomb(M, x, basis, coeffs)`, see above.\n   \n * `G = grammatrix(M, x, vectors)` <br>\n   Given $n$ tangent vectors $v_1, \\ldots, v_n$ to the manifold `M` at point `x` in a cell `vectors`, outputs a symmetric, positive semidefinite matrix `G` of size $n\\times n$ such that $G_{ij} = \\inner{v_i}{v_j}_x$.\n   \n * `[orthobasis, L] = orthogonalize(M, x, basis)` <br>\n   Given a cell `basis` which contains linearly independent tangent vectors to the manifold `M` at `x`, outputs an _orthonormal_ basis of the subspace spanned by the given basis.\n   `L` is an upper triangular matrix containing the coefficients of the linear combinations needed to transform `basis` into `orthobasis`.\n   This is essentially a QR factorization, via modified Gram--Schmidt.\n   \n * `[orthobasis, L] = orthogonalizetwice(M, x, basis)` <br>\n   Same as `orthogonalize`, but calls it twice in sequence for (much) improved numerical stability (at twice the computational cost).\n   \n * `obasis = tangentorthobasis(M, x, n)` <br>\n   Given a point `x` on the manifold `M`, generates `n` unit-norm, pairwise orthogonal vectors in the tangent space to `M` at `x`, in a cell.\n   See `help tangentorthobasis` for more advanced call patterns.\n   \n * `[u_norm, coeffs, u] = smallestinconvexhull(M, x, U)` <br>\n   Computes `u`, a tangent vector to `M` at `x` contained in the convex hull spanned by the $n$ vectors in the cell `U`, with minimal norm (according to the Riemannian metric on `M`).\n   This is obtained by solving a convex quadratic program involving the Gram matrix of the given tangent vectors.\n   The quadratic program is solved using Matlab's built-in `quadprog`, which requires the Optimization Toolbox.\n   \n * `[A, B1, B2] = operator2matrix(M1, x, y, F, B1, B2, M2)` <br>\n   Given manifold structures `M1` and `M2`, two points `x` and `y` on these manifolds, and a function `F` encoding a linear operator from the tangent space $\\T_x \\calM_1$ to the tangent space $\\T_y \\calM_2$, this tool uses two orthonormal bases `B1` and `B2` (one for $\\T_x \\calM_1$, and one for $\\T_y \\calM_2$; generated at random if omitted), and forms the matrix `A` which represents the operator `F` in those bases.\n   In particular, the singular values of `A` are equal to the singular values of `F`.\n   If `M2` is omitted, then `M2 = M1`.\n   See the code for more use cases.\n \n\n\n## Interactive stopping criteria\n\nAn interactive stopping criterion allows the user to stop the execution of a Manopt solver in real time.\nWhen it is triggered, the solver gracefully terminates and outputs the best iterate it produced so far.\nMatlab then proceeds to keep running the code that follows the call to the solver, so that the work done until that point is not lost.\n\nOne such tool open a special figure once the solver starts running.\nThe solver terminates if the figure is closed.\n```matlab\noptions.stopfun = @stopifclosedfigure; % add this option\ntrustregions(problem, x0, options);    % run this or any other solver\n```\n\nAnother such tool (better suited if you are running Matlab without graphical user interface, e.g., over SSH) creates a special file.\nThe solver terminates if that file is deleted.\n```matlab\noptions.stopfun = stopifdeletedfile(); % add this option\ntrustregions(problem, x0, options);    % run this or any other solver\n```\nBy default, the file is called `MANOPT_DELETE_ME_TO_STOP_SOLVER`.\nYou may also specify another file name as optional input to `stopifdeletedfile`.\n\nNote that termination may not be immediate as the solver has to finish the current iteration first.\nIn particular, certain solvers (including `trustregions`) check stopping criteria only at outer iterations, not during inner iterations, further increasing the delay.\n\n\n## Utilities for solvers\n\n* `statsfunhelper` <br>\n  Helper function to place a function handle in the field `options.statsfun`, with the purpose of recording or displaying information about individual iterations.\n  See [this page](solvers.qmd#statsfun-option-for-recording-info-at-each-iteration) for documentation.\n  Also consider using `statscounters` and `incrementcounter` as documented [on this page](#counters-to-track-computations).\n\n* `manoptsolve` <br>\n  This tool presents itself as a solver, with their usual calling pattern:\n  ```matlab\n  [x, cost, info, options] = manoptsolve(problem, x0, options);\n  ```\n  It is a gateway function to call an actual Manopt solver.\n  You may specify which one to call by setting `options.solver` to a function handle corresponding to [a solver](https://github.com/NicolasBoumal/manopt/tree/master/manopt/solvers).\n  For example,\n  ```matlab\n  options.solver = @trustregions;\n  ```\n  If not, a solver is picked automatically.\n  This is mainly useful when programming meta algorithms which need to solve a Manopt problem, but one wants to leave the decision of which solver to use up to the final user (therefore making it an option).\n\n\n## Creating manifolds\n\n * `productmanifold` and `powermanifold` <br>\n   These tools generate a structure that represents a product of manifolds.\n   See [this page](manifolds.qmd#product-manifolds) for documentation.\n\n * `N = tangentspacefactory(M, x)` <br>\n   Given a manifold structure `M` and a point `x` on that manifold, outputs a manifold structure `N` representing the tangent space to `M` at `x`.\n   This is used in [preconhessiansolve](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/preconditioners/preconhessiansolve.m).\n\n * `N = tangentspherefactory(M, x)` <br>\n   Given a manifold structure `M` and a point `x` on that manifold, outputs a manifold structure `N` representing the unit sphere on the tangent space to `M` at `x`.\n   This is used by the [hessianextreme](https://github.com/NicolasBoumal/manopt/blob/master/manopt/tools/hessianextreme.m) tool.\n   \n\n## Miscellaneous\n\n* `y = sinxoverx(x)` <br>\n  Computes $y = \\sin(x)/x$, with the convention $\\sin(0)/0 = 1$.\n\n* `s = getsize(x)` <br>\n  Estimates the memory usage of the input variable.\n","srcMarkdownNoYaml":"\n\nManopt offers a suite of tools to help with studying a problem's landscape, detecting bugs, speeding up computations, handling geometric objects, keeping track of computational efforts, etc.\nThese are located in the folder [/manopt/tools](https://github.com/NicolasBoumal/manopt/tree/master/manopt/tools), and documented below.\n\n\n## Checks for the cost function\n\nThe cost function $f$ and its derivatives satisfy certain relationships.\nBy checking these numerically, we can detect possible coding errors.\nManopt provides tools to this effect:\n\n* `checkdiff(problem)` checks directional derivatives (`problem.diff` etc.)\n* `checkgradient(problem)` checks the Riemannian gradient (`problem.egrad`, `problem.grad`, `problem.costgrad` etc.)\n* `checkhessian(problem)` checks the Riemannian Hessian (`problem.ehess`, `problem.hess` etc.)\n\nThe theory underlying these checks is explained in Sections 4.8 and 6.8 of [this book](https://www.nicolasboumal.net/book).\n\n### Gradient check\n\nPick a point $x$ on the manifold and a tangent vector $u$ at $x$.\nFrom a truncated Taylor expansion, we know that the following holds with any retraction $\\Retr$:\n$$\n  E(t) = \\Big| f(\\Retr_x(tu)) - \\big[ f(x) + t\\cdot\\D f(x)[u] \\big] \\Big| = \\mathcal{O}(t^2).\n$$\nHence, in a log-log plot with $\\log(t)$ on the abscissa, the error should behave as $\\log(t^2) = 2\\log(t)$.\nThat is, we should observe a slope of 2.\nCalling `checkdiff(problem, x, u)` produces such a plot and reports the slope of it in a text output.\nNumerical errors prevent the curve to have a slope of 2 everywhere even if directional derivatives are correct, so you should really inspect the plot visually.\nIf `x` and `u` are omitted, they are picked at random.\n\nThe Riemannian gradient is the (only) tangent vector field that satisfies\n$$\n  \\inner{\\grad f(x)}{u}_x = \\D f(x)[u]\n$$\nfor all $x, u$.\nCalling `checkgradient(problem, x, u)` computes $\\grad f(x)$ then does two things:\n\n1. It runs `checkdiff(problem, x, u)` with $\\D f(x)[u]$ replaced by $\\inner{\\grad f(x)}{u}_x$ in the expression for $E(t)$: this outputs the slope plot and text.\n2. It checks that the gradient is indeed a tangent vector, by reporting (as text) the norm of the difference between the gradient `gradfx` and the output of `problem.M.tangent(x, gradfx)`. This should be zero.\n\nIf `x` and `u` are omitted, they are picked at random.\n\n### Hessian check\n\nGoing back to $E(t)$ and including the next term in the Taylor expansion, we know that\n$$\n  E(t) = \\left| f(\\Retr_x(tu)) - \\left[ f(x) + t\\cdot\\D f(x)[u] + \\frac{t^2}{2} \\cdot \\inner{u}{\\Hess f(x)[u]}_x \\right] \\right| = \\mathcal{O}(t^3)\n$$\nas long as one (or both) of the two conditions holds:\n\n1. The retraction $\\Retr$ is second order (see remarks below), or\n2. The point $x$ is a critical point: $\\grad f(x) = 0$.\n\nHence, in a log-log plot with $\\log(t)$ on the abscissa, the error should behave as $\\log(t^3) = 3\\log(t)$, i.e., we should observe a slope of 3.\nThis tool produces such a plot and tries to compute the slope of it.\nAgain, numerical errors prevent the curve to have a slope of 3 everywhere even if the derivatives are correct, so you should inspect the plot visually.\n\nThe tool also verifies that the Hessian indeed returns a tangent vector, in the same way that we checked above that the gradient is a tangent vector.\nThis produces a text output.\n\nThe Hessian is a linear, symmetric operator from the tangent space at $x$ to itself.\nThe tool generates two random scalars $a_1, a_2$ and two random tangent vectors $u_1$ and $u_2$ at $x$ to test linearity:\n$$\n  \\Big\\| a_1 \\cdot \\Hess f(x)[u_1] + a_2 \\cdot \\Hess f(x)[u_2] - \\Hess f(x)[a_1 u_1 + a_2 u_2] \\Big\\|_x = 0.\n$$\nThe quantity on the left-hand side is reported in text output, and should be zero up to machine precision.\n\nTo verify symmetry, the tool further computes the difference\n$$\n  \\inner{\\Hess f(x)[u_1]}{u_2}_x - \\inner{u_1}{\\Hess f(x)[u_2]}_x,\n$$\nwhich should also be zero.\n\nIf `x` and `u` are omitted, they are picked at random.\n\nA couple of remarks:\n\n* It is important to check _both_ the slope test and the symmetry test. That is because $\\inner{u}{\\Hess f(x)[u]}_x$ only \"sees\" the symmetric part of $\\Hess f(x)$. If the code for the Hessian has a skew-symmetric part, then the Hessian is wrong, yet the slope test could succeed.\n* The tool `checkhessian` tries to use the exponential map `M.exp` by default (since this is a second-order retraction). If that is not available, the default retraction is used. It may be second order: read the `help` section of your manifold factory to confirm this. If it is not, then the slope test is inconclusive.\n\n\n\n## Checks for manifolds\n\n* `checkretraction(M, x, v)` <br>\n  For manifolds `M` which have a correct exponential map `M.exp` implemented, this tool produces a slopt-test plot to check the order of agreement of the retraction `M.retr` with the exponential.\n  A slope of 2 indicates the retraction is a first-order approximation of the exponential (which is necessary for most (all?) convergence theorems to hold.)\n  A slope of 3 indicates the retraction is second-order, which is convenient in some cases.\n  The check is conducted at point `x` along direction `v`; they are generated at random if omitted.\n\n* `checkmanifold(M)` <br>\n  Runs a collection of tests on a manifold structure produced by a factory.\n  The purpose of this tool is ease the process of creating factories for new manifolds.\n  Contributions are welcome to extend it.\n\n\n## Eigenvalues and eigenvectors of the Hessian\n\nGiven a function $f \\colon \\calM \\to \\reals$ and a point $x \\in \\calM$, we may want to investigate the spectrum of $\\Hess f(x)$, that is, the Riemannian Hessian at $x$.\nWith a `problem` structure to describe $f$, and `x` to identify the point $x$, we can do so in several ways using the tools\n\n* `hessianmatrix` (then `eig`)\n* `hessianspectrum` (internally via `eigs`)\n* `hessianextreme` (via optimization)\n\n### Via matrix representation\n\nThe first way is to obtain a representation of the Hessian as a matrix, then to compute the eigenvalues of that matrix.\nThe one-line vesion goes as follows:\n```matlab\neig(hessianmatrix(problem, x)) % eigenvalues of Hess f(x)\n```\nMore explicitly: $\\Hess f(x)$ is a symmetric linear map on the tangent space $\\T_x\\calM$.\nIf $b_1, \\ldots, b_k \\in \\T_x\\calM$ form an orthonormal basis for the tangent space, then the symmetric matrix $H \\in \\reals^{k \\times k}$ with\n$$H_{ij} = \\inner{b_i}{\\Hess f(x)[b_j]}_x$$\nrepresents the Hessian in those coordinates, in the sense that if $v = a_1 b_1 + \\cdots + a_k b_k$, then $\\Hess f(x)[v] = c_1 b_1 + \\cdots + c_k b_k$ where $c = Ha$.\nIn particular, the eigenvalues of $H$ are the same as the eigenvalues of $\\Hess f(x)$ because $b_1, \\ldots, b_k$ are orthonormal.\n\nIf `hessianmatrix` is called without providing an orthonormal basis, then a basis is generated at random via `tangentorthobasis`.\nYou can recover that basis too, as follows:\n```matlab\n[H, basis] = hessianmatrix(problem, x);\n```\nThen, `basis` is a cell such that `basis{1}, basis{2}, ...` form an orthonormal basis $b_1, b_2, \\ldots$ for $\\T_x\\calM$.\nThis makes it possible to access the eigenvectors of $\\Hess f(x)$ too, like so:\n```matlab\n[H, basis] = hessianmatrix(problem, x);\n[V, D] = eig(H);\nv = lincomb(problem.M, x, basis, V(:, 1)); % eigenvector for eigenvalue D(1, 1)\n```\nThat code:\n\n1. generates an orthonormal basis for $\\T_x\\calM$ and computes the matrix $H$ which represents $\\Hess f(x)$ in that basis with `hessianmatrix`,\n2. determines the eigenvalues and eigenvectors of $H$ with `eig`, then\n3. expands the first such eigenvector as a linear combination of the basis vectors with `lincomb`.\n\nThe result is a tangent vector $v$ at $x$ which is an eigenvector of $\\Hess f(x)$.\nYou can check this by comparing `v` with `getHessian(problem, x, v)`.\n\nIf you already have an orthonormal basis, you can use that one by calling\n```matlab\nH = hessianmatrix(problem, x, basis);\n```\nIf `basis` is an orthonormal basis for a _subspace_ of the tangent space, then `H` is a matrix that represents the restriction of the Hessian to that subspace.\n\n:::{.callout-note}\n## The matrix-way does not scale well\nGenerating the orthonormal basis takes time.\nSo does applying the Hessian to each basis vector.\nThis is a convenient tool for prototyping and exploration, but expect performance to degrade as dimension increases.\n:::\n\n\n### In a matrix-free way\n\nIn contrast to the `hessianmatrix` tool, the `hessianspectrum` tool provides access to the eigenvalues of the Hessian without building a basis for the tangent vector (and therefore also without constructing a matrix representation of the Hessian).\nIt relies on Matlab's `eigs`.\nAn additional advantage is that it also provides access to the spectrum of the preconditioned Hessian (if a preconditioner is included in the `problem` structure).\n\nTo compute the eigenvalues of the Hessian $\\Hess f(x)$ at $x$ with this tool, call\n```matlab\nhessianspectrum(problem, x) % eigenvalues of Hess f(x)\n```\nIf a preconditioner $\\mathrm{Prec}$ is specified in the problem structure and you call\n```matlab\nhessianspectrum(problem, x, 'precon') % eigenvalues of preconditioned Hess f(x)\n```\nthen the eigenvalues of the preconditioned Hessian $\\Hess f(x) \\circ \\mathrm{Prec}(x)$ are computed.\n\nThis function relies on `problem.M.vec` and `problem.M.mat` to pass the computations down to Matlab's built-in `eigs` function.\nFor the eigenvalue problem to remain symmetric in the column-vector representation domain, we need `M.vec` and `M.mat` to be orthonormal, i.e., isometries (see `matvecareisometries` in the [manifold section](manifolds.qmd)).\nIf they are not isometries, computations may take longer.\n\nIndeed, let $G$ denote the `M.vec` operator and let $G^{-1}$ represent the `M.mat` operator (on the appropriate domains).\nLet $H$ and $P$ denotes the Hessian and preconditioner at $x$ (with $P$ being identity if there is none).\nThen, `eigs` computes the spectrum of $GHG^{-1}$ or $GHPG^{-1}$, which are identical to, respectively, the spectra of $H$ and $HP$.\nThis is only symmetric if there is no preconditioner and $G^\\top = G^{-1}$.\n\nIf a preconditioner is used, the symmetry of the eigenvalue problem is lost: $H$ and $P$ are symmetric, but $HP$ is not.\nIf `M.vec` and `M.mat` are isometries and the dimension of the manifold is large, it may be useful to restore symmetry by giving this tool a function handle for the _square root_ of the preconditioner, $P^{1/2}$ (optional).\nThen, `eigs` is given the problem of computing the spectrum of $GP^{1/2}HP^{1/2}G^\\top$ (symmetric), which is equal to the spectrum of $HP$.\nTypically, the square root of the preconditioner is given via `problem.sqrtprecon` (see [cost description](costdescription.qmd)).\n\nThis tool can be faster than `hessianmatrix`, but it still aims to compute all eigenvalues.\nIf you only need to compute an eigenvector for the largest or smallest eigenvalue, try `hessianextreme` as follows:\n```matlab\n[u_min, lambda_min] = hessianextreme(problem, x, 'min');\n[u_max, lambda_max] = hessianextreme(problem, x, 'max');\n```\nThese run a Manopt solver on the Rayleigh quotient over the unit sphere in the tangent space $\\T_x\\calM$, aiming to compute (respectively) a minimizer and a maximizer.\nAs such, this tool is not guaranteed to succeed, but it always provides an upperbound on the smallest eigenvalue and a lowerbound on the largest eigenvalue of $\\Hess f(x)$.\nCall `help hessianextreme` for more options.\n\nComments:\n\n* At this time, `hessianspectrum` outputs the eigenvalues only.\n  It does not provide access to the eigenvectors, though it could be modified to that effect.\n  It could also be modified to call `eigs` in a way that targets only extreme eigenvalues.\n* Both `hessianspectrum` and `hessianextreme` accept `(storedb, key)` as optional inputs, to use the caching system.\n\n\n\n## Finding critical points\n\nWhen studying the landscape of an optimization problem, we may want to find critical points of $f$, that is, points where the Riemannian gradient is zero.\nIf `problem` is the structure that describes your manifold $\\calM$ and cost function $f$ (with derivatives), call\n```matlab\ncp_problem = criticalpointfinder(problem);\n```\nto create a new problem structure.\nThis one is on the same manifold $\\calM$, but with the cost function\n$$\n  g(x) = \\frac{1}{2} \\| \\grad f(x) \\|^2_x.\n$$\nThe gradient of $g$ is computed via $\\grad g(x) = \\Hess f(x)[\\grad f(x)]$.\nAn approximate Hessian can also be generated.\n\nEvidently, the minimizers of $g$ are the critical points of $f$.\nThus, running a solver such as `x = trustregions(cp_problem)` could find a critical point of $f$.\nThis is not guaranteed to work because $g$ may have non-global local minima.\nAccordingly, it is best to run the solver many times from various random initial guesses, and to check the gradient norm.\nFor example:\n\n```matlab\n% first define the problem structure, then:\ncp_problem = criticalpointfinder(problem);\nnrepeats = 100;\npoints = cell(nrepeats, 1);\ngradfnorms = inf(nrepeats, 1);\ncp_options.tolgradnorm = 1e-10;\nfor rep = 1 : nrepeats\n    x = trustregions(cp_problem, [], cp_options); % random init\n    points{rep} = x;\n    gradfnorms(rep) = problem.M.norm(x, getGradient(problem, x));\nend\n% Now check which points have a satisfactorily small gradient norm.\n```\n\n\n## Plotting the cost function\n\n* `plotprofile(problem, x, d, t)` <br>\n  Plots the cost function along a geodesic or a retraction path starting at $x$, along direction $d$.\n  All inputs are optional except `problem`.\n  See `help plotprofile` for more information.\n\n* `surfprofile(problem, x, d1, d2, t1, t2)` <br>\n  Plots the cost function, lifted and restricted to a 2-dimensional subspace of the tangent space at $x$.\n  All inputs are optional except `problem`.\n  See `help surfprofile` for more information.\n\n\n## Matrix computations\n\nManopt includes tools to facilitate certain matrix computations as listed in the first table below.\nThey provide help to:\n\n* differentiate matrix functions,\n* solve matrix equations, and\n* compute factorizations.\n\n| Call | Description |\n|-|---|\n| `dfunm`, `dlogm`, `dexpm`, `dsqrtm` | Fréchet derivatives of the (built-in) matrix functions, and their particularization to `logm`, `expm` and `sqrtm`. For example, the call `[A, B] = dexpm(X, Xdot)` outputs both $A = \\D\\mathrm{exp}(X)[\\dot X]$ and $B = \\mathrm{exp}(X)$. |\n| `lyapunov_symmetric` | Tool to solve the Lyapunov matrix equation $AX + XA = C$ when $A = A^*$ (real symmetric or Hermitian). Can solve for more than one right-hand side at a time. |\n| `lyapunov_symmetric_eig` | Same as `lyapunov_symmetric` but the user supplies the eigenvalue decomposition of $A$ instead of $A$. This is more efficient if several systems with the same $A$ need to be solved, but the various right-hand sides are not all known at the same time. |\n| `sylvester_nochecks` | Solves the Sylvester equation $AX + XB = C$, where $A$ is an m-by-m matrix, $B$ is an n-by-n matrix, and $X$ and $C$ are two m-by-n matrices. This is a stripped-down version of Matlab's own `sylvester` function that bypasses any input checks. This is significantly faster for small m and n, which is often useful in Manopt. |\n| `qr_unique` | Given $A$ with full columns rank, `Q = qr_unique(A)` computes $Q$ of the same size as $A$ such that $A = QR$, $Q$ has orthonormal columns and $R$ is upper triangular with positive diagonal entries. This fully specifies $Q$. (Matlab's `[Q, ~] = qr(A, 0)` does not enforce positive diagonal entries of $R$ by default, losing the uniqueness property). This Q-factor is exactly what one would compute through Gram--Schmidt orthonormalization of the columns of $A$, but it is computed differently. Works with 3D arrays (on each slice separately) and with both real and complex matrices. |\n\n: Tools for matrix computations that sometimes come up when using Manopt. {.striped}\n\nMoreover, it is often useful to apply the same operations to many matrices.\nFor best performance, it is important to vectorize such computations (in order to exploit [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) features of processors).\nThe table below list tools Manopt provides to do just that:\n\n| Call | Description |\n|-|---|\n| `B = multiscale(scale, A)` | For a 3D matrix `A` of size nxmxN and a vector `scale` of length N, outputs `B`: a 3D matrix of the same size as `A` such that `B(:, :, k) = scale(k) * A(:, :, k)` for each `k`. |\n| `tr = multitrace(A)` | For a 3D matrix `A` of size nxnxN, outputs a column vector `tr` of length N such that `tr(k) = trace(A(:, :, k))` for each `k`. |\n| `sq = multisqnorm(A)` | For a 3D matrix `A` of size nxmxN, outputs a column vector `sq` of length N such that `sq(k) = norm(A(:, :, k), 'fro')^2` for each `k`. |\n| `B = multitransp(A)` | For a 3D matrix `A` of size nxmxN, outputs `B`, a 3D matrix of size mxnxN such that `B(:, :, k) = A(:, :, k).'` for each `k` (transpose). |\n| `B = multihconj(A)` | For a 3D matrix `A` of size nxmxN, outputs `B`, a 3D matrix of size mxnxN such that `B(:, :, k) = A(:, :, k)'` for each `k` (conjugate transpose). |\n| `C = multiprod(A, B)` | For 3D matrices `A` of size nxpxN and `B` of size pxmxN, outputs `C`, a 3D matrix of size nxmxN such that `C(:, :, k) = A(:, :, k) * B(:, :, k)` for each `k`. |\n| `B = multiskew(A)` | For a 3D matrix `A` of size nxnxN, outputs a 3D matrix `B` the same size as `A` such that each slice `B(:, :, i)` is the skew-symmetric part of the slice `A(:, :, i)`, that is, `(A(:, :, i)-A(:, :, i).')/2`. |\n| `B = multiskewh(A)` | For a 3D matrix `A` of size nxnxN, outputs a 3D matrix `B` the same size as `A` such that each slice `B(:, :, i)` is the Hermitian skew-symmetric part of the slice `A(:, :, i)`, that is, `(A(:, :, i)-A(:, :, i)')/2`. |\n| `B = multisym(A)` | For a 3D matrix `A` of size nxnxN, outputs a 3D matrix `B` the same size as `A` such that each slice `B(:, :, i)` is the symmetric part of the slice `A(:, :, i)`, that is, `(A(:, :, i)+A(:, :, i).')/2`. |\n| `B = multiherm(A)` | For a 3D matrix `A` of size nxnxN, outputs a 3D matrix `B` the same size as `A` such that each slice `B(:, :, i)` is the Hermitian part of the slice `A(:, :, i)`, that is, `(A(:, :, i)+A(:, :, i)')/2`. |\n\n: Tools to apply the same computations to many matrices without for-loop. This improves performance significantly. {.striped}\n\n\n## Counters (to track computations)\n\nManopt counters provide a way to track all sorts of metrics, including function calls, time spent in specific parts of them, particular operations, etc.\nThey are accessed via two tools:\n\n* `S = statscounters(names)` is used to register Manopt counters in `options.statsfun` via `statsfunhelper`.\n* `incrementcounter(store, countername, increment)` increments a counter in a `store` or `storedb`.\n\nA basic usage would go as follows.\nSee the [cost description](costdescription.qmd) page, especially the section about caching, for more information about how `store` and `prepare` are used here.\n\n```matlab\nfunction foo()\n\n    n = 100;\n    A = randsym(n);\n\n    problem.M = spherefactory(size(A, 1));\n\n    problem.cost = @cost;\n    problem.egrad = @egrad;\n    problem.ehess = @ehess;\n\n    % List the names of counters we want the optimization algorithm to log.\n    % The fields in the structure stats are function handles: one for each\n    % counter. Before passing stats to statsfunhelper, we could add more\n    % fields to stats to log other things as well.\n    %\n    % Names of the counters (here, Aproducts and some_other_counter) are\n    % for us to choose: they only need to be valid structure field names.\n    % They need not have been defined in advance.\n    stats = statscounters({'Aproducts', 'some_other_counter'});\n    options.statsfun = statsfunhelper(stats);\n\n    [x, fx, info] = trustregions(problem, [], options);\n\n\n    semilogy([info.Aproducts], [info.gradnorm], '.-');\n    xlabel('Number of matrix-vector products with A');\n    ylabel('Riemannian gradient norm');\n\n    % Below, we have the code for the cost function and its derivatives.\n    % Everytime we use a matrix-vector product with A, we increment the\n    % counter.\n\n    function store = prepare(x, store)\n        if ~isfield(store, 'Ax')\n            store.Ax = A*x;\n            store = incrementcounter(store, 'Aproducts');\n        end\n    end\n    function [f, store] = cost(x, store)\n        store = prepare(x, store);\n        Ax = store.Ax;\n        f = .5*x'*Ax;\n    end\n    function [g, store] = egrad(x, store)\n        store = prepare(x, store);\n        g = store.Ax;\n    end\n    function [h, store] = ehess(x, u, store)\n        h = A*u;\n        store = incrementcounter(store, 'Aproducts');\n    end\n\nend\n``` \n\nBy default, `incrementcounter` increments by 1.\nYou may also specify the increment as the last input (it can be any `double` value, not necessarily integer or positive).\n\nSee the [full working example](https://github.com/NicolasBoumal/manopt/blob/master/examples/using_counters.m) in the [/examples](https://github.com/NicolasBoumal/manopt/blob/master/examples) folder to see how to:\n\n* register more than one counter,\n* use counters in a stopping criterion,\n* run several solvers on the same problem and compare the metrics tracked by counters.\n\n:::{.callout-note}\nCounter names (such as `'Aproducts'` in the example) must be valid names for structure fields.\nEssentially, this means they should be valid variable names (no spaces, do not start with a digit, etc.)\n:::\n\n\n## Working with tangent vectors\n\nThe following tools ease certain tasks involving tangent spaces and tangent vectors.\n\n * `vec = lincomb(M, x, vectors, coeffs)` <br>\n   Given a cell `vectors` of $n$ tangent vectors to the manifold `M` at `x` and a vector `coeffs` of $n$ real coefficients, outputs the linear combination of the given vectors with the given coefficients.\n   The empty linear combination is the zero vector at `x`.\n   \n * `coeffs = tangent2vec(M, x, basis, u)` <br>\n   Given a tangent vector `u` at `x` and an _orthonormal_ basis `basis` for the corresponding tangent space, outputs the coordinates `coeffs` of `u` in that basis.\n   The inverse operation is `u = lincomb(M, x, basis, coeffs)`, see above.\n   \n * `G = grammatrix(M, x, vectors)` <br>\n   Given $n$ tangent vectors $v_1, \\ldots, v_n$ to the manifold `M` at point `x` in a cell `vectors`, outputs a symmetric, positive semidefinite matrix `G` of size $n\\times n$ such that $G_{ij} = \\inner{v_i}{v_j}_x$.\n   \n * `[orthobasis, L] = orthogonalize(M, x, basis)` <br>\n   Given a cell `basis` which contains linearly independent tangent vectors to the manifold `M` at `x`, outputs an _orthonormal_ basis of the subspace spanned by the given basis.\n   `L` is an upper triangular matrix containing the coefficients of the linear combinations needed to transform `basis` into `orthobasis`.\n   This is essentially a QR factorization, via modified Gram--Schmidt.\n   \n * `[orthobasis, L] = orthogonalizetwice(M, x, basis)` <br>\n   Same as `orthogonalize`, but calls it twice in sequence for (much) improved numerical stability (at twice the computational cost).\n   \n * `obasis = tangentorthobasis(M, x, n)` <br>\n   Given a point `x` on the manifold `M`, generates `n` unit-norm, pairwise orthogonal vectors in the tangent space to `M` at `x`, in a cell.\n   See `help tangentorthobasis` for more advanced call patterns.\n   \n * `[u_norm, coeffs, u] = smallestinconvexhull(M, x, U)` <br>\n   Computes `u`, a tangent vector to `M` at `x` contained in the convex hull spanned by the $n$ vectors in the cell `U`, with minimal norm (according to the Riemannian metric on `M`).\n   This is obtained by solving a convex quadratic program involving the Gram matrix of the given tangent vectors.\n   The quadratic program is solved using Matlab's built-in `quadprog`, which requires the Optimization Toolbox.\n   \n * `[A, B1, B2] = operator2matrix(M1, x, y, F, B1, B2, M2)` <br>\n   Given manifold structures `M1` and `M2`, two points `x` and `y` on these manifolds, and a function `F` encoding a linear operator from the tangent space $\\T_x \\calM_1$ to the tangent space $\\T_y \\calM_2$, this tool uses two orthonormal bases `B1` and `B2` (one for $\\T_x \\calM_1$, and one for $\\T_y \\calM_2$; generated at random if omitted), and forms the matrix `A` which represents the operator `F` in those bases.\n   In particular, the singular values of `A` are equal to the singular values of `F`.\n   If `M2` is omitted, then `M2 = M1`.\n   See the code for more use cases.\n \n\n\n## Interactive stopping criteria\n\nAn interactive stopping criterion allows the user to stop the execution of a Manopt solver in real time.\nWhen it is triggered, the solver gracefully terminates and outputs the best iterate it produced so far.\nMatlab then proceeds to keep running the code that follows the call to the solver, so that the work done until that point is not lost.\n\nOne such tool open a special figure once the solver starts running.\nThe solver terminates if the figure is closed.\n```matlab\noptions.stopfun = @stopifclosedfigure; % add this option\ntrustregions(problem, x0, options);    % run this or any other solver\n```\n\nAnother such tool (better suited if you are running Matlab without graphical user interface, e.g., over SSH) creates a special file.\nThe solver terminates if that file is deleted.\n```matlab\noptions.stopfun = stopifdeletedfile(); % add this option\ntrustregions(problem, x0, options);    % run this or any other solver\n```\nBy default, the file is called `MANOPT_DELETE_ME_TO_STOP_SOLVER`.\nYou may also specify another file name as optional input to `stopifdeletedfile`.\n\nNote that termination may not be immediate as the solver has to finish the current iteration first.\nIn particular, certain solvers (including `trustregions`) check stopping criteria only at outer iterations, not during inner iterations, further increasing the delay.\n\n\n## Utilities for solvers\n\n* `statsfunhelper` <br>\n  Helper function to place a function handle in the field `options.statsfun`, with the purpose of recording or displaying information about individual iterations.\n  See [this page](solvers.qmd#statsfun-option-for-recording-info-at-each-iteration) for documentation.\n  Also consider using `statscounters` and `incrementcounter` as documented [on this page](#counters-to-track-computations).\n\n* `manoptsolve` <br>\n  This tool presents itself as a solver, with their usual calling pattern:\n  ```matlab\n  [x, cost, info, options] = manoptsolve(problem, x0, options);\n  ```\n  It is a gateway function to call an actual Manopt solver.\n  You may specify which one to call by setting `options.solver` to a function handle corresponding to [a solver](https://github.com/NicolasBoumal/manopt/tree/master/manopt/solvers).\n  For example,\n  ```matlab\n  options.solver = @trustregions;\n  ```\n  If not, a solver is picked automatically.\n  This is mainly useful when programming meta algorithms which need to solve a Manopt problem, but one wants to leave the decision of which solver to use up to the final user (therefore making it an option).\n\n\n## Creating manifolds\n\n * `productmanifold` and `powermanifold` <br>\n   These tools generate a structure that represents a product of manifolds.\n   See [this page](manifolds.qmd#product-manifolds) for documentation.\n\n * `N = tangentspacefactory(M, x)` <br>\n   Given a manifold structure `M` and a point `x` on that manifold, outputs a manifold structure `N` representing the tangent space to `M` at `x`.\n   This is used in [preconhessiansolve](https://github.com/NicolasBoumal/manopt/blob/master/manopt/solvers/preconditioners/preconhessiansolve.m).\n\n * `N = tangentspherefactory(M, x)` <br>\n   Given a manifold structure `M` and a point `x` on that manifold, outputs a manifold structure `N` representing the unit sphere on the tangent space to `M` at `x`.\n   This is used by the [hessianextreme](https://github.com/NicolasBoumal/manopt/blob/master/manopt/tools/hessianextreme.m) tool.\n   \n\n## Miscellaneous\n\n* `y = sinxoverx(x)` <br>\n  Computes $y = \\sin(x)/x$, with the convention $\\sin(0)/0 = 1$.\n\n* `s = getsize(x)` <br>\n  Estimates the memory usage of the input variable.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":{"file":"_header.html"},"highlight-style":"pygments","css":["styles.css"],"toc":true,"html-math-method":"mathjax","strip-comments":true,"toc-depth":3,"output-file":"tools.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","crossref":{"eq-prefix":"Eq."},"theme":{"dark":["darkly","custom_darkly.scss"],"light":"sandstone"},"grid":{"sidebar-width":"230px","body-width":"800px","margin-width":"270px","gutter-width":"1.5rem"},"anchor-sections":true,"smooth-scroll":true,"title":"Helpful tools","toc-expand":2,"sidebar":"tutorial","aliases":["tutorial.html#tools"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}