<!DOCTYPE html>
<html lang="en">
  <head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type">
    <title>Manopt, fixed-rank manifold</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link href="favicon.ico" rel="icon" type="image/x-icon">
    <!-- Le styles -->
    <link href="bootstrap/css/bootstrap.css" rel="stylesheet">
    <style type="text/css">
body {
	padding-top: 80px;
	padding-bottom: 40px;
}
thead {
	font-weight: bold;
}
</style> <link href="bootstrap/css/bootstrap-responsive.css" rel="stylesheet">
    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script> <![endif]-->
    <link href="bootstrap/css/prettify.css" type="text/css" rel="stylesheet">
    <link href="bootstrap/css/lang-matlab.css" type="text/css" rel="stylesheet">
  </head>
  <body onload="prettyPrint()">
    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner"> <a class="btn btn-navbar"><span class="icon-bar"></span>
          <span class="icon-bar"></span> <span class="icon-bar"></span> </a>
        <div class="container"> <a class="brand" href="index.html">Manopt</a>
          <div class="nav-collapse collapse">
            <ul class="nav">
              <li><a href="index.html"><i class="icon-home"></i> Home</a></li>
              <li><a href="tutorial.html"><i class="icon-road"></i> Tutorial </a></li>
              <li><a href="forum.html"><i class="icon-edit"></i> Forum</a></li>
              <li><a href="about.html"><i class="icon-user"></i> About</a></li>
              <li><a href="#contactmodal"><i class="icon-envelope"></i> Contact</a></li>
            </ul>
          </div>
          <!--/.nav-collapse --> </div>
      </div>
    </div>
    <!-- Contact modal Begin -->
    <div id="contactmodal" class="modal hide fade" tabindex="-1">
      <div class="modal-header"> <button type="button" class="close">Ã—</button>
        <h3 id="myModalLabel">To contact us</h3>
      </div>
      <div class="modal-body">
        <p>To discuss code, it is best to use the <a href="forum.html">forum</a>.</p>
        <p>For things not suitable for the forum, e-mail us at <a href="mailto:manopttoolbox@gmail.com">manopttoolbox@gmail.com</a>.</p>
        <p>We are happy to receive feedback and bug reports or requests for more
          features, to discuss the toolbox in general as well as its
          documentation and to help you use it.</p>
        <p>We would also love to know how you use the toolbox, and if you built
          nice manifold factories, solvers or tools that could benefit others.</p>
      </div>
      <div class="modal-footer"> <button class="btn">Close</button> </div>
    </div>
    <!-- Contact modal End -->
    <div class="container">
      <div class="row">
        <div class="span12">
          <!--Body content-->
          <section id="geometry">
            <div class="page-header">
              <h1>A Riemannian quotient geometry for the set of fixed-rank
                matrices</h1>
            </div>
            <p>We denote the set of rank-$k$ matrices of size $m\times n$ as
              $\mathbb{R}_k ^{m \times n}$. We parameterize it as $X = LR^T$
              where $L \in \mathbb{R}^{m\times k}_k$ and $R \in
              \mathbb{R}^{n\times k}_k$. Notice that this factorization is not
              unique. Indeed, for any invertible matrix $M$ of size $k$, $(L,
              R)$ and $(LM^{-1}, RM^T)$ are mapped to the same $X$. This
              observation leads to the identification of $\mathbb{R}_k ^{m
              \times n}$ with the quotient space $(\mathbb{R}_k ^{m \times k}
              \times \mathbb{R}_k ^{n \times k}) / \mathbb{R}_k ^{k \times k}$.
              Points on this quotient space are equivalence classes $[(L, R)] =
              \{(LM^{-1}, RM^T) : M \in \mathbb{R}_k^{k\times k}\}$.
              Conceptually, we move on the quotient space but matrix
              representations are done in the total space, $\mathbb{R}_k ^{m
              \times k} \times \mathbb{R}_k ^{n \times k} $.</p>
            <p>A function $\phi :\mathbb{R}_k ^{m \times n} \rightarrow
              \mathbb{R}: X \mapsto \phi(X) $ induces a function $f:
              \mathbb{R}_k ^{m \times k} \times \mathbb{R}_k ^{n \times k}
              \rightarrow \mathbb{R}: (L, R) \mapsto f(L, R)$ where $f(L, R) =
              \phi(LR^T)$ (though in some developments belows we use the
              notation $f(X)$ for simplicity). The total space $\mathbb{R}_k ^{m
              \times k} \times \mathbb{R}_k ^{n \times k} $ is endowed with <strong>a
                scaled metric that is different from the standard Euclidean
                metric</strong>. This choice leads, in turn, to a Riemannian
              quotient geometry for $\mathbb{R}_k^{m \times n}$.</p>
            <p>The following material is referenced from the papers [AAM12] and
              [MMBS12] (see below).</p>
            <p>Factory call: <code>M = fixedrankfactory_2factors(m,n,k)</code>.
            </p>
            <table style="width: 100%;" class="table table-striped table-bordered">
              <thead>
                <tr>
                  <td>Name<br>
                  </td>
                  <td>Formula<br>
                  </td>
                  <td>Numerical representation</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Set</td>
                  <td>$\mathbb{R}_k ^{m \times n} = \{ LR^T: L \in \mathbb{R}^{m
                    \times k}_k, R \in \mathbb{R}^{n\times k}_k \} $</td>
                  <td>$X = (L, R)$ is represented as a structure <code>X</code>
                    with two fields. $L$ and $R$ are represented as matrices <code>X.L</code>
                    and <code>X.R</code> of size $m\times k$ and $n \times k$
                    respectively.</td>
                </tr>
                <tr>
                  <td>Horizontal space at $X$</td>
                  <td>$\{ (U_L, U_R) \in \mathbb{R}^{m\times k} \times
                    \mathbb{R}^{n\times k}: U_L^T L R^T R = L^T L R^T U_R \}$</td>
                  <td>A horizontal vector $(U_L, U_R)$ is represented as a
                    structure with two fields. $U_L$ and $U_R$ are represented
                    with matrices <code>U.L</code> and <code>U.R</code> of
                    size $m\times k$ and $n \times k$ respectively.</td>
                </tr>
                <tr>
                  <td>Ambient space</td>
                  <td>$\mathbb{R}^{m\times k}\times \mathbb{R}^{n\times k}$</td>
                  <td>Vectors in the ambient space are, naturally, represented
                    in the same way as horizontal vectors, but without
                    restrictions on the matrices <code>U.L</code> and <code>U.R</code>.</td>
                </tr>
              </tbody>
            </table>
          </section>
          <section id="toolset">
            <div class="page-header">
              <h1>Toolset</h1>
            </div>
            <p>The following table shows some of the nontrivial available
              functions in the structure <code>M</code>. The <a href="tutorial.html#manifolds">tutorial
                page</a> gives more details about the functionality implemented
              by each function.</p>
            <table style="width: 100%;" class="table table-striped table-bordered">
              <thead>
                <tr>
                  <td>Name<br>
                  </td>
                  <td>Field usage<br>
                  </td>
                  <td>Formula<br>
                  </td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Dimension<br>
                  </td>
                  <td><code>M.dim() </code></td>
                  <td>$\operatorname{dim}\mathcal{M} = (m + n - k)k;$<br>
                  </td>
                </tr>
                <tr>
                  <td>Metric<br>
                  </td>
                  <td><code>M.inner(X, U, V) </code></td>
                  <td>$\langle U, V\rangle_X =
                    \operatorname{trace}((L^TL)^{-1}U_L^T V_L) +
                    \operatorname{trace}((R^T R)^{-1}U_R^T V_R)$<br>
                  </td>
                </tr>
                <tr>
                  <td>Norm<br>
                  </td>
                  <td><code>M.norm(X, U) </code></td>
                  <td>$\|U\|_X = \sqrt{\langle U, U \rangle_X}$</td>
                </tr>
                <tr>
                  <td>Horizontal space projector<br>
                  </td>
                  <td><code>M.proj(X, H) </code></td>
                  <td>$P_X(H) = (H_L + L\Lambda^T, H_R - R \Lambda)$, where the
                    structure <code>H</code> represents a vector in the ambient
                    space. $\Lambda$ is the unique solution to the <a href="http://en.wikipedia.org/wiki/Lyapunov_equation">Lyapunov
                      equation</a>, $\Lambda (L^T L) (R^T R) + (L^T L)( R^T R)
                    \Lambda = (L^T L) (R^T H_R) - (H_L^T L) (R^T R)$. The output
                    is a horizontal vector at $(L, R)$.<br>
                  </td>
                </tr>
                <tr>
                  <td>Euclidean to Riemannian gradient</td>
                  <td><nobr><code>M.egrad2rgrad(X, egrad)</code></nobr></td>
                  <td>$\operatorname{grad} f(L, R) = (\nabla_Lf(X)\ L^T
                    L,\nabla_R f(X)\ R^T R)$, where <code>egrad</code>
                    represents the Euclidean gradient as a structure with two
                    fields corresponding to the partial derivatives $(\nabla _L
                    f(X), \nabla_R f(X))$, which is a vector in the ambient
                    space.<br>
                  </td>
                </tr>
                <tr>
                  <td>Euclidean to Riemannian Hessian</td>
                  <td><nobr><code>M.ehess2rhess(X, egrad, ehess, U)</code></nobr><br>
                  </td>
                  <td>$\operatorname{Hess} f(X)[U] = P_X(T_L, T_R)$, where \[
                    \begin{array}{lll} T_L = &amp; (\nabla^2 f(X)[U])_L L^T L +
                    2\nabla_Lf(X){\rm sym}(L^T U_L) - U_L (L^T L)^{-1} {\rm
                    sym}(L^T \nabla_Lf(X) L^T L) \\ &amp; - \nabla_Lf(X) {\rm
                    sym}(L^T U_L) + L (L^T L)^{-1} {\rm sym}(U_L ^T \nabla_Lf(X)
                    L^T L ) \\ T_R = &amp; (\nabla^2 f(X)[U])_R R^T R +
                    2\nabla_R f(X){\rm sym}(R^T U_R) - U_R (R^T R)^{-1} {\rm
                    sym}(R^T \nabla_R f(X) R^T R) \\ &amp; - \nabla_R f(X) {\rm
                    sym}(R^T U_R) + R (R^T R)^{-1} {\rm sym}(U_R ^T \nabla_R
                    f(X) R^T R ). \end{array} \] Here ${\rm sym}(\cdot)$
                    extracts the symmetric part of a square matrix, i.e., ${\rm
                    sym}(A) = (A + A^T) /2$. <code>egrad</code> represents the
                    Euclidean gradient $(\nabla _L f(X), \nabla _R f(X))$ and <code>ehess</code>
                    represents the Euclidean Hessian $({(\nabla^2 f(X)[U])_L},
                    (\nabla^2 f(X)[U])_R )$, both being vectors in the ambient
                    space.</td>
                </tr>
                <tr>
                  <td>Retraction<br>
                  </td>
                  <td><code>M.retr(X, U, t) </code></td>
                  <td>$\operatorname{Retr}_X(tU) = (L + tU_L, R + tU_R)$<br>
                  </td>
                </tr>
                <tr>
                  <td>Random point<br>
                  </td>
                  <td><code>M.rand() </code></td>
                  <td>Returns a random point generated as follows: generate $L$
                    and $R$ with i.i.d. normal entries; return $(L, R)$. With
                    high probability, $L$ and $R$ are indeed full-rank.<br>
                  </td>
                </tr>
                <tr>
                  <td>Random vector<br>
                  </td>
                  <td><code>M.randvec(X) </code></td>
                  <td>Returns a horizontal vector at $X$ with uniformly random
                    direction, obtained as follows: generate $H_L$ and $H_R$
                    with i.i.d. normal entries; return $P_X(H)/\|P_X(H)\|_X$. </td>
                </tr>
                <tr>
                  <td>Vector transport<br>
                  </td>
                  <td><code>M.transp(X, Y, U) </code></td>
                  <td>$\operatorname{Transp}_{Y\leftarrow X}(U) = P_{(L_Y,R_Y)}
                    (U)$, where $U$ is a horizontal vector at $(L_X, R_X)$ that
                    is transported to the horizontal space at $(L_Y, R_Y)$. </td>
                </tr>
              </tbody>
            </table>
          </section>
          <section id="differentials">
            <div class="page-header">
              <h1>Example</h1>
            </div>
            <p>This is an example of low-rank matrix approximation. Let
              $A\in\mathbb{R}^{m\times n}$ be given. The best rank-$k$
              approximation of $A$ w.r.t. the Frobenius norm can be computed via
              SVD of $A$. Here, instead, we will compute it using optimization
              on the manifold we just described. There is no claim that this is
              a good way to proceed for this particular problem: this is merely
              an example to illustrate the use of the manifold.</p>
            <p>We wish to minimize the following cost function:</p>
            <p>$$\phi(X) = \frac{1}{2}\| X - A \|_F ^2,$$</p>
            <p>such that $X \in \mathbb{R}_k ^{m \times n}$. $X$ is factorized
              as $LR ^T$ with $L \in \mathbb{R}_k ^{m \times k}$ and $R \in
              \mathbb{R}_k ^{n \times k}$. The function $f:\mathbb{R}_k ^{m
              \times k} \times \mathbb{R}_k ^{n \times k} \rightarrow
              \mathbb{R}$ is defined as $f(L, R) = (1/2) \cdot \| LR^T - A \|_F
              ^2 $. Compute the Euclidean gradient and Hessian of $f$ w.r.t. $L$
              and $R$:</p>
            <p>$$\nabla _L f(X) = (LR^T - A)R,$$</p>
            <p>$$\nabla _R f(X) = (LR^T - A)^T L,$$</p>
            <p>$$(\nabla^2 f(X)[U])_L = (U_L R^T + LU_R^T )R + (LR^T - A)U_R, $$</p>
            <p>$$(\nabla^2 f(X)[U])_R = (U_L R^T + LU_R^T )^T L + (LR^T - A)^T
              U_L.$$</p>
            <p>The Riemannian gradient and Hessian are obtained automatically by
              applying the <code>M.egrad2rgrad</code> and <code>M.ehess2rhess</code>
              operators. After the prototyping stage, it may be helpful though
              to check what the expressions for the Riemannian gradient and
              Hessian turn out to be, as it is often possible to compute them
              more efficiently then with the generic conversion tools.</p>
            <pre class="prettyprint lang-matlab linenums">function fixedrank_test()

% Generate the problem data. 

% Problem
m = 500;
n = 1000;
A = randn(m, n);

% We aim for a rank k approximation of A
k = 5;

% Create the problem structure.
manifold = fixedrankfactory_2factors(m,n,k);
problem.M = manifold;

% Define the problem cost function and its gradient.

problem.cost = @cost;
    function f = cost(X)
        f = .5*norm(X.L*X.R' - A, 'fro')^2;
    end

problem.grad = @(X) problem.M.egrad2rgrad(X, egrad(X)); 
    function g = egrad(X)
        P = (X.L*X.R' - A);
        g.L = P*X.R;        
        g.R = P'*X.L;
    end

problem.hess = @(X, U) problem.M.ehess2rhess(X, egrad(X), ehess(X, U), U); 
    function Hess = ehess(X, eta)
        P = (X.L*X.R' - A);
        Pdot = (eta.L*X.R' + X.L*eta.R');
        Hess.L = Pdot*X.R + P*eta.R;
        Hess.R = Pdot'*X.L + P'*eta.L;
    end


% Numerically check the differentials.
checkgradient(problem); pause;
checkhessian(problem); pause;

X = trustregions(problem);

end
</pre>
            <p>Notice that there are a lot of redundant computations in this
              implementation. For example, computing the gradient at $(L, R)$
              will recompute the residue $LR^T - A$ which was probably already
              computed when the cost function was evaluated at the same point.
              See the <a href="tutorial.html#costdescription">tutorial </a>for
              ways to avoid redundancy. This can result in significant speed
              ups.</p>
            <p>Notice that the <code>checkhessian</code> test fails: the slope
              is not right. This is because the retraction is not second-order
              compatible with the Riemannian exponential on this manifold,
              making the <code>checkhessian</code> tool unusable. The Hessian
              is correct though. The warning about the exponential map can be
              disabled with this command: <code>warning('off',
                'manopt:fixedrankfactory_2factors:exp')</code>.</p>
          </section>
          <section id="references">
            <div class="page-header">
              <h1>References</h1>
            </div>
            [AAM12] P.-A. Absil, L. Amodei and G. Meyer, <a href="http://arxiv.org/abs/1209.0068">Two
              Newton methods on the manifold of fixed-rank matrices endowed with
              Riemannian quotient geometries</a>, Tech. report, arXiv:1209.0068,
            2012.<br>
            [MMBS12] B. Mishra, G. Meyer, S. Bonnabel and R. Sepulchre, <a href="http://arxiv.org/abs/1209.0430">Fixed-rank
              matrix factorizations and Riemannian low-rank optimization</a>,
            Tech. report, arXiv:1209.0430, 2012<br>
            [AMS08] P.-A. Absil, R. Mahony and R. Sepulchre, <a target="_blank"
              href="http://press.princeton.edu/chapters/absil/">Optimization
              Algorithms on Matrix Manifolds</a>, Princeton University Press,
            2008. </section>
          <section id="examples">
            <div class="page-header">
              <h1>List of examples</h1>
            </div>
            We are still working on building a collection of examples for
            Manopt. The relevant ones will be referenced here when the time
            comes. </section>
        </div>
      </div>
    </div>
    <!-- /container -->
    <!-- Le javascript ================================================== -->
    <script type="text/javascript" src="bootstrap/js/jquery.min.js">
	</script>
    <script type="text/javascript" src="bootstrap/js/bootstrap.js">
	</script>
    <script type="text/javascript" src="bootstrap/js/prettify.js">
	</script>
    <script type="text/javascript" src="bootstrap/js/lang-matlab.js">
	</script>
    <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});</script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> <script type="text/javascript">

var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-37402854-1']);
_gaq.push(['_trackPageview']);

(function() {
 var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
 })();
</script> </body>
</html>
